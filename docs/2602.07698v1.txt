On Sequence-to-Sequence Models for Automated Log Parsing
Adam Sorrentia , Andriy Miranskyya
a Department of Computer Science, Toronto Metropolitan University, 350 Victoria

Street, Toronto, M5B 2K3, Ontario, Canada

arXiv:2602.07698v1 [cs.SE] 7 Feb 2026

Abstract
Context: Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to
heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of
rule-based approaches.
Objectives: This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance
and computational cost.
Methods: We conduct a controlled empirical study comparing four sequence modelling architectures:
Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396
models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit
distance with statistical significance testing.
Results: Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145),
mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive
accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and
Transformer demonstrate stronger sample efficiency than recurrent models.
Conclusion: Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative
under data or compute constraints. These results also clarify the roles of representation choice, sequence
length, and sample efficiency, providing practical guidance for researchers and practitioners.
Keywords: Automated log parsing, Recurrent neural networks, Sequence-to-sequence models, State space
models
1. Introduction
Software systems produce vast amounts of log data, a rich source of information crucial for system
diagnostics [1–3], data analytics [4, 3], and anomaly detection [5–9]. Software logs are often the only available
record of software runtime state. These logs, generated at an unprecedented scale [10], encapsulate a detailed
history of system activities, errors, and transactions. However, the volume and complexity of these data
present significant challenges in effective log parsing and analysis. Traditional, rule-based methods struggle to
keep pace with the evolving log formats found in modern large-scale software systems. Tools for log analysis
prefer a unified format [11], yet many log sources are inherently diverse in structure, varying significantly
across systems and applications. This diversity of log formats necessitates more insight into adaptable
methods for effective log parsing.
Previous empirical studies show that cloud monitoring logs exhibit inherent structural heterogeneity
due to the scale, velocity, and diversity of monitored components [12], that operational log volumes render
manual analysis infeasible even within a single cloud provider [13], and that elasticity and continuous service
evolution invalidate static monitoring assumptions over time [14].
Many traditional log parsing methods rely on domain experts to craft and maintain structured templates
for extraction. Related rule-based approaches [15, 16] include tree parsing and heuristics. There also exists a
Email addresses: adam.sorrenti@torontomu.ca (Adam Sorrenti), avm@torontomu.ca (Andriy Miranskyy)

large body of research on data-driven log parsing techniques, such as frequent pattern mining [17], clustering
[18, 19], and longest common subsequence analysis [20] that extract log templates. Neural network-based
approaches to automated log parsing, such as masked language modelling [21, 22] and large language models
(LLMs) [23], have also shown efficacy. More recently, a hybrid approach has been investigated using an
encoder-only Transformer model [24] to extract vector embeddings from log strings, followed by clustering
and template extraction [25].
Log template extraction approaches often rely on unified log formats to distinguish between static and
variable components of a log by comparing multiple, similar log strings. The advancements in neural network
architectures offer a promising alternative approach that does not rely on log template extraction. Long
Short-Term Memory (LSTM), a variant of the recurrent neural network (RNN), has demonstrated promising
log parsing capabilities using a sequence-to-sequence approach [26] to map each input log token to its
corresponding log field.
While emerging state space and linear-time RNN models, such as S4 [27], xLSTM [28], and ParaRNN [29],
introduce promising efficiency characteristics, their application in log parsing remains unexplored. In particular, architectures like the Mamba state space model [30] have not been systematically evaluated for this
application, representing a novel direction of investigation in this work.
This work is structured around several key research questions (RQs) as follows:
RQ1. What is the impact of sequence length on model performance?
RQ2. How do different tokenization methods affect performance?
RQ3. How does sample efficiency vary across model architectures?
RQ4. Which model architecture performs best?
By investigating these research questions, we have the potential to inform researchers and practitioners on
the best practices for applying deep sequence-to-sequence models to log parsing.
In this work, we use the term generalization to refer specifically to robustness under log format distribution shift, including changes in field order, field presence, and formatting conventions between training and
validation data. We do not evaluate the semantic understanding of log content nor cross-domain generalization across fundamentally different logging paradigms (e.g., structured JSON or event-based logs).
The contributions of this work are summarized as follows.
• Large-scale empirical analysis of key modelling factors. We systematically quantify the impact
of sequence length, tokenization strategy, and training data availability on log parsing performance,
providing actionable guidance on when specific architectures are likely to succeed or fail in practice.
• Novel application of state space models (Mamba) for log parsing. We demonstrate that
Mamba-style architectures can achieve competitive accuracy while offering substantially lower computational cost, highlighting their suitability for production environments with resource constraints.
• Cross-paradigm comparison of modern sequence modelling architectures. We provide a
controlled comparison of Transformers, bidirectional and monodirectional LSTMs, and state-space
models, enabling architecture-level insights grounded in four research questions spanning accuracy,
efficiency, and data sensitivity.
• Quantitative characterization of dataset structure and format diversity. Beyond model
benchmarking, we analyze log format variability and field complexity, clarifying how real-world log
heterogeneity influences parsing difficulty and model behaviour.
The remainder of this work is structured as follows. Section 2 highlights background material relevant
to the work. Section 3 discusses the proposed methodology and its implementation. Evaluation and results
of the aforementioned research question are given in Section 4. Finally, Section 5 concludes the paper.

2

2. Background
Logging mechanisms construct statements that contain static and variable components, often devised by
domain experts with little documentation and lax standards [31]. The following subsection provides relevant
background to the task and evaluation of automated log parsing.
2.1. Log Parsing
Log parsing is the act of extracting and identifying the variable log components (otherwise known as ‘log
fields’). The differentiation between static and variable log components is often given [32] as a log format
(otherwise known as ‘event template’) where special tokens such as ‘<*>’ represent the variable substring
(the log field).
Log fields can also be parsed using a fixed vocabulary. Figure 1 shows an Apache web server log and
its associated log format on a per-character basis. Each character of the log input is assigned a category
that corresponds to the semantic meaning of that substring. For example, the field characters which map to
08/Apr/2025:16:47:52 -0600 represent a timestamp, denoted by t. Additionally, the underscores denote
a separator between the fields. Reference Table 1 for a description of each log field type.
Table 1: A list of the relevant Apache web server log fields [33].

Field
acronym

Field
description

h
l

IP address of the client host. Can be IPv4 or IPv6.
The remote logname. We were unable to find a good example of what kinds of values are
returned by Apache servers, thus for this paper we only supplied the commonly-given value
‘-’. This field could not be omitted as it is present in both the ELF and CLF formats.
The remote username. Can be empty (‘-’)
The datetime of the request, presented in the default [day/month/year:hour:minute:second
zone] format.
The request line from the client. Made up of the method, path and querystring, and protocol.
The status of the request.
The number of bytes sent.
The request method.
The requested URI path.
The request protocol.
The request querystring.
The canonical servername of the server servicing the request.
The servername according to UseCanonical. In our generator, this field is identical to the v
field.
The user agent of the request† .
The referrer of the request† .
Represents a separator between log fields.

u
t
r
s
b
m
U
H
q
v
V
i
R
_

† In a real Apache HTTP server deployment, this field is extracted from the %i log parameter, see [33] for details.

197.34.164.236 - - 08/Apr/2025:16:47:52 -0600 "POST HTTP/1.1" 500 79060
hhhhhhhhhhhhhh_l_u_tttttttttttttttttttttttttt_"rrrrrrrrrrrrr"_sss_bbbbb
Figure 1: A sample sequence-to-sequence mapping of an Apache web server log. The first line contains the raw log message.
The second line contains the field type to which a given input character is mapped.

3

2.2. Benchmarks
The systematic evaluation of log parsing techniques enables the research community to take steps in the
direction of more robust and accurate methods. LogHub [34] is a well-known benchmark dataset collection in
the field of automatic log parsing and log-based anomaly detection. LogHub offers a comprehensive collection
of real-world log datasets gathered from diverse software systems, including cloud platforms, distributed
systems, and high-performance computing environments.
Table 2 summarizes the Loghub-2k datasets [34] (a preprocessed subset of the LogHub dataset containing
2000 randomly sampled examples) by the number of unique log templates and the statistics that describe
the number of variables per log record. Notice that the largest mean number of fields in a log is 4.5 (refer
to the ‘Proxifier’ dataset), and the largest number of unique log formats is 341 (refer to the ‘Mac’ dataset).
While the diversity of production logging mechanisms is a strength of Loghub-2k, the range and magnitude
of unique log formats and fields are limited compared to industry-standard log formats, such as Apache web
server logs.
Table 2: The number of log formats (left) and unique log field statistics (right) per Loghub-2k dataset.

Unique Log Field
Dataset
Android
Apache
BGL
HDFS
HPC
Hadoop
HealthApp
Linux
Mac
OpenSSH
OpenStack
Proxifier
Spark
Thunderbird
Windows
Zookeeper

Count

Mean

Median

Min.

Max.

Std. Dev.

166
6
120
14
46
114
75
118
341
27
43
8
36
149
50
50

2.14
1.50
2.18
3.64
1.24
1.75
1.32
1.19
3.06
2.07
2.63
4.50
1.08
1.58
1.40
1.82

1.00
1.50
1.00
3.50
1.00
1.00
1.00
1.00
1.00
2.00
1.00
4.50
1.00
1.00
1.00
1.00

0
1
0
1
0
0
0
0
0
0
0
2
0
0
0
0

20
2
25
6
9
12
8
13
36
4
14
6
4
10
7
7

3.10
0.55
2.85
1.34
1.80
2.24
1.72
1.65
4.69
1.27
2.55
1.31
1.13
1.99
1.62
1.73

Rand et al. [26] introduced a synthetically generated Apache web server log parsing dataset (we denote
this dataset as HTTPd-parse) and validated its efficacy with publicly available logs from three productiongrade systems. HTTPd-parse consists of four synthetically generated training datasets of varying difficulty.
TT , TE , TM , and TH are training datasets labelled to describe their similarity to the validation datasets (VA ,
VB , and VC ) and their presumed modelling difficulty (trivial, easy, medium, and hard, respectively).
The heuristic used to modulate training dataset difficulty (as described in [26]) operates by varying the
percentage of standard and random Apache log formats used in the training dataset versus the validation
dataset’s composition. A complete description of the training and validation dataset composition is provided
in Section 3.2. Table 3 describes the order in which web server log fields (whose description is given in Table 1)
appear in the source log record. A key differentiation is shown in Table 4, which indicates that HTTPdparse contains a larger range and magnitude of log formats and unique log fields compared to Loghub-2k.
Furthermore, Table 5 reports the percentage of ELF log records in each training dataset, illustrating the
degree of format mismatch between training and validation data, particularly for the medium- and highdifficulty settings.
2.2.1. Evaluation Metrics
To motivate the discussion of log parsing evaluation, we will first consider Parsing Accuracy (PA). PA is
defined as the ratio of correctly parsed log records to the total number of log records parsed. For example, if
4

Table 3: Standard Apache log formats.

Name

Fields (acronyms)

Common Log Format (CLF)
Combined Log Format (ELF)

h l u t "r" s b
h l u t "r" s b "R" "i"

Table 4: The number of log formats (left) and unique log field statistics (right) per HTTPd-parse training (top) and validation
(bottom) dataset.

Unique Log Field
Dataset

Count

Mean

Median

Min.

Max.

Std. Dev.

TT
TE
TM
TH

1
7078
47089
92103

9.00
8.31
7.76
8.52

9.00
9.00
7.00
9.00

9
2
2
2

9
15
15
15

0.00
2.27
2.63
3.56

VA
VB
VC

1
1
1

9.00
9.00
9.00

9.00
9.00
9.00

9
9
9

9
9
9

0.00
0.00
0.00

Table 5: Summary of HTTPd-parse training datasets and their percentage of ELF logs.

Dataset
TT
TE
TM
TH

# of logs

# of ELF

% of ELF

100000
20000
100000
100000

100000
8011
0
0

100.0
40.1
0.0
0.0

5

a dataset under study has a set of three possible log formats {e1 , e2 , e3 } and a total of four log records have
been parsed and assigned formats [e2 , e2 , e3 , e1 ]. Then, if the ground truth log formats for the four parsed
log records are actually [e1 , e2 , e2 , e3 ], we would have a PA = 14 since the second parsed log record is the only
one correctly labelled.
When evaluating parsed log formats in terms of a sequence mapping (as shown in Figure 1), rather than
simply assigning each log record a format based on the order in which fields appear, Levenshtein edit distance
can be used to quantify string (dis)similarity [26]. An absolute edit distance denoted DA , measures the total
number of single-character edits (insertion, substitution, or deletion) required to transform one string into
another. The relative edit distance, denoted DR , normalizes the DA by the expected string length. When
comparing models, it is essential to consider the DR , as differences in sequence length across validation
datasets may introduce scaling bias in absolute performance. Note that for all results, we truncate the
predicted character-based target fields to match the input logs before calculating DA and DR between the
prediction and ground truth. Truncation was performed equivalently for all experiments and is a common
log parsing heuristic in production systems.
We will report Levenshtein edit distance as our primary metric, in accordance with the authors of the
HTTPd-parse [26], because it offers an interpretable evaluation of partially correct predictions, distinguishing
near misses from severe errors by penalizing the number of edits required.
2.3. Solution Space
Automated log parsing has been extensively studied, resulting in a range of approaches. For completeness,
the following items will explore the space of log parsing solutions by briefly describing the common techniques
already in the literature:
1) Frequent Pattern Mining: Frequent pattern mining-based log parsing methods leverage the idea that
groups of similar log records share repeated (frequent) tokens. These approaches often rely on strategies for
identifying frequently co-occurring token sets and treating the remaining substrings as variable parameters.
For example, MoLFI [35] employs a two-phase clustering approach to discover frequent token patterns
and subsequently refines these patterns to produce event templates. In general, frequent pattern mining
approaches can handle large log datasets but may require careful parameter tuning (e.g., support thresholds)
to avoid over-segmentation or under-segmentation of log statements.
2) Clustering: Clustering-based approaches assume that log records belonging to the same event template form natural clusters in a chosen feature space (e.g., token embeddings or string similarity measures).
Techniques such as LogSig [19] and Logcluster [18] apply token-level similarity or n-gram-based measures to
group logs into distinct clusters, and each cluster then corresponds to a log template. Clustering techniques
can be effective in capturing subtle differences between similar logs, although they often require determining
the optimal number of clusters or similarity thresholds.
3) Longest Common Subsequence: Approaches based on the longest common subsequence view two log
records as sequences of tokens and identify their shared (static) components by computing the LCS. Any
tokens not captured within the LCS are assumed to be variable fields. An example is the LCS-based log
parsing framework proposed in [20], which uses a multi-step process to identify the static subsequence,
refine it to remove noise, and then construct log templates. LCS-based methods typically perform well
in scenarios where logs exhibit relatively small modifications (e.g., minor differences in parameter values)
around a common template. However, performance may degrade in highly diverse or noisy logs due to an
explosion in pairwise comparisons.
4) Heuristics: Heuristic-based approaches rely on systematic rules to identify variable tokens by traversing and comparing log strings. For instance, the tree-based parsing models proposed in [15, 16] first hierarchically split log records by delimiters (e.g., whitespace or punctuation) and then employ domain-specific
heuristics (e.g., numeric detection, IP-address pattern recognition) to recognize and replace variable fields.
Another example is the Drain parser [15], which incrementally partitions logs based on prefix and length
heuristics to group messages into templates. Heuristic-based approaches often achieve fast parsing speeds,
but their success relies heavily on well-chosen rules or domain knowledge to handle edge cases.

6

2.4. Deep Sequence Models
A deep sequence model is defined as a parameterized function y = fθ (x) that transforms an input sequence
′
x ∈ RL×Din into an output sequence y ∈ RL ×Dout . Here L and L′ denote the input and output lengths,
while Din and Dout are the sequence element feature dimensions. The learned parameters θ of the function
f are computed by a stochastic gradient descent approach via back-propagation to minimize a task-specific
loss.
Deep sequence models often follow an encoder–decoder structure in order to map sequences of varying
length and dimensionality [36, 37]. The following items review the major classes of deep sequence models
relevant to our study of automatic log parsing, highlighting their computational trade-offs and motivating
the exploration of state space models.
2.4.1. Recurrent Neural Network
Recurrent neural networks (RNNs) process sequences by maintaining a hidden state that is updated
sequentially over time. Gated variants, most notably the LSTM [38] and gated recurrent unit (GRU) [36],
mitigate vanishing and exploding gradients through explicit gating mechanisms and have demonstrated
strong performance in sequence-to-sequence log parsing formulations [26].
Additionally, RNN-based models suffer from two fundamental limitations: (i) training requires backpropagationthrough-time (BPTT), which is memory-intensive for long sequences, and (ii) computation is inherently
sequential, limiting parallelism.
2.4.2. Transformer
Many sequence modelling tasks require maintaining long-term dependencies, an operation that has been
difficult for RNNs [39]. The recurrent state(s) of a RNN is required to maintain relevant information across
the sequence in order to realize long-term dependencies upon prediction.
Attention mechanisms allow for the effective modelling of long-term dependencies [40, 41]. While attention mechanisms were successfully introduced in conjunction with RNNs, the Transformer architecture
[24] subsequently found that removing recurrence and structuring its computation wholly around a highly
optimizable self-attention mechanism both improved sequence modelling performance and parallelism.
While this approach forgoes issues related to BPTT and non-linear recurrence, requiring only a constant
number of sequential operations compared to O(L) operations in an RNN, its computational complexity grows
quadratically with sequence length. The trade-off between parallelization and unfavourable computational
complexity growth with sequence length has prompted innovation in self-attention, such as sparse attention,
low-rank methods, and down-sampling [42].
2.4.3. State Space Models
Structured state space models (SSMs) [27, 43] have recently emerged as a promising class of deep sequence models aiming to build on RNN principles with analytic linear operators to reconcile efficiency and
expressivity. SSMs enable highly parallel training via global convolution, while retaining recurrent inference
with modest memory requirements.
3. Methodology
This chapter outlines the proposed methodology and implementation parameters which are used to explore our research questions. Five main experimental branches were constructed to evaluate the performance
of deep sequence modelling in automatic log parsing. These branches focus on training dataset composition,
tokenization methods, sequence lengths, training dataset sizes, and model architectures.
3.1. Model Architectures
The model architectures, LSTM and Transformer, are two prominent neural network architectures used
extensively in sequence modelling. LSTMs are specifically designed for sequential data, featuring recurrent computational cells and gating mechanisms that handle long-term dependencies (as described in Section 2.4.1). In contrast, Transformers rely on a self-attention mechanism to capture long-range dependencies
(as described in Section 2.4.2). Recent advancements in the area of deep sequence modelling have shown
7

promise for a new class of recurrent models that are parallelizable and efficiently scalable (as described in
Section 2.4.3). We study one such RNN architecture: Mamba [30] (an implementation of SSMs). We will
now denote and detail specific experimental design and implementation choices for each architecture.
LSTM. We use a mono-directional LSTM, denoted as ML , reproduced to reflect prior work [26]. Additionally, we examine a bi-directional LSTM model, MB , to comprehensively evaluate and compare performance
across different sequence modelling modalities for log parsing. Bi-directional LSTM considers the input sequence in both the forward and backward directions, aiming to utilize sequential dependencies between input
tokens to better predict the current output (as described in Section 2.4.1). ML and MB utilize identical
underlying recurrent units with 512 cells, featuring a dropout rate of 0.2 in the encoder and decoder LSTM
layers. These hyperparameters were chosen based on the best-performing models in prior work [26] and serve
as a baseline against which to compare other model architectures.
Transformer. Our Transformer model implementation, denoted MT , aligns with the original implementation
using 256 embedding units in both the encoder and decoder, a hidden state size of 2048 in each feed-forward
block, and eight attention heads [24]. Additionally, we include a dropout rate of 0.2 in the decoder layer.
Mamba. We explore a Mamba model, denoted MM , using an embedding dimension of 128. Additionally, we
use the default parameters provided by the reference PyTorch implementation [44], notably a state expansion
factor of 16, a local convolution width of 4, and a block expansion factor of 2.
3.2. Data
Training data is separated into four distinct datasets of varying composition (as introduced in Section 2.2).
These training datasets, denoted TT , TE , TM , and TH , represent their presumed modelling difficulty: trivial
(TT ), easy (TE ), medium (TM ), and hard (TH ). Table 6 details the specific log format description for each
training and validation dataset in terms of the percentage of standardized Apache log formats (ELF and
CLF) and random log formats. Random log formats (which are present in TE , TM , and TH at varying
degrees), contain logs with 2 to 15 unique Apache log fields (as described in Table 1) arranged in a random
order. To reiterate for clarity, it should be noted that validation datasets VA , VB , and VC contain log formats
entirely constructed using the ELF. Thus, training datasets that contain more ELF examples best reflect
the validation datasets.
3.2.1. Data Preprocessing
Architectural limitations of the Transformer model, MT , with respect to sequence length (as described
in Section 2.4.2) led to the exploration of different tokenization methods to evaluate the architecture’s log
parsing ability. For ML , MB , and MM , character-based and word-based tokenization methods are performed,
followed by one-hot encoding with a maximum frequency-based vocabulary size of 15, 000. Word-tokenization
is performed by delimiting log strings by whitespace and punctuation (as defined by the C locale).
MT is trained with word-based tokenization only. Additionally, MT training sequences are truncated at
varying lengths, low, medium, and high, which represent sequence lengths of 256, 765, and 4188, respectively.
These sequence lengths map to approximately the 70th, 99th, and 100th percentiles of log sequence lengths
in the validation datasets.
All training datasets are divided into three bins denoting the percentage of observations used: 10%, 50%,
and 100%. Partial training datasets (i.e., 10% and 50%) are randomly sampled five times with different
seeds to minimize sampling bias and enhance reproducibility.
3.3. Experiment Design
Accounting for the specific permutations of training datasets (TT , TE , TM , and TH ), tokenization methods
(character- and word-based), sequence lengths (256, 756, and 4188, for MT only), training dataset sizes (10%,
50%, and 100%, where partial datasets are resampled five times), and model architectures (ML , MB , MT ,
and MM ) – a total of 396 models are trained.
Training is performed using 300 epochs and a mini-batch size of 64 log records. The Adam optimizer [45]
is used with an initial learning rate of 10−3 , β1 = 0.9, β2 = 0.999, and ε = 10−7 .

8

Table 6: Datasets’ descriptions adapted from [26]. TT , TE , TM , and TH , represent the presumed modelling difficulty of trivial,
easy, medium, and hard, respectively. Log record length is denominated in characters of the respective (min, median, or max)
log strings. The Common Log Format (CLF) and the Extended Log Format (ELF) are standardized Apache log formats.

Dataset

Log records

Log records length

count

min

median

max

TT
TE

100,000
20,000

136
4

272
250

1173
1173

TM

100,000

4

161

1294

TH

100,000

4

291

1528

VA
VB
VC

7,314
6,539
10,000

194
79
81

238
238
231

602
4398
1363

Log records’ format description
100% ELF
≈ 40% of the ELF format, ≈ 24% of the CLF format, and ≈ 36% of randomly drawn and reshuffled fields shown in Table 1. The random strings
have 2 to 14 records in them.
≈ 50% of the CLF format and ≈ 50% of the
randomly generated records using the same approach as in the TE case. The random strings
have 2 to 14 fields in them.
100% of the randomly generated records using
the same approach as in the TE case. The random strings have 2 to 15 fields in them.
100% ELF
100% ELF
100% ELF

3.4. Specifications
This section provides a detailed description of the software environment, computational resources, and
model complexity used in the experiments, supporting reproducibility and transparency.
Software Environment. All models were trained on the Digital Research Alliance of Canada’s (the Alliance)
compute clusters using Python 3.10 as the base runtime environment. The primary deep learning framework
consisted of TensorFlow 2.15.1 [46] and Keras 2.15.0 [47]. In addition, experiments that explored state
space models utilized the mamba_ssm package (version 1.2.0.post1) [30], which relies on PyTorch 2.2 [48]
as its backend. Experiment orchestration and job management were handled using Hydra 1.1.1 [49] and
SLURM [50] scheduling on the Alliance’s high-performance computing infrastructure.
Compute Resources. Training runs all utilize NVIDIA A100-40GB GPUs. In total, the experiments consumed approximately 2.92 GPU-years.
Model Costs. A range of model architectures and hyperparameter configurations were explored, including
LSTM, Transformer, and Mamba models with varying tokenization strategies and sequence lengths. Table 7 details the number of trainable parameters for each relevant model configuration used. As expected,
increasing the input sequence length results in an increase in parameter count for Transformers, whereas the
parameter count for recurrent models remains constant.
Word-level tokenization generally resulted in higher parameter counts than character-level tokenization,
primarily due to the larger vocabulary embedding matrices. Similarly, bi-directional LSTM architectures
consistently exhibited higher parameter counts than their mono-directional counterparts, reflecting the doubled recurrence. Notably, the Mamba models utilized significantly fewer parameters compared to LSTM and
Transformer architectures, indicating their potential for lower inference cost.
We also report the wall-clock time for training and inference on an NVIDIA A100-40GB GPU, measured
using single-sample batches after a warmup period and averaged over multiple runs to ensure stable estimates.
Transformer models exhibit rapidly increasing training and inference time with increasing sequence length,
incurring costs orders of magnitude higher. Recurrent models (ML , MB ) exhibit moderate, relatively stable
computational costs, consistent with their linear-time sequential processing. In contrast, Mamba (MM )

9

achieves the lowest training and inference latency across all configurations, with near-constant cost across
tokenization schemes, reflecting the architectural differences of state-space sequence modelling.
Table 7: Number of trainable parameters and profiled per-batch cost for different model configurations.

Model configuration

# of Params.

Train (ms)

Infer (ms)

56,752,280
53,247,128
52,725,912
18,207,264
11,895,328
10,556,436
4,250,644
2,040,736
129,556

630.91
72.59
25.29
52.58
28.56
51.96
27.87
1.82
1.80

158.58
29.06
26.59
48.27
22.18
41.87
23.13
0.46
0.46

MT (L=4188, Word)
MT (L=765, Word)
MT (L=256, Word)
MB (Word)
ML (Word)
MB (Char)
ML (Char)
MM (Word)
MM (Char)

4. Evaluation
To assess the performance of our models, we use absolute and relative Levenshtein distance (also known
as edit distance). A lower Levenshtein distance indicates greater similarity, as fewer edits are needed to
make the two strings identical. Refer to Section 2.2.1 for further details on log parsing evaluation metrics,
and Appendix A.6 for a verbose record of all validation results.
In this chapter, we systematically address each research question using experimental evidence and statistical analyses.
4.1. RQ1: What is the impact of sequence length on model performance?
To evaluate the impact of sequence length on the performance of MT , we will analyze DR across three
sequence lengths: 256, 756, and 4188. Statistical analyses and descriptive summaries are computed across
training datasets T(·) , validation datasets V(·) , and different training data sampling levels (10%, 50%, and
100%) to assess both the magnitude and significance of performance differences.
4.1.1. Descriptive Statistics
Across all experimental conditions, the mean relative edit distance increased with sequence length. However, Table 8 shows the median relative edit distance remained stable around 0.10, suggesting that improvements primarily affect the upper distribution tail rather than the central tendency. Notably, very long
sequences constitute a small fraction of the data (upper percentiles only), which may limit sensitivity to the
effect of higher sequence lengths in training.
The standard deviation and the number of outliers exhibit a similar increase with sequence length,
suggesting that longer sequences are associated with greater variability in performance.
These descriptive trends were consistent across most training and validation sets as well as across different
training data percentages, confirming that the effect is robust across data regimes (see Appendix A.1). An
exception exists for higher sequence lengths with TT training data or 100% of training data, indicating
sensitivity to outliers (as discussed in Section 4.2.3).
Table 8: Descriptive statistics measured in relative edit distance DR and the number of outliers across sequence lengths.

Sequence Length

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

256
756
4188

0.15
0.18
0.19

0.10
0.10
0.10

0.06
0.04
0.04

0.18
0.22
0.22

0.17
0.22
0.23

87,432
106,507
112,199

9.35
11.39
12.00

10

4.1.2. Statistical Significance Testing
Approach. To assess the statistical significance of these observed differences in relative edit distance with
sequence length, pairwise comparisons were conducted using the Wilcoxon signed-rank test at a significance
level of α = 0.05. Multiple test correction was also performed using the Benjamini–Hochberg method [51]
to control for the false discovery rate. The effect size (r) was also calculated to quantify the practical
magnitude of the observed differences, independent of sample size. Values of |r| < 0.1 indicate a negligible
effect, 0.1 ≤ |r| < 0.3 a small effect, 0.3 ≤ |r| < 0.5 a medium effect, and |r| ≥ 0.5 a large effect.
All pairwise comparisons of sequence length on relative edit distance (i.e., 256 vs 756, 256 vs 4188, and
756 vs 4188) were statistically significant at α = 0.05 after applying the Benjamini–Hochberg correction.
Despite this, the corresponding effect sizes were negligible in magnitude (i.e., |r| < 0.1), indicating that the
observed differences, while statistically detectable, are of limited practical significance. These results suggest
that the practical impact of sequence length on performance may be negligible for MT in our setting.
4.1.3. Validation Dataset Trends
The analysis was extended to each validation dataset (VA , VB , and VC ), revealing a largely consistent
trend. Specifically, the median DR tends to increase slightly with longer sequence lengths; however, this is
explained not by systematic performance changes, but rather by increased variance. Figure 2 shows these
differences in DR variance, and Appendix A.2 confirms that all statistically significant differences exhibit a
negligible effect size.

Sequence Length

256

756

4188

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
V_A

V_B

V_C

Validation Dataset
Figure 2: Box-plots of the relative edit distance of MT by sequence length and validation dataset.

4.1.4. Discussion
While our findings are consistent across splits, they may not generalize to settings with a different
distribution of sequence length. In our setting, we observe that MT struggles to effectively utilize the
additional information longer sequence lengths provide for the task of automated log parsing with the given
data. The negligible effect sizes imply that longer sequences do not meaningfully improve performance on
this task. Given that self-attention scales quadratically in memory and time with sequence length, the
computational cost of longer sequences may outweigh any benefits. That said, environments with a higher
proportion of long sequences may yield different results.

11

4.2. RQ2: How do different tokenization methods affect performance?
Sequence representation is a key consideration in all sequence modelling tasks. To evaluate the impact of
log string tokenization on DR for log parsing, we will analyze both character- and word-based tokenization.
Statistical analyses and descriptive summaries are computed across training datasets T(·) , validation datasets
V(·) , and different training data sampling levels (10%, 50%, and 100%) to assess both the magnitude and
significance of performance differences. Since MT is only trained with word tokenization, MT results will be
omitted from this analysis for the sake of more direct comparisons with MB , ML , and MM .
Approach. Since character- and word-based tokenization produce outputs at different levels of granularity,
direct comparison would not be appropriate. To address this, predicted word sequences are post-processed
through an inverse tokenization procedure that removes artificial separators, normalizes tokens, and reconstructs a continuous character string structurally aligned with the original log format. This ensures that the
edit distance reflects true parsing performance rather than tokenization effects.
4.2.1. Descriptive Statistics
For all model architectures considered, across all experimental configurations, character-level tokenization
achieves superior performance. Table 9 shows a consistent increase in mean and median edit distance
when using word-level tokenization, while maintaining near-identical standard deviations. These descriptive
trends remain consistent when stratifying model configurations according to training datasets T(·) , validation
datasets V(·) , and varying training data sampling levels (see Appendix A.3).
Table 9: Descriptive statistics measured in relative edit distance DR and the number of outliers across tokenization methods
and model architectures.

Model

Tokenization

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

MB
MB
ML
ML
MM
MM

char
word
char
word
char
word

0.21
0.37
0.23
0.37
0.08
0.15

0.17
0.37
0.21
0.37
0.04
0.14

0.09
0.26
0.11
0.27
0.01
0.06

0.28
0.46
0.33
0.48
0.13
0.22

0.16
0.15
0.15
0.16
0.09
0.11

31,769
10,262
9,205
6,918
26,173
12,427

3.40
1.10
0.98
0.74
2.80
1.33

4.2.2. Statistical Significance Testing
To validate the statistical significance and magnitude of median performance difference between tokenization methods across all model configurations, we again employ a Wilcoxon signed-rank test protocol
(as described in Section 4.1.2). Averaged across all experimental configurations, statistical testing reaffirms
both the significance (α = 0.05) and “large” effect (|r| ≥ 0.5) of the tokenization method on median relative
edit distance.
With few exceptions, stratified model configurations also show consistent statistical significance and large
effect (see Appendix A.3). The exceptions include the ML models with only 10% of training data used, which
exhibit a “medium” effect size (0.3 ≤ |r| < 0.5), and certain models trained with TM and TH . Next, we will
further explore the exceptions in the training dataset configurations.
4.2.3. Training Dataset Trends
Figure A.5 depicts the relative edit distance by tokenization method and training dataset for each model
architecture. The outcome of our Wilcoxon test suggests a qualitative difference in the effect size (ranging
from “small” to “medium”) for ML and MB models trained using TM and TH datasets (see Appendix A.3).
The smaller effect size observed for TM and TH suggests that the advantage of character-level tokenization diminishes as format diversity and randomness increase. To further explore tokenization method effects,
Section 4.2.4 examines out-of-vocabulary (OOV) rates and vocabulary coverage across datasets and tokenization schemes. Next, we will examine the outsized impact that word tokenization has on TT training dataset
results.

12

Tokenization Method
M_B

char

word

M_L

M_M

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
T_E

T_H

T_M

T_T

T_E

T_H

T_M

T_T

T_E

T_H

T_M

T_T

Training Dataset
Figure 3: Box-plots of the relative edit distance by tokenization method and training dataset for each model architecture.

Trivial Dataset Analysis. In Figure 3, we observe that differences in median DR between word and character
tokenization are 2-5× larger for TT and TE than TM and TH (see Appendix A.3.2 for more details). One
exception to this is MM with TT , whose median performance degrades minimally between tokenization
methods.
Upon further inspection of training and test examples in TT and V(·) , various syntactical differences were
found. The following differences are noted: missing square brackets around timestamp fields and missing
double quotes wrapping the log strings (see Appendix A.3.2 for a log string example). While syntactical
differences were found in TT , the log record format still followed the Extended Log Format (ELF) as described
in Table 6.
To test the impact of these syntactical differences on model performance, we reran model inference on
all validation datasets after standardizing the timestamp and log string syntax (denoted as V ∗). This does
not require retraining the underlying model, allowing us to compare the impact of word tokenization with
and without syntactical differences between the training and test sets.
Table 10 shows that DR decreases consistently across all models when evaluating on V ∗. Additionally,
we can see that MT performed best even with non-representative training examples when compared to the
recurrent models.
Table 10: Descriptive statistics measured in relative edit distance DR for models evaluated on the V and V* datasets using
word-level tokenization and 100% of TT . MT uses a sequence length of 256.

Model

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

MB
MB
ML
ML
MT
MT

V
V*
V
V*
V
V*

0.40
0.15
0.43
0.31
0.11
0.03

0.41
0.14
0.43
0.38
0.08
0.00

0.31
0.08
0.39
0.12
0.06
0.00

0.49
0.21
0.51
0.43
0.11
0.01

0.14
0.10
0.12
0.18
0.08
0.07

175
220
1429
10
1898
3233

0.82
1.04
6.72
0.05
8.93
15.21

13

4.2.4. Out of Vocabulary
An analysis of the OOV token rate can help further contextualize the differences in tokenization methods
across datasets. Table 11 shows the number of OOV tokens in relation to the total number of tokens, as
well as the number of unique tokens, for each dataset and tokenization method. Notice that character-based
tokenization does not result in any OOV tokens since the number of unique tokens is less than our maximum
vocabulary size of 15,000. While our word tokenization method makes a trade-off between the length of
input logs and the size of the vocabulary, it also introduces OOV tokens.
The difficulty of each training set (as detailed in Table 6) does not necessarily take into account the
differences in OOV token occurrences with word tokenization. A large number of OOV tokens may lead to
degraded performance for certain datasets.
Table 11: OOV Token Summary by Dataset and Tokenization Method

Dataset

OOV Count

Token Count

OOV %

Unique OOV

Unique Tokens

TT -char
TT -word
TE -char
TE -word
TM -char
TM -word
TH -char
TH -word

0
1,444,752
0
265,589
0
1,249,397
0
1,537,716

117,300,000
34,000,000
29,260,000
7,980,000
152,700,000
40,000,000
152,800,000
40,700,000

0.0
4.2
0.0
3.3
0.0
3.1
0.0
3.9

0
1,302,427
0
263,880
0
1147343
0
1,427,386

79
1,317,427
81
278,880
81
1,162,343
81
1,442,386

4.2.5. Discussion
Overall, character-level tokenization performs better than word-level tokenization on DR across most
settings, and the aggregate tests indicate that this difference is statistically reliable. The advantage appears
smaller for TM and TH , where format diversity increases. We also observe non-zero OOV rates (about 3-4%)
for word tokenization in these datasets (as shown in Table 11).
It is important to note that edit distance operates at the character level, which introduces an inherent
interaction between the evaluation metric and the representation choice. As a result, character-level tokenization is structurally advantaged under this metric, since small boundary or delimiter mismatches at
the word level may incur multiple character edits even when the semantic field assignment is largely correct. Consequently, the observed superiority of character-based tokenization should be interpreted in the
context of this metric choice, rather than as a universal property of tokenization strategies for log parsing.
Complementary evaluation metrics that operate at the field or span level (e.g., field-level accuracy or segment overlap) could further illuminate cases in which word-level tokenization preserves semantic correctness
despite a higher character-level edit distance.
Taken together, these results suggest that character tokenization is a reasonable choice when architectural
constraints permit its use, while word tokenization remains viable for diverse datasets. However, the observed
performance differences may not generalize to other domains, datasets, or evaluation protocols. Furthermore,
subword tokenization could offer a balanced compromise between the granularity of character-level representations and the efficiency of word-level approaches. Exploring subword-based methods, therefore, represents
a sensible direction for future work.
4.3. RQ3: How does sample efficiency vary across model architectures?
We will now examine how performance changes as the available training data increases (10%, 50%, 100%),
aggregating over seeds for partial datasets and evaluating DR across all validation sets. Guided by RQ1 and
RQ2, we restrict the following analysis to sequence length 256 for MT and adopt character tokenization for
MB , ML , and MM .
4.3.1. Descriptive Statistics
Table 12 shows that for MM and MT , the mean DR remain stable with deviations of only ±0.02.
Conversely, MB and ML exhibit a monotonic decrease in median DR as the percentage of training data used
14

increases. To validate that the differences in median DR are not attributable to distributional variance, we
will again employ statistical testing.
Table 12: Descriptive statistics measured in relative edit distance DR and the number of outliers across maximum observation
percentages and model architectures.

Model
MB
MB
MB
ML
ML
ML
MM
MM
MM
MT
MT
MT

Training %

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%‡

10
50
100
10
50
100
10
50
100
10
50
100

0.22
0.20
0.22
0.26
0.22
0.19
0.08
0.09
0.09
0.15
0.14
0.16

0.18
0.16
0.10
0.24
0.18
0.16
0.04
0.04
0.04
0.09
0.09
0.13

0.11
0.09
0.07
0.14
0.09
0.09
0.01
0.01
0.01
0.05
0.05
0.07

0.29
0.27
0.33
0.36
0.32
0.23
0.13
0.13
0.15
0.18
0.18
0.20

0.15
0.16
0.21
0.15
0.15
0.14
0.09
0.09
0.09
0.18
0.16
0.15

14,101
15,295
2,809
5,183
4,571
4,948
11,603
13,059
1,205
42,732
37,996
6,491

3.32
3.60
3.30
1.22
1.08
5.82
2.73
3.07
1.42
10.05
8.94
7.63

‡ Outlier percentages for 10% and 50% configurations are based on 5 resampled runs each, while 100% configurations are based

on a single run. Percentages in Table 12 are therefore not all directly comparable across training levels.

4.3.2. Statistical Significance Testing
Approach. In addition to following the testing procedure outlined in Section 4.1.2, we will apply additional
preprocessing to enable statistical testing between models trained with 100% of T(·) and those with only
10% or 50%. Since models trained with partial training datasets are sampled five times (as described in
Section 3.2), the number of test observations is not constant between 10%, 50%, and 100% variants. To
reconcile the number of observations before statistical testing, we calculate the median edit distance for each
evaluation log parsing example in the 10% and 50% model variants.
All tests, with the exception of MB , 50% versus 100%, show statistically significant results with varying
effect sizes (see Appendix A.4 for more details). Both MM and MT show a negligible effect size when going
from 10% to 50%, with only a small effect for MM and a medium effect for MT in the remaining tests (i.e.,
10% versus 100% and 50% versus 100%). MB exhibits a small effect size for both 10% versus 50% and 10%
versus 100% tests. Finally, ML show a medium effect for 10% versus 50%, a large effect for 10% versus 100%,
and a small effect for 50% versus 100%. Overall, these results suggest that recurrent models benefit most
from additional training data, while state-space and transformer architectures achieve stable performance
even under limited data conditions.
4.3.3. Discussion
The statistical and descriptive analyses together indicate that model architectures differ substantially
in their data efficiency. The stability of MM and MT across all training proportions suggests that both
architectures can generalize effectively with limited data availability. In contrast, MB and ML exhibit
stronger sensitivity to the quantity of data, showing clear improvements as the proportion of training data
increases.
4.4. RQ4: Which model architecture performs best?
Having examined the effects of sequence length, tokenization, and training data availability in the preceding research questions, we now turn to the central question of comparative model performance.
To ensure a fair comparison, we control for the factors previously analyzed. That means, ML , MB , and
MM are evaluated using character-level tokenization, while MT is evaluated using word-level tokenization
with a sequence length of 256. All models are trained with 100% of their respective training datasets.

15

This section examines per-dataset generalization: how each architecture trained on a given dataset (TT ,
TE , TM , or TH ) performs across all validation sets. This analysis provides insight into architectural robustness, highlighting which models maintain strong performance when exposed to log formats that differ from
their training distribution.
4.4.1. Descriptive Statistics
Table 13 summarizes the DR distributions for each model architecture across training datasets TT , TE ,
TM , and TH . For the simpler datasets (TT and TE ), MM achieves the lowest mean and median DR by a
wide margin, with low variability and median values near zero.
As the training datasets increase in complexity (TM and TH ), the relative performance rankings shift.
For TM , MM continues to outperform, while MT narrows the gap. On the most challenging dataset (TH ),
MT attains the best overall median DR (0.09), suggesting robustness under irregular log formats.
Table 13: Descriptive statistics measured in relative edit distance DR and the number of outliers across model architectures
and training datasets. Bold values denote the lowest mean and median DR for each training dataset group.

Dataset

Model

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

TT
TT
TT
TT

MB
ML
MM
MT

0.15
0.15
0.02
0.11

0.07
0.12
0.01
0.08

0.06
0.08
0.01
0.06

0.10
0.16
0.02
0.11

0.20
0.14
0.03
0.08

4190
1843
2408
1898

19.71
8.67
11.33
8.93

TE
TE
TE
TE

MB
ML
MM
MT

0.15
0.14
0.05
0.24

0.07
0.12
0.02
0.19

0.05
0.05
0.01
0.15

0.09
0.18
0.07
0.27

0.22
0.12
0.07
0.16

4148
1129
905
2124

19.51
5.31
4.26
9.99

TM
TM
TM
TM

MB
ML
MM
MT

0.24
0.26
0.12
0.18

0.19
0.22
0.12
0.14

0.11
0.17
0.02
0.08

0.28
0.37
0.18
0.22

0.17
0.14
0.09
0.17

1276
210
28
2569

6.00
0.99
0.13
12.09

TH
TH
TH
TH

MB
ML
MM
MT

0.34
0.19
0.15
0.12

0.39
0.16
0.17
0.09

0.13
0.08
0.07
0.04

0.44
0.24
0.21
0.18

0.18
0.12
0.09
0.12

32
457
150
718

0.15
2.15
0.71
3.38

4.4.2. Statistical Significance Testing
To assess whether the observed performance differences between architectures are statistically significant,
pairwise Wilcoxon signed-rank tests were conducted for each training dataset (TT , TE , TM , and TH ). Appendix A.5 summarizes the test outcomes, indicating whether differences were statistically significant and
the corresponding effect size magnitude.
For the simpler datasets (TT and TE ), all tests are statistically significant and often exhibit a large or
medium effect. The effect sizes are consistently large when comparing MM with other models, reaffirming
its superiority on datasets with homogeneous log structures. In contrast, differences between MB and MT
are negligible for TT , suggesting comparable performance between these two architectures in this setting.
For TM , performance differences remain statistically significant with a large or medium effect. However,
the effect size between MM and MT is negligible. This indicates that both architectures achieve practically
similar performance when trained with a moderately complex dataset composition.
On the most challenging dataset (TH ), all tests are again statistically significant. All comparisons show a
large effect, with the exception of the comparison between MM and MT , which only yields a medium effect.
This suggests that while Mamba maintains strong performance, the Transformer gains a relative advantage
under log format heterogeneity.

16

4.4.3. Discussion
Although this study does not directly evaluate semantic understanding, the sustained log parsing ability,
with a median DR of 0.09 to 0.12 for medium and hard training datasets, suggests that models generalize
field relationships rather than relying on pure memorization. To further support this suggestion, Section 2.2
Table 5 confirms that TM and TH do not contain any ELF log records.
Additionally, MM achieves the lowest DR with structured training datasets, such as TT and TE , indicating
that it models consistent log formats efficiently and with low variability. As the diversity of log formats
increases, MT achieves or outperforms the performance of all other models, suggesting a greater ability
to generalize from random log formats. Both recurrent architectures exhibit declining performance as log
formats become more varied.
From an economic and operational perspective, the compute-accuracy trade-off is also significant. As
shown in Table 7, Mamba consistently falls in the low-cost range for both training and inference, while
Transformer costs are two to three orders of magnitude higher. This difference can be critical in production
environments with tight computation or budget constraints.
Overall, these findings indicate that Mamba and Transformer models perform best across the evaluated
settings, each offering distinct advantages. Notably, Mamba’s comparatively low computational cost may
still enable its use for heterogeneous logs where efficiency is a key constraint.
4.5. Threats to Validity
Following established guidance in empirical research [52, 53], we discuss threats to conclusion, internal,
construct, and external validity.
4.5.1. Conclusion Validity
Conclusion validity concerns whether our statistical claims are supported by the data, which may be
threatened by low statistical power and metric sensitivity.
To reduce statistical risks, we complement p-values with effect sizes, apply non-parametric Wilcoxon
tests for pairwise comparisons, and use p-value corrections to guard against inflated false discovery rates.
We repeat all stochastic procedures with ten different random seeds and consistently apply standardized
preprocessing pipelines to improve reliability. To address metric sensitivity, we normalize inputs before
comparison and report both absolute and relative edit distances, thereby reducing artifacts from whitespace
or tokenization differences and mitigating scale sensitivity.
4.5.2. Internal Validity
Internal validity addresses whether observed differences can be causally attributed to our treatments
rather than to implementation errors or environmental variation.
To reduce the risk of implementation errors, all experimental code was subjected to peer review and supported by unit tests for core components. To stabilize instrumentation, we version-controlled both software
and hardware configurations via Git [54] and scheduled experiments with SLURM [50], ensuring reproducibility across runs and minimizing the influence of hidden confounders.
4.5.3. Construct Validity
Construct validity asks whether our measures accurately reflect parsing ability, which may be challenged
by our reliance on edit distance rather than binary parsing accuracy.
We chose edit distance because it captures both exactness and partial correctness, providing a more
nuanced view of parsing quality in diverse log formats. While binary parsing accuracy is common in the
literature, prior work has also employed edit distance [26], which supports our approach. Furthermore, to
mitigate potential scaling biases, we report edit distance in both absolute and relative terms.
4.5.4. External Validity
External validity concerns the extent to which findings generalize beyond our study (including changes
to datasets, architectures, and tokenization strategies).
We conducted an extensive empirical analysis using an experimental sweep of four training dataset difficulties, three model architectures, two tokenization methods, three training data percentages, and evaluation on

17

three real-world validation datasets, resulting in a total of 396 individually trained models. This experimental setup provides evidence for generalization to Apache-style web server logs, as well as format mismatches
between training and deployment. Generalization to logs with substantially different field cardinalities and
length distributions should be treated with caution (as is the case for the majority of empirical findings
in software engineering due to the variability of real-world environments [55]). Although we characterize
automated log parsing under a large variety of conditions, we do not claim universality across all logging
ecosystems or training recipes. However, the experimental framework presented is reproducible and adaptable, providing a template for well-designed experiments to evaluate new log sources, representations, and
model architectures.
5. Conclusion
This study presented a systematic empirical evaluation of sequence-to-sequence deep learning architectures for automated log parsing, addressing four research questions spanning sequence length, tokenization strategy, sample efficiency, and model architecture. Relative edit distance was used as the primary
evaluation metric, and statistical conclusions were supported through Wilcoxon signed-rank testing with
multiple-comparison correction and effect size analysis.
First, we showed that sequence length has negligible practical impact on Transformer parsing performance
in our experimental setting. Although differences were statistically detectable, effect sizes were consistently
negligible, indicating that longer input sequences do not translate into meaningful parsing improvements for
the evaluated workloads. Given the quadratic computational scaling of self-attention, these results suggest
that increasing sequence length is unlikely to be cost-effective for log parsing under similar data distributions.
Second, character-level tokenization generally produced superior performance compared to word-level
tokenization across architectures. However, this advantage decreases as log format diversity increases, highlighting an interaction between representation granularity and structural variability in logs. Importantly, the
results should be interpreted in light of the character-level evaluation metric, suggesting that future work
should explore field-level or span-level metrics to better capture semantic correctness.
Third, we observed substantial differences in sample efficiency across architectures. Mamba and Transformer models maintained relatively stable performance even with limited training data, whereas recurrent
models benefited more strongly from increased data availability. This finding suggests that modern sequence
architectures may reduce data requirements for practical deployment scenarios where labelled logs are scarce.
Finally, comparative architectural analysis revealed that Mamba and Transformer models represent the
strongest overall choices, but under different operating conditions. Mamba consistently performed best
on structured or moderately variable log datasets and offered significantly lower computational cost. In
contrast, Transformers demonstrated stronger robustness under highly heterogeneous or distribution-shifted
log formats. From an operational perspective, this highlights a practical compute-accuracy trade-off: Mamba
provides strong performance in cost-constrained environments, while Transformers may be preferred when
maximum robustness is required.
Taken together, these results provide a structured empirical foundation for selecting sequence modelling
approaches for log parsing. In this work, generalization is defined in terms of syntactic and structural
variation in log formats, including field ordering, presence, and formatting changes, rather than semantic
or cross-domain transfer. The experimental framework also provides a reproducible template for evaluating
future architectures under controlled distribution-shift conditions.
Several research directions remain open. Hybrid architectures combining structured sequence modelling
with pretrained foundation models may further improve robustness. Subword tokenization approaches (e.g.,
Byte Pair Encoding [56], WordPiece [57], and SentencePiece [58]) offer a promising middle ground between
character and word representations. Additionally, modelling temporal dependencies across log streams rather
than parsing records independently may better reflect real-world production monitoring pipelines. Finally,
systematic evaluation of inference latency, energy cost, and deployment-scale efficiency will be critical for
translating model advances into production-ready log analytics systems.

18

Acknowledgments
This work was partially supported by the Natural Sciences and Engineering Research Council of Canada
(grant # RGPIN-2022-03886). The authors thank the Digital Research Alliance of Canada for providing
computational resources.
References
[1] D. Yuan, H. Mai, W. Xiong, L. Tan, Y. Zhou, S. Pasupathy, Sherlog: error diagnosis by connecting clues
from run-time logs, in: J. C. Hoe, V. S. Adve (Eds.), Proceedings of the 15th International Conference on
Architectural Support for Programming Languages and Operating Systems, ASPLOS 2010, Pittsburgh,
Pennsylvania, USA, March 13-17, 2010, ACM, 2010, pp. 143–154. doi:10.1145/1736020.1736038.
URL https://doi.org/10.1145/1736020.1736038
[2] S. Lu, B. Rao, X. Wei, B. Tak, L. Wang, L. Wang, Log-based abnormal task detection and root cause
analysis for spark, in: I. Altintas, S. Chen (Eds.), 2017 IEEE International Conference on Web Services,
ICWS 2017, Honolulu, HI, USA, June 25-30, 2017, IEEE, 2017, pp. 389–396. doi:10.1109/ICWS.201
7.135.
URL https://doi.org/10.1109/ICWS.2017.135
[3] A. Das, F. Mueller, C. Siegel, A. Vishnu, Desh: deep learning for system health prediction of lead times to
failure in HPC, in: M. Zhao, A. Chandra, L. Ramakrishnan (Eds.), Proceedings of the 27th International
Symposium on High-Performance Parallel and Distributed Computing, HPDC 2018, Tempe, AZ, USA,
June 11-15, 2018, ACM, 2018, pp. 40–51. doi:10.1145/3208040.3208051.
URL https://doi.org/10.1145/3208040.3208051
[4] G. Lee, J. Lin, C. Liu, A. Lorek, D. V. Ryaboy, The unified logging infrastructure for data analytics at
twitter, Proc. VLDB Endow. 5 (12) (2012) 1771–1780. doi:10.14778/2367502.2367516.
URL http://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf
[5] M. Du, F. Li, G. Zheng, V. Srikumar, Deeplog: Anomaly detection and diagnosis from system logs
through deep learning, in: B. Thuraisingham, D. Evans, T. Malkin, D. Xu (Eds.), Proceedings of the
2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017, Dallas, TX,
USA, October 30 - November 03, 2017, ACM, 2017, pp. 1285–1298. doi:10.1145/3133956.3134015.
URL https://doi.org/10.1145/3133956.3134015
[6] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li, J. Chen,
X. He, R. Yao, J. Lou, M. Chintalapati, F. Shen, D. Zhang, Robust log-based anomaly detection on
unstable log data, in: M. Dumas, D. Pfahl, S. Apel, A. Russo (Eds.), Proceedings of the ACM Joint
Meeting on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, ACM, 2019, pp. 807–
817. doi:10.1145/3338906.3338931.
URL https://doi.org/10.1145/3338906.3338931
[7] B. Zhang, H. Zhang, P. Moscato, A. Zhang, Anomaly detection via mining numerical workflow relations
from logs, in: International Symposium on Reliable Distributed Systems, SRDS 2020, Shanghai, China,
September 21-24, 2020, IEEE, 2020, pp. 195–204. doi:10.1109/SRDS51746.2020.00027.
URL https://doi.org/10.1109/SRDS51746.2020.00027
[8] M. S. Islam, W. Pourmajidi, L. Zhang, J. Steinbacher, T. Erwin, A. V. Miranskyy, Anomaly detection
in a large-scale cloud platform, in: 43rd IEEE/ACM International Conference on Software Engineering:
Software Engineering in Practice, ICSE (SEIP) 2021, Madrid, Spain, May 25-28, 2021, IEEE, 2021, pp.
150–159. doi:10.1109/ICSE-SEIP52600.2021.00024.
URL https://doi.org/10.1109/ICSE-SEIP52600.2021.00024

19

[9] M. S. Islam, M. S. Rakha, W. Pourmajidi, J. Sivaloganathan, J. Steinbacher, A. V. Miranskyy, Anomaly
detection in large-scale cloud systems: An industry case and dataset, in: 47th IEEE/ACM International
Conference on Software Engineering: Software Engineering in Practice, SEIP@ICSE 2025, Ottawa, ON,
Canada, April 27 - May 3, 2025, IEEE, 2025, pp. 377–388. doi:10.1109/ICSE-SEIP66354.2025.00039.
URL https://doi.org/10.1109/ICSE-SEIP66354.2025.00039
[10] H. Mi, H. Wang, Y. Zhou, M. R. Lyu, H. Cai, Toward fine-grained, unsupervised, scalable performance
diagnosis for production cloud computing systems, IEEE Trans. Parallel Distributed Syst. 24 (6) (2013)
1245–1255. doi:10.1109/TPDS.2013.21.
URL https://doi.org/10.1109/TPDS.2013.21
[11] A. V. Miranskyy, A. Hamou-Lhadj, E. Cialini, A. Larsson, Operational-log analysis for big data systems:
Challenges and solutions, IEEE Softw. 33 (2) (2016) 52–59. doi:10.1109/MS.2016.33.
URL https://doi.org/10.1109/MS.2016.33
[12] W. Pourmajidi, J. Steinbacher, T. Erwin, A. Miranskyy, On challenges of cloud monitoring, in: Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering,
2017, pp. 259–265.
[13] W. Pourmajidi, A. Miranskyy, J. Steinbacher, T. Erwin, D. Godwin, Dogfooding: Using ibm cloud services to monitor ibm cloud infrastructure, in: Proceedings of the 29th Annual International Conference
on Computer Science and Software Engineering, 2019, pp. 344–353.
[14] W. Pourmajidi, L. Zhang, A. Miranskyy, J. Steinbacher, D. Godwin, T. Erwin, The challenging landscape of cloud monitoring, in: Knowledge Management in the Development of Data-Intensive Systems,
CRC Press, 2021, pp. 157–189.
[15] P. He, J. Zhu, Z. Zheng, M. R. Lyu, Drain: An online log parsing approach with fixed depth tree, in: 2017
IEEE International Conference on Web Services (ICWS), 2017, pp. 33–40. doi:10.1109/ICWS.2017.13.
[16] Z. M. Jiang, A. E. Hassan, P. Flora, G. Hamann, Abstracting execution logs to execution events for
enterprise applications (short paper), in: 2008 The Eighth International Conference on Quality Software,
2008, pp. 181–186. doi:10.1109/QSIC.2008.50.
[17] H. Dai, H. Li, C. Chen, W. Shang, T. Chen, Logram: Efficient log parsing using $n$n-gram dictionaries,
IEEE Trans. Software Eng. 48 (3) (2022) 879–892. doi:10.1109/TSE.2020.3007554.
URL https://doi.org/10.1109/TSE.2020.3007554
[18] R. Vaarandi, M. Pihelgas, Logcluster - A data clustering and pattern mining algorithm for event logs,
in: M. Tortonesi, J. Schönwälder, E. R. M. Madeira, C. Schmitt, J. Serrat (Eds.), 11th International
Conference on Network and Service Management, CNSM 2015, Barcelona, Spain, November 9-13, 2015,
IEEE Computer Society, 2015, pp. 1–7. doi:10.1109/CNSM.2015.7367331.
URL https://doi.org/10.1109/CNSM.2015.7367331
[19] L. Tang, T. Li, C.-S. Perng, Logsig: generating system events from raw textual logs, in: Proceedings
of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11,
Association for Computing Machinery, New York, NY, USA, 2011, p. 785–794. doi:10.1145/206357
6.2063690.
URL https://doi.org/10.1145/2063576.2063690
[20] M. Du, F. Li, Spell: Online streaming parsing of large unstructured system logs, IEEE Transactions on
Knowledge and Data Engineering 31 (11) (2019) 2213–2227. doi:10.1109/TKDE.2018.2875442.
[21] H. Guo, S. Yuan, X. Wu, Logbert: Log anomaly detection via bert, in: 2021 International Joint
Conference on Neural Networks (IJCNN), 2021, pp. 1–8. doi:10.1109/IJCNN52387.2021.9534113.
[22] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, O. Kao, Self-supervised log parsing, in: Y. Dong,
D. Mladenić, C. Saunders (Eds.), Machine Learning and Knowledge Discovery in Databases: Applied
Data Science Track, Springer International Publishing, Cham, 2021, pp. 122–138.
20

[23] Z. Ma, A. R. Chen, D. J. Kim, T.-H. Chen, S. Wang, Llmparser: An exploratory study on using large
language models for log parsing, in: Proceedings of the IEEE/ACM 46th International Conference on
Software Engineering, ICSE ’24, Association for Computing Machinery, New York, NY, USA, 2024.
doi:10.1145/3597503.3639150.
URL https://doi.org/10.1145/3597503.3639150
[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, I. Polosukhin,
Attention is all you need, in: I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 30, Curran
Associates, Inc., 2017.
URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd05
3c1c4a845aa-Paper.pdf
[25] V. Bertalan, D. Aloise, Using transformer models and textual analysis for log parsing, in: 2023 IEEE
34th International Symposium on Software Reliability Engineering (ISSRE), 2023, pp. 367–378. doi:
10.1109/ISSRE59848.2023.00037.
[26] J. Rand, A. Miranskyy, On automatic parsing of log records, in: 2021 IEEE/ACM 43rd International
Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), 2021, pp. 41–45.
doi:10.1109/ICSE-NIER52604.2021.00017.
[27] A. Gu, K. Goel, C. Ré, Efficiently modeling long sequences with structured state spaces, CoRR
abs/2111.00396 (2021). arXiv:2111.00396.
URL https://arxiv.org/abs/2111.00396
[28] M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter,
S. Hochreiter, xlstm: Extended long short-term memory, in: A. Globersons, L. Mackey, D. Belgrave,
A. Fan, U. Paquet, J. M. Tomczak, C. Zhang (Eds.), Advances in Neural Information Processing Systems
38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC,
Canada, December 10 - 15, 2024, 2024.
URL http://papers.nips.cc/paper_files/paper/2024/hash/c2ce2f2701c10a2b2f2ea0bfa43cf
aa3-Abstract-Conference.html
[29] F. Danieli, P. Rodriguez, M. Sarabia, X. Suau, L. Zappella, Pararnn: Unlocking parallel training of
nonlinear rnns for large language models (2025). arXiv:2510.21450.
URL https://arxiv.org/abs/2510.21450
[30] A. Gu, T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, CoRR
abs/2312.00752 (2023). arXiv:2312.00752, doi:10.48550/ARXIV.2312.00752.
URL https://doi.org/10.48550/arXiv.2312.00752
[31] A. Pecchia, M. Cinque, G. Carrozza, D. Cotroneo, Industry practices and event logging: Assessment of
a critical software development process, in: 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering, Vol. 2, 2015, pp. 169–178. doi:10.1109/ICSE.2015.145.
[32] J. Zhu, S. He, J. Liu, P. He, Q. Xie, Z. Zheng, M. R. Lyu, Tools and benchmarks for automated
log parsing, in: 2019 IEEE/ACM 41st International Conference on Software Engineering: Software
Engineering in Practice (ICSE-SEIP), 2019, pp. 121–130. doi:10.1109/ICSE-SEIP.2019.00021.
[33] Apache HTTP Server Project, Apache module mod_log_config, available at https://httpd.apache
.org/docs/current/mod/mod_log_config.html (2025).
URL https://httpd.apache.org/docs/current/mod/mod_log_config.html
[34] J. Zhu, S. He, P. He, J. Liu, M. R. Lyu, Loghub: A large collection of system log datasets for ai-driven log
analytics, in: 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE),
2023, pp. 355–366. doi:10.1109/ISSRE59848.2023.00071.

21

[35] S. Messaoudi, A. Panichella, D. Bianculli, L. Briand, R. Sasnauskas, A search-based approach for
accurate identification of log message formats, in: Proceedings of the 26th Conference on Program
Comprehension, 2018, pp. 167–177.
[36] K. Cho, B. van Merrienboer, Ç. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio, Learning
phrase representations using RNN encoder-decoder for statistical machine translation, in: A. Moschitti,
B. Pang, W. Daelemans (Eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a
Special Interest Group of the ACL, ACL, 2014, pp. 1724–1734. doi:10.3115/V1/D14-1179.
URL https://doi.org/10.3115/v1/d14-1179
[37] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with neural networks (2014). arXiv:
1409.3215.
URL https://arxiv.org/abs/1409.3215
[38] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997) 1735–1780.
doi:10.1162/NECO.1997.9.8.1735.
URL https://doi.org/10.1162/neco.1997.9.8.1735
[39] S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, et al., Gradient flow in recurrent nets: the
difficulty of learning long-term dependencies (2001).
[40] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate,
in: Y. Bengio, Y. LeCun (Eds.), 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
URL http://arxiv.org/abs/1409.0473
[41] Y. Kim, C. Denton, L. Hoang, A. M. Rush, Structured attention networks, in: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings, OpenReview.net, 2017.
URL https://openreview.net/forum?id=HkE0Nvqlg
[42] Y. Tay, M. Dehghani, D. Bahri, D. Metzler, Efficient transformers: A survey, ACM Comput. Surv.
55 (6) (2023) 109:1–109:28. doi:10.1145/3530811.
URL https://doi.org/10.1145/3530811
[43] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, C. Ré, Combining recurrent, convolutional,
and continuous-time models with linear state-space layers, CoRR abs/2110.13985 (2021). arXiv:2110
.13985.
URL https://arxiv.org/abs/2110.13985
[44] A. Gu, T. Dao, mamba-ssm (ver. 1.2.0.post1), https://pypi.org/project/mamba-ssm/1.2.0.post1/
(Mar. 2024).
[45] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Y. Bengio, Y. LeCun (Eds.),
3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015.
URL http://arxiv.org/abs/1412.6980
[46] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, et al., Tensorflow: Large-scale machine learning on heterogeneous distributed systems, arXiv
preprint arXiv:1603.04467 (2016).
[47] F. Chollet, et al., Keras, https://keras.io (2015).
[48] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, et al., Pytorch: An imperative style, high-performance deep learning library, Advances in
neural information processing systems 32 (2019).

22

[49] O. Yadan, Hydra - a framework for elegantly configuring complex applications, Github (2019).
URL https://github.com/facebookresearch/hydra
[50] A. B. Yoo, M. A. Jette, M. Grondona, Slurm: Simple linux utility for resource management, in: Workshop on job scheduling strategies for parallel processing, Springer, 2003, pp. 44–60.
[51] Y. Benjamini, Y. Hochberg, Controlling the false discovery rate: a practical and powerful approach
to multiple testing, Journal of the Royal statistical society: series B (Methodological) 57 (1) (1995)
289–300.
[52] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, A. Wesslén, Experimentation in Software
Engineering, Springer, 2012. doi:10.1007/978-3-642-29044-2.
URL https://doi.org/10.1007/978-3-642-29044-2
[53] R. K. Yin, Case study research: Design and methods, Vol. 5, sage, 2009.
[54] The Git Development Team, Git: Fast, distributed version control system, https://git-scm.com/
(2005).
[55] R. Wieringa, M. Daneva, Six strategies for generalizing software engineering theories, Science of computer programming 101 (2015) 136–152.
[56] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare words with subword units, CoRR
abs/1508.07909 (2015). arXiv:1508.07909.
URL http://arxiv.org/abs/1508.07909
[57] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo,
H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick,
O. Vinyals, G. Corrado, M. Hughes, J. Dean, Google’s neural machine translation system: Bridging the
gap between human and machine translation, CoRR abs/1609.08144 (2016). arXiv:1609.08144.
URL http://arxiv.org/abs/1609.08144
[58] T. Kudo, J. Richardson, Sentencepiece: A simple and language independent subword tokenizer and
detokenizer for neural text processing, CoRR abs/1808.06226 (2018). arXiv:1808.06226.
URL http://arxiv.org/abs/1808.06226

23

Appendix A. Extended Evaluation Results
Appendix A.1. Sequence Length Extended Analysis
The significance analysis in Table A.14 confirms that overall differences are statistically significant but
of negligible magnitude. Figure A.4 depicts the minimal performance differences between sequence lengths
over all experimental configurations.
Table A.14: Wilcoxon test results for sequence length. Only significance and effect size magnitude are reported.

Group 1

Group 2

256
256
756

756
4188
4188

Significant

Magnitude

TRUE
TRUE
TRUE

negligible
negligible
negligible

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
256

756

4188

Sequence Length
Figure A.4: Box-plots of the relative edit distance of MT by sequence length.

Appendix A.2. Validation Dataset Trends
Table A.15 shows the full descriptive statistics for changes in sequence length across each validation
dataset. As discussed in Section 4.1.3, these trends are consistent with increased parsing error for higher
sequence length due to more varied model performance.

24

Table A.15: Descriptive statistics measured in relative edit distance DR and the number of outliers across validation datasets
and sequence lengths.

Seq. Length
256
756
4188
256
756
4188
256
756
4188

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

VA
VA
VA
VB
VB
VB
VC
VC
VC

0.14
0.19
0.18
0.13
0.19
0.17
0.17
0.17
0.20

0.07
0.08
0.07
0.07
0.08
0.07
0.14
0.14
0.14

0.03
0.01
0.01
0.05
0.03
0.03
0.08
0.08
0.08

0.15
0.21
0.18
0.09
0.16
0.10
0.21
0.22
0.25

0.19
0.26
0.27
0.19
0.28
0.27
0.13
0.15
0.19

28780
34922
40182
39595
48513
52569
31795
32810
39769

11.68
14.17
16.30
15.86
19.43
21.06
7.24
7.47
9.06

Table A.16: Wilcoxon test results by validation dataset.

Dataset
VA
VA
VA
VB
VB
VB
VC
VC
VC

Group 1

Group 2

256
256
756
256
256
756
256
256
756

4188
756
4188
4188
756
4188
4188
756
4188

Significant

Magnitude

TRUE
TRUE
FALSE
FALSE
TRUE
TRUE
TRUE
TRUE
TRUE

negligible
negligible
negligible
negligible
negligible
negligible
negligible
negligible
negligible

Appendix A.2.1. Training Dataset Trends
Across most training datasets, increasing the sequence length resulted in minimal changes in variability
and median performance (Table A.17). Further analysis of the Wilcoxon test results in Table A.18 shows
the differences in median performance are statistically significant and exhibit meaningful effect size for TE ,
TM , and TT .
A notable observation for TT shows both the median and variance increase substantially at 756 and 4188
tokens. This deviation is clearly visible in Figure A.5, indicating that TT is more sensitive to sequence length
than the other training datasets.

25

Table A.17: Descriptive statistics measured in relative edit distance DR and the number of outliers across training datasets
and sequence lengths.

Seq. Length
256
756
4188
256
756
4188
256
756
4188
256
756
4188

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

TE
TE
TE
TH
TH
TH
TM
TM
TM
TT
TT
TT

0.11
0.09
0.08
0.14
0.14
0.18
0.15
0.14
0.13
0.19
0.36
0.37

0.07
0.04
0.04
0.11
0.10
0.11
0.10
0.09
0.09
0.10
0.23
0.30

0.03
0.01
0.01
0.07
0.05
0.06
0.05
0.03
0.04
0.07
0.10
0.09

0.15
0.11
0.10
0.19
0.19
0.20
0.19
0.18
0.18
0.19
0.57
0.57

0.13
0.13
0.11
0.13
0.13
0.22
0.16
0.15
0.13
0.22
0.31
0.31

13112
21799
20553
13743
12425
27221
22646
18642
12289
37612
0
0

5.61
9.32
8.79
5.88
5.31
11.64
9.69
7.97
5.26
16.09
0.00
0.00

Table A.18: Wilcoxon test results by training dataset.

Dataset
TT
TT
TT
TE
TE
TE
TM
TM
TM
TH
TH
TH

Group 1

Group 2

256
256
756
256
256
756
256
256
756
256
256
756

4188
756
4188
4188
756
4188
4188
756
4188
4188
756
4188

26

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

medium
medium
negligible
medium
medium
negligible
small
small
negligible
negligible
negligible
negligible

Sequence Length

256

756

4188

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
T_E

T_H

T_M

T_T

Training Dataset
Figure A.5: Box-plots of the relative edit distance of MT by sequence length and training dataset.

Appendix A.2.2. Data Percentage Trends
Similarly, across different training data percentages, longer sequences are associated with greater variability but only minor shifts in median performance (Table A.19). This is confirmed by the statistical testing
results in Table A.20, which show statistically significant but negligible effect sizes, consistent with the global
sequence length trend (Table A.14).
However, at 100% training data, the longest sequence length (4188) shows a noticeable upward shift
in both median error and variability (Figure A.6), representing a meaningful deviation from the otherwise
stable trend.
Table A.19: Descriptive statistics measured in relative edit distance DR and the number of outliers across training data
percentages and sequence lengths.

Seq. Length

%

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

256
756
4188
256
756
4188
256
756
4188

10
10
10
50
50
50
100
100
100

0.15
0.20
0.16
0.14
0.15
0.18
0.16
0.22
0.36

0.09
0.10
0.09
0.09
0.10
0.10
0.13
0.10
0.30

0.05
0.04
0.04
0.05
0.04
0.04
0.07
0.06
0.06

0.18
0.24
0.17
0.18
0.21
0.24
0.20
0.21
0.63

0.18
0.25
0.22
0.16
0.17
0.22
0.15
0.30
0.31

42732
58013
43042
37996
30309
33959
6491
12602
0

10.05
13.65
10.12
8.94
7.13
7.99
7.63
14.82
0.00

27

Sequence Length

256

756

4188

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
10

50

100

Training Data Percentage
Figure A.6: Box-plots of the relative edit distance of MT by sequence length.

Table A.20: Wilcoxon test results by maximum observation percentage.

Training %

Group 1

Group 2

10
10
10
50
50
50
100
100
100

256
256
756
256
256
756
256
256
756

4188
756
4188
4188
756
4188
4188
756
4188

28

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

small
negligible
small
negligible
negligible
negligible
medium
small
medium

Appendix A.3. Tokenization Method Extended Analysis
Table A.21 shows that all tokenization method tests are statistically significant and exhibit a large effect.
Figure A.7 shows the increase in DR when using word tokenization for all recurrent models.
Table A.21: Wilcoxon test results for tokenization method for each model architecture.

Model

Group 1

Group 2

ML
MB
MM

char
char
char

word
word
word

M_B

Significant

Magnitude

TRUE
TRUE
TRUE

large
large
large

M_L

M_M

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
char

word

char

word

char

word

Tokenization Method
Figure A.7: Box-plots of the relative edit distance by tokenization method for each model architecture.

Appendix A.3.1. Validation Dataset Trends
Table A.22 and Figure A.8 both show consistent out-performance with character-level autoionization
across all three validation datasets. Statistical testing show in Table A.23 confirms the median performance
differences are significant with a large effect.

29

Table A.22: Descriptive statistics measured in relative edit distance DR and the number of outliers across validation datasets,
tokenization methods, and model architectures.

Arch.

Rep.

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outlier

%

MB
MB
MB
MB
MB
MB
ML
ML
ML
ML
ML
ML
MM
MM
MM
MM
MM
MM

char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word

VA
VA
VB
VB
VC
VC
VA
VA
VB
VB
VC
VC
VA
VA
VB
VB
VC
VC

0.23
0.35
0.21
0.33
0.20
0.41
0.23
0.35
0.20
0.33
0.25
0.41
0.08
0.14
0.06
0.10
0.10
0.18

0.19
0.33
0.17
0.32
0.15
0.41
0.21
0.35
0.18
0.33
0.23
0.41
0.04
0.13
0.03
0.09
0.05
0.18

0.11
0.23
0.11
0.24
0.07
0.31
0.11
0.23
0.11
0.23
0.10
0.32
0.02
0.05
0.01
0.05
0.01
0.09

0.29
0.44
0.22
0.42
0.29
0.50
0.32
0.47
0.27
0.44
0.37
0.50
0.13
0.20
0.11
0.14
0.19
0.26

0.16
0.16
0.15
0.13
0.17
0.14
0.15
0.16
0.12
0.14
0.17
0.15
0.08
0.10
0.07
0.08
0.11
0.12

9176
5794
28751
4285
9185
3762
4568
459
3321
846
1810
8424
3399
2498
2542
7005
438
3136

3.72
2.35
11.52
1.72
2.09
0.86
1.85
0.19
1.33
0.34
0.41
1.92
1.38
1.01
1.02
2.81
0.10
0.71

Table A.23: Wilcoxon test results for tokenization method by validation dataset for each model architecture.

Model

Dataset

Group 1

Group 2

ML
ML
ML
MB
MB
MB
MM
MM
MM

VA
VB
VC
VA
VB
VC
VA
VB
VC

char
char
char
char
char
char
char
char
char

word
word
word
word
word
word
word
word
word

30

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

large
large
large
large
large
large
large
large
large

Tokenization Method
M_B

char

word

M_L

M_M

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
V_A

V_B

V_C

V_A

V_B

V_C

V_A

V_B

V_C

Validation Dataset
Figure A.8: Box-plots of the relative edit distance by validation dataset and tokenization method for each model architecture.

Appendix A.3.2. Training Dataset Trends
Table A.24 shows consistently high DR metrics with word tokenization across each training dataset and
model architecture. Table A.25 validates the significance and show most a large and medium effect size with
ML trained on TM exhibiting a small effect.
Trivial Dataset Trends. Section 4.2.3 describes structural inconsistencies between TT and the validation
datasets as shown in Figure A.9. This analysis was motivated by the disproportionate increase in DR
exhibited by the word tokenization model trained on TT and TE models. Table A.26 shows the median
DR for different architectures and training datasets, along with the absolute difference between them. In
addition, we calculate the ratio of these differences to those of TM and TH within the same architecture as
a baseline. For example, MB trained on TT has a median DR difference of 0.41 − 0.08 = 0.33, a TM ratio of
0.328/0.108 = 3.029, and a TH ratio of 0.328/0.115 = 2.856.
T_T: 119.29.60.133 - - 26/Sep/9824:17:05:35 +0800 "OPTIONS 46rpb HTTP/1.1" 400 65690 ...
V_A: "192.168.4.164 - - [22/Dec/2016:15:19:05 +0300] "GET /DVWA/ HTTP/1.1" 200 2020 ... "

Figure A.9: An example log string from both TT and VA which highlights the syntax differences present between the train and
test datasets. A portion of the log strings is truncated for illustrative purposes and denoted by ‘· · · ’.

31

Table A.24: Descriptive statistics measured in relative edit distance DR and the number of outliers across training datasets,
tokenization methods, and model architectures.

Model

Rep.

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

MB
MB
MB
MB
MB
MB
MB
MB
ML
ML
ML
ML
ML
ML
ML
ML
MM
MM
MM
MM
MM
MM
MM
MM

char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word

TE
TE
TH
TH
TM
TM
TT
TT
TE
TE
TH
TH
TM
TM
TT
TT
TE
TE
TH
TH
TM
TM
TT
TT

0.18
0.38
0.27
0.35
0.25
0.33
0.14
0.42
0.17
0.36
0.28
0.36
0.31
0.36
0.17
0.41
0.04
0.14
0.15
0.19
0.13
0.17
0.02
0.11

0.12
0.37
0.24
0.35
0.22
0.33
0.08
0.41
0.13
0.35
0.26
0.36
0.30
0.36
0.15
0.41
0.02
0.12
0.13
0.18
0.11
0.16
0.02
0.05

0.08
0.26
0.15
0.25
0.14
0.23
0.05
0.33
0.09
0.26
0.15
0.22
0.20
0.23
0.06
0.35
0.00
0.08
0.07
0.11
0.04
0.08
0.01
0.03

0.20
0.47
0.34
0.44
0.31
0.43
0.18
0.50
0.21
0.44
0.38
0.50
0.41
0.46
0.27
0.49
0.05
0.18
0.20
0.25
0.18
0.25
0.02
0.18

0.15
0.14
0.17
0.15
0.14
0.15
0.14
0.13
0.12
0.14
0.15
0.19
0.14
0.16
0.14
0.11
0.07
0.10
0.10
0.11
0.09
0.11
0.04
0.11

26510
1713
10264
3928
6891
3070
18088
3231
12823
8191
1329
113
2248
465
2024
2975
17480
9946
2375
3683
798
1762
21818
5175

11.34
0.73
4.39
1.68
2.95
1.31
7.74
1.38
5.48
3.50
0.57
0.05
0.96
0.20
0.87
1.27
7.48
4.25
1.02
1.58
0.34
0.75
9.33
2.21

Table A.25: Wilcoxon test results for tokenization method by training dataset for each model architecture.

Model

Dataset

Group 1

Group 2

ML
ML
ML
ML
MB
MB
MB
MB
MM
MM
MM
MM

TT
TE
TM
TH
TT
TE
TM
TH
TT
TE
TM
TH

char
char
char
char
char
char
char
char
char
char
char
char

word
word
word
word
word
word
word
word
word
word
word
word

32

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

large
large
small
medium
large
large
large
large
large
large
medium
medium

Table A.26: Median relative edit distance (DR ) comparison between character-level and word-level tokenization by training
dataset and model architecture. The difference is calculated as word-level minus character-level. Ratios show the relative
magnitude of each difference compared to TM and TH baselines.

Model

Dataset

MB
MB
MB
MB
ML
ML
ML
ML
MM
MM
MM
MM

TE
TH
TM
TT
TE
TH
TM
TT
TE
TH
TM
TT

DR Char

DR Word

Difference

Ratio to TM

Ratio to TH

0.121
0.237
0.220
0.084
0.133
0.260
0.303
0.146
0.024
0.134
0.113
0.015

0.374
0.352
0.328
0.412
0.346
0.364
0.356
0.414
0.125
0.175
0.159
0.052

0.253
0.115
0.108
0.328
0.213
0.104
0.053
0.268
0.101
0.041
0.045
0.037

2.334
1.061
1.000
3.029
3.999
1.960
1.000
5.044
2.219
0.902
1.000
0.820

2.201
1.000
0.943
2.856
2.040
1.000
0.510
2.573
2.458
1.000
1.108
0.908

Appendix A.3.3. Data Percentage Trends
Table A.27 and Figure A.10 show consistent underperformance of word tokenization across all training
data percentages and model architectures. Table A.28 validates the significance and effect size of the difference in median DR . All tests show a large effect except for ML with 10%, which exhibits a medium
effect.
Table A.27: Descriptive statistics measured in relative edit distance DR and the number of outliers across maximum observation
percentages, tokenization methods, and model architectures.

Model

Rep.

%

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

MB
MB
MB
MB
MB
MB
ML
ML
ML
ML
ML
ML
MM
MM
MM
MM
MM
MM

char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word
char
word

10
10
50
50
100
100
10
10
50
50
100
100
10
10
50
50
100
100

0.22
0.35
0.20
0.38
0.22
0.41
0.26
0.34
0.22
0.40
0.19
0.39
0.08
0.15
0.09
0.15
0.09
0.19

0.18
0.33
0.16
0.39
0.10
0.41
0.24
0.33
0.18
0.41
0.16
0.41
0.04
0.14
0.04
0.13
0.04
0.18

0.11
0.25
0.09
0.29
0.07
0.32
0.14
0.23
0.09
0.31
0.09
0.31
0.01
0.06
0.01
0.06
0.01
0.08

0.29
0.44
0.27
0.48
0.33
0.50
0.36
0.44
0.32
0.50
0.23
0.50
0.13
0.22
0.13
0.21
0.15
0.26

0.15
0.14
0.16
0.16
0.21
0.15
0.15
0.15
0.15
0.15
0.14
0.15
0.09
0.11
0.09
0.11
0.09
0.12

14101
4590
15295
5371
2809
2507
5183
7454
4571
4262
4948
1195
11603
4666
13059
8277
1205
488

3.32
1.08
3.60
1.26
3.30
2.95
1.22
1.75
1.08
1.00
5.82
1.41
2.73
1.10
3.07
1.95
1.42
0.57

33

Table A.28: Wilcoxon test results for tokenization method by training data percentage for each model architecture.

Model

Training %
10
50
100
10
50
100
10
50
100

ML
ML
ML
MB
MB
MB
MM
MM
MM

Group 1

Group 2

char
char
char
char
char
char
char
char
char

word
word
word
word
word
word
word
word
word

Tokenization Method
M_B

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

medium
large
large
large
large
large
large
large
large

char

word

M_L

M_M

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
10

50

100

10

50

100

10

50

100

Training Data Percentage
Figure A.10: Box-plots of the relative edit distance by tokenization method and training data percentage for each model
architecture.

34

Appendix A.4. Sample Efficiency Extended Analysis
Figure A.11 shows the DR of various training dataset percentages for each model architecture. Table A.29
verifies the mixed statistical significance and effect size depending on model architecture.
M_B

M_L

M_M

M_T

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
10

50

100

10

50

100

10

50

100

10

50

100

Training %
Figure A.11: Box-plots of the relative edit distance by training data percentage for each model architecture.

Table A.29: Wilcoxon test results for max observations percentage for each model architecture.

Model
ML
ML
ML
MB
MB
MB
MT
MT
MT
MM
MM
MM

Group 1

Group 2

10
10
50
10
10
50
10
10
50
10
10
50

50
100
100
50
100
100
50
100
100
50
100
100

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
FALSE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

medium
large
small
small
small
negligible
negligible
medium
medium
negligible
small
small

Appendix A.4.1. Validation Dataset Trends
Tables A.30 and A.31 and Figure A.12 show trends consistent with the aggregated results. Specifically,
MM and MT exhibit a higher number of negligible effect sizes. This suggests that the effect of training
dataset percentage is consistent across the validation datasets tested.

35

Table A.30: Descriptive statistics measured in relative edit distance DR and the number of outliers across validation datasets,
maximum observation percentages, and model architectures.

Model
MB
MB
MB
MB
MB
MB
MB
MB
MB
ML
ML
ML
ML
ML
ML
ML
ML
ML
MM
MM
MM
MM
MM
MM
MM
MM
MM
MT
MT
MT
MT
MT
MT
MT
MT
MT

Training %
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC

0.24
0.21
0.22
0.23
0.19
0.20
0.20
0.19
0.22
0.24
0.24
0.20
0.23
0.19
0.15
0.28
0.23
0.20
0.07
0.08
0.09
0.06
0.06
0.07
0.10
0.10
0.09
0.14
0.14
0.14
0.13
0.12
0.14
0.17
0.16
0.19

0.20
0.18
0.09
0.19
0.16
0.09
0.17
0.14
0.12
0.22
0.20
0.17
0.23
0.16
0.15
0.28
0.20
0.15
0.04
0.04
0.06
0.03
0.04
0.03
0.05
0.05
0.03
0.07
0.07
0.11
0.07
0.07
0.08
0.14
0.14
0.16

0.12
0.10
0.07
0.12
0.10
0.08
0.09
0.06
0.06
0.13
0.10
0.10
0.16
0.09
0.11
0.13
0.08
0.06
0.02
0.01
0.02
0.02
0.01
0.02
0.01
0.01
0.01
0.04
0.02
0.06
0.04
0.04
0.07
0.09
0.08
0.10

0.31
0.27
0.33
0.24
0.21
0.27
0.29
0.29
0.34
0.32
0.33
0.23
0.31
0.25
0.17
0.39
0.34
0.30
0.13
0.14
0.14
0.11
0.11
0.12
0.19
0.19
0.18
0.14
0.14
0.19
0.09
0.09
0.16
0.21
0.21
0.23

0.15
0.15
0.20
0.15
0.14
0.20
0.15
0.17
0.22
0.14
0.16
0.13
0.12
0.12
0.07
0.17
0.16
0.16
0.07
0.08
0.08
0.07
0.07
0.08
0.11
0.11
0.10
0.19
0.19
0.14
0.21
0.17
0.16
0.14
0.13
0.14

2114
5578
601
18017
6056
1341
4143
4221
1455
2354
1063
2070
1310
2042
925
1100
1206
513
1117
1847
373
1236
1134
140
406
152
30
12884
14029
1589
15461
18245
2273
16476
13076
2574

1.89
4.98
2.68
15.88
5.34
5.91
2.08
2.11
3.64
2.10
0.95
9.24
1.15
1.80
4.08
0.55
0.60
1.28
1.00
1.65
1.66
1.09
1.00
0.62
0.20
0.08
0.08
11.50
12.52
7.09
13.62
16.08
10.01
8.25
6.55
6.45

36

Table A.31: Wilcoxon test results for max observations percentage by validation dataset for each model architecture.

Model

Dataset

ML
ML
ML
ML
ML
ML
ML
ML
ML
MB
MB
MB
MB
MB
MB
MB
MB
MB
MT
MT
MT
MT
MT
MT
MT
MT
MT
MM
MM
MM
MM
MM
MM
MM
MM
MM

VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC
VA
VA
VA
VB
VB
VB
VC
VC
VC

Group 1

Group 2

10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50

50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100

37

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

negligible
large
small
medium
large
small
medium
large
small
small
small
small
small
small
small
small
negligible
small
negligible
small
medium
negligible
medium
medium
negligible
medium
medium
negligible
large
medium
negligible
medium
medium
small
small
small

Training %
M_B

10

50

M_L

100

M_M

M_T

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
V_A

V_B

V_C

V_A

V_B

V_C

V_A

V_B

V_C

V_A

V_B

V_C

Validation Dataset
Figure A.12: Box-plots of the relative edit distance by validation dataset and training data percentage for each model architecture.

Appendix A.4.2. Training Dataset Trends
Tables A.32 and A.33 and Figure A.13 may suggest that training datasets are disproportionately impacted
by the percentage of data used. For example, both MB trained on TH and MT trained on TE exhibit an
increased median DR with 100% of data.

38

Table A.32: Descriptive statistics measured in relative edit distance DR and the number of outliers across training datasets,
maximum observation percentages, and model architectures.

Model
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT

Training %
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100
10
50
100

Dataset

Mean

Median

Q1

Q3

Std. Dev.

# Outliers

%

TE
TE
TE
TH
TH
TH
TM
TM
TM
TT
TT
TT
TE
TE
TE
TH
TH
TH
TM
TM
TM
TT
TT
TT
TE
TE
TE
TH
TH
TH
TM
TM
TM
TT
TT
TT
TE
TE
TE
TH
TH
TH
TM
TM
TM
TT
TT
TT

0.24
0.12
0.15
0.27
0.27
0.34
0.23
0.27
0.24
0.14
0.13
0.15
0.20
0.15
0.14
0.29
0.28
0.19
0.34
0.30
0.26
0.20
0.15
0.15
0.04
0.04
0.05
0.15
0.15
0.15
0.13
0.13
0.12
0.02
0.02
0.02
0.11
0.09
0.24
0.16
0.13
0.12
0.15
0.14
0.18
0.19
0.21
0.11

0.17
0.10
0.07
0.24
0.23
0.39
0.20
0.25
0.19
0.08
0.11
0.07
0.16
0.11
0.12
0.30
0.26
0.16
0.33
0.28
0.22
0.22
0.11
0.12
0.02
0.02
0.02
0.13
0.13
0.17
0.11
0.11
0.12
0.02
0.02
0.01
0.07
0.06
0.19
0.13
0.10
0.09
0.10
0.11
0.14
0.09
0.11
0.08

0.12
0.07
0.05
0.16
0.15
0.13
0.13
0.18
0.11
0.05
0.04
0.06
0.11
0.07
0.05
0.16
0.16
0.08
0.24
0.18
0.17
0.05
0.06
0.08
0.01
0.00
0.01
0.08
0.06
0.07
0.04
0.05
0.02
0.01
0.01
0.01
0.03
0.01
0.15
0.08
0.05
0.04
0.05
0.05
0.08
0.07
0.07
0.06

0.33
0.13
0.09
0.32
0.32
0.44
0.28
0.33
0.28
0.19
0.17
0.10
0.23
0.18
0.18
0.38
0.38
0.24
0.43
0.39
0.37
0.30
0.24
0.16
0.05
0.05
0.07
0.20
0.21
0.21
0.18
0.19
0.18
0.02
0.02
0.02
0.13
0.13
0.27
0.20
0.18
0.18
0.18
0.19
0.22
0.16
0.27
0.11

0.16
0.10
0.22
0.15
0.19
0.18
0.13
0.13
0.17
0.14
0.12
0.20
0.12
0.12
0.12
0.15
0.15
0.12
0.14
0.14
0.14
0.14
0.12
0.14
0.07
0.07
0.07
0.09
0.10
0.09
0.09
0.09
0.09
0.03
0.04
0.03
0.12
0.12
0.16
0.14
0.12
0.12
0.17
0.14
0.17
0.24
0.21
0.08

1033
10225
4148
4984
7064
32
3322
3227
1276
12337
3239
4190
5008
6746
1129
696
486
457
1495
789
210
374
1370
1843
8089
8499
905
2288
497
150
389
495
28
9881
10416
2408
8669
5813
2124
7327
5572
718
12420
7818
2569
17272
7745
1898

0.97
9.62
19.51
4.69
6.65
0.15
3.13
3.04
6.00
11.61
3.05
19.71
4.71
6.35
5.31
0.65
0.46
2.15
1.41
0.74
0.99
0.35
1.29
8.67
7.61
8.00
4.26
2.15
0.47
0.71
0.37
0.47
0.13
9.30
9.80
11.33
8.16
5.47
9.99
6.89
5.24
3.38
11.69
7.36
12.09
16.25
7.29
8.93

39

Table A.33: Wilcoxon test results for max observations percentage by training dataset for each model architecture.

Model

Dataset

ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
ML
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
MB
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MT
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM
MM

TT
TT
TT
TE
TE
TE
TM
TM
TM
TH
TH
TH
TT
TT
TT
TE
TE
TE
TM
TM
TM
TH
TH
TH
TT
TT
TT
TE
TE
TE
TM
TM
TM
TH
TH
TH
TT
TT
TT
TE
TE
TE
TM
TM
TM
TH
TH
TH

Group 1

Group 2

10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50
10
10
50

50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100
50
100
100

40

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
FALSE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

small
large
medium
medium
large
negligible
medium
large
small
small
large
large
small
small
negligible
large
large
medium
medium
medium
large
small
large
large
small
small
medium
small
large
large
negligible
medium
large
small
medium
small
small
small
small
negligible
small
small
negligible
small
negligible
negligible
small
medium

Training %
M_B

10

50

M_L

100

M_M

M_T

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
T_E

T_H

T_M

T_T

T_E

T_H

T_M

T_T

T_E

T_H

T_M

T_T

T_E

T_H

T_M

T_T

Training Dataset
Figure A.13: Box-plots of the relative edit distance by training dataset and training data percentage for each model architecture.

Appendix A.5. Model Performance Extended Analysis
Figure A.14 shows the distribution of DR for each model architecture split by training dataset. This
exhibits the differences in generalization capability of the model architectures under study. Table A.34
presents the significance and effect size of the difference in median DR between architectures split by the
training dataset used.
Appendix A.6. Comprehensive Results
The following tables present comprehensive descriptive statistics of all model configurations evaluated in
terms of both relative and absolute edit distance (DA and DR , respectively):

41

Table A.34: Wilcoxon test results for model architecture comparisons by training dataset.

Dataset

Group 1

Group 2

TT
TT
TT
TT
TT
TT
TE
TE
TE
TE
TE
TE
TM
TM
TM
TM
TM
TM
TH
TH
TH
TH
TH
TH

ML
ML
MB
MB
MB
MM
ML
ML
MB
MB
MB
MM
ML
ML
MB
MB
MB
MM
ML
ML
MB
MB
MB
MM

MT
MM
ML
MT
MM
MT
MT
MM
ML
MT
MM
MT
MT
MM
ML
MT
MM
MT
MT
MM
ML
MT
MM
MT

Training Dataset
T_E

M_B

Significant

Magnitude

TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE
TRUE

medium
large
medium
negligible
large
large
large
large
medium
large
large
large
large
large
medium
medium
large
negligible
large
large
large
large
large
medium

M_L

M_M

T_H

M_T

T_M

T_T

Relative Edit Distance

1.00

0.75

0.50

0.25

0.00
M_B

M_L

M_M

M_T

M_B

M_L

M_M

M_T

M_B

M_L

M_M

M_T

M_B

M_L

M_M

Model Architecture
Figure A.14: Box-plots of the relative edit distance by model architecture for each training dataset.

42

M_T

Table A.35: ML , char Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

5
19
23
20

48
56
90
79

48
46
83
76

66
70
114
102

84
94
158
153

102
106
196
196

211
252
357
333

383
439
491
490

4
20
23
20

48
59
78
70

50
49
68
55

63
58
88
82

68
83
102
92

80
86
111
104

120
117
189
210

4166
4200
4303
4373

2
14
19
16

55
39
92
77

58
29
93
81

79
43
115
106

94
74
133
119

119
99
149
129

262
152
234
191

1165
1117
1227
1291

DR

TT
TE
TM
TH

0.02
0.06
0.10
0.08

0.17
0.20
0.32
0.27

0.20
0.17
0.30
0.26

0.25
0.27
0.41
0.37

0.30
0.33
0.52
0.53

0.33
0.38
0.59
0.60

0.47
0.53
0.71
0.71

0.64
0.73
0.82
0.82

0.02
0.07
0.10
0.07

0.17
0.22
0.29
0.26

0.21
0.20
0.29
0.23

0.27
0.23
0.37
0.34

0.31
0.36
0.42
0.39

0.34
0.37
0.43
0.40

0.41
0.42
0.59
0.62

0.95
0.96
0.99
1.00

0.01
0.04
0.07
0.05

0.23
0.18
0.39
0.33

0.27
0.13
0.39
0.33

0.34
0.20
0.47
0.41

0.43
0.35
0.55
0.50

0.48
0.52
0.62
0.59

0.65
0.83
0.75
0.69

1.00
1.00
0.97
0.95

VA

VB

Table A.36: ML , char Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

5
11
17
16

43
56
87
87

31
46
81
72

57
63
113
118

82
104
159
169

105
145
198
196

283
309
315
347

404
491
491
545

4
11
15
15

40
42
68
62

25
26
52
42

49
46
81
88

60
58
96
93

77
63
115
99

140
135
204
181

4222
4267
4290
4290

2
8
15
13

39
28
76
69

27
19
78
69

62
28
103
93

86
50
124
115

96
83
134
122

148
162
187
189

1170
1288
1278
1281

DR

TT
TE
TM
TH

0.03
0.05
0.07
0.06

0.15
0.19
0.30
0.30

0.11
0.17
0.30
0.29

0.21
0.23
0.40
0.42

0.29
0.35
0.51
0.54

0.33
0.45
0.60
0.59

0.52
0.65
0.71
0.70

0.67
0.82
0.82
0.91

0.02
0.04
0.04
0.04

0.13
0.14
0.25
0.22

0.10
0.11
0.22
0.18

0.18
0.16
0.34
0.37

0.27
0.24
0.40
0.39

0.32
0.26
0.44
0.41

0.42
0.43
0.62
0.56

0.96
0.98
0.98
0.98

0.01
0.03
0.05
0.03

0.17
0.13
0.32
0.29

0.12
0.08
0.31
0.28

0.27
0.13
0.41
0.38

0.36
0.24
0.50
0.47

0.42
0.48
0.57
0.53

0.54
0.73
0.68
0.67

0.95
0.99
0.99
0.94

VA

VB

Table A.37: ML , char Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

18
24
18
10

36
65
69
59

37
54
52
48

47
66
89
75

55
91
143
119

56
143
171
157

79
333
368
411

139
460
451
490

18
15
18
11

41
41
50
43

37
29
44
39

39
32
44
40

42
44
46
41

44
75
107
65

63
137
173
152

3950
4246
3938
4288

11
5
14
9

35
20
83
49

18
12
84
54

44
18
112
73

77
36
134
80

112
59
151
89

147
98
207
121

565
985
1038
1041

DR

TT
TE
TM
TH

0.06
0.09
0.07
0.04

0.13
0.23
0.23
0.20

0.13
0.19
0.20
0.17

0.16
0.23
0.31
0.24

0.17
0.34
0.45
0.43

0.18
0.50
0.51
0.50

0.25
0.66
0.69
0.74

0.30
0.77
0.75
0.87

0.06
0.05
0.08
0.03

0.14
0.14
0.17
0.15

0.15
0.12
0.18
0.16

0.16
0.13
0.18
0.16

0.17
0.15
0.20
0.18

0.18
0.25
0.37
0.22

0.19
0.45
0.52
0.50

0.90
0.97
0.90
0.97

0.03
0.02
0.08
0.03

0.17
0.09
0.33
0.20

0.07
0.05
0.34
0.21

0.22
0.08
0.42
0.28

0.43
0.19
0.47
0.34

0.55
0.41
0.51
0.41

1.00
0.56
0.67
0.44

1.00
0.78
0.76
0.76

VA

VB

Table A.38: ML , word Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

24
28
12
9

111
86
74
74

108
71
61
66

125
101
88
105

140
158
128
144

157
194
168
180

288
329
391
368

465
502
532
498

23
32
17
7

102
68
65
69

95
57
49
56

116
63
77
78

124
73
83
115

128
105
94
135

164
215
178
206

4313
4333
4328
4323

27
25
15
9

96
100
91
83

92
94
88
79

114
122
113
107

133
144
138
138

150
161
158
161

202
242
298
257

1214
1216
1283
1320

DR

TT
TE
TM
TH

0.10
0.11
0.05
0.03

0.42
0.30
0.25
0.25

0.42
0.27
0.23
0.24

0.49
0.35
0.31
0.35

0.56
0.52
0.43
0.49

0.58
0.59
0.51
0.56

0.64
0.70
0.75
0.71

0.78
0.84
0.90
0.83

0.12
0.10
0.06
0.04

0.40
0.25
0.24
0.25

0.39
0.24
0.21
0.24

0.49
0.27
0.31
0.32

0.53
0.29
0.34
0.47

0.59
0.37
0.37
0.50

0.60
0.64
0.54
0.60

0.98
0.99
0.98
0.99

0.06
0.06
0.08
0.04

0.42
0.44
0.38
0.34

0.40
0.39
0.38
0.33

0.48
0.50
0.46
0.44

0.56
0.76
0.56
0.53

0.65
0.84
0.62
0.59

0.73
0.90
0.74
0.72

0.89
0.98
0.96
0.97

VA

VB

Table A.39: ML , word Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

37
15
5
7

114
86
108
113

109
85
100
113

130
99
134
160

166
118
189
199

187
146
217
235

300
308
386
420

419
474
511
541

28
15
6
7

99
89
97
106

86
79
90
103

110
92
110
124

124
118
139
155

145
122
158
183

191
163
222
251

4315
4309
4241
4332

17
25
2
2

102
97
108
112

99
94
106
116

124
117
139
147

145
143
169
176

166
155
193
201

202
183
281
286

1162
1329
1175
1260

DR

TT
TE
TM
TH

0.13
0.06
0.02
0.03

0.42
0.32
0.38
0.40

0.44
0.31
0.39
0.43

0.50
0.40
0.50
0.55

0.54
0.45
0.61
0.68

0.57
0.50
0.66
0.73

0.64
0.65
0.76
0.82

0.74
0.79
0.85
0.90

0.13
0.06
0.03
0.03

0.38
0.34
0.36
0.40

0.37
0.32
0.38
0.44

0.47
0.42
0.46
0.52

0.51
0.49
0.55
0.63

0.52
0.51
0.58
0.66

0.58
0.51
0.67
0.76

0.98
0.98
0.97
0.99

0.08
0.05
0.02
0.02

0.43
0.42
0.44
0.46

0.43
0.41
0.45
0.49

0.50
0.48
0.54
0.59

0.55
0.55
0.63
0.68

0.58
0.60
0.69
0.73

0.64
0.71
0.79
0.78

0.99
0.98
0.88
0.92

VA

VB

43

Table A.40: ML , word Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

32
18
8
3

135
98
111
92

124
94
101
101

149
123
141
129

175
152
177
170

189
173
206
199

254
480
405
415

333
558
490
499

36
27
8
3

119
108
86
101

103
102
93
101

116
123
93
120

141
130
110
121

169
137
158
154

221
188
211
221

4263
4347
4138
4294

11
33
14
11

90
94
92
108

95
88
94
113

115
119
117
151

130
135
138
177

146
147
155
197

213
193
214
219

1054
965
1113
1311

DR

TT
TE
TM
TH

0.16
0.07
0.03
0.01

0.51
0.34
0.40
0.32

0.51
0.35
0.40
0.37

0.54
0.43
0.52
0.47

0.56
0.49
0.57
0.56

0.58
0.54
0.62
0.63

0.62
0.90
0.76
0.77

0.68
0.93
0.82
0.83

0.17
0.10
0.04
0.01

0.47
0.42
0.32
0.38

0.45
0.42
0.39
0.43

0.52
0.52
0.39
0.50

0.54
0.55
0.42
0.50

0.56
0.55
0.53
0.54

0.63
0.59
0.65
0.67

0.97
0.99
0.94
0.98

0.05
0.09
0.07
0.05

0.36
0.40
0.37
0.43

0.38
0.38
0.37
0.46

0.43
0.46
0.46
0.58

0.48
0.58
0.55
0.63

0.52
0.71
0.64
0.66

0.65
0.80
0.65
0.69

0.82
0.94
0.81
0.96

VA

VB

Table A.41: MB , char Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

8
19
18
19

57
80
61
73

42
65
53
62

82
109
74
86

128
146
98
148

152
177
133
181

208
277
353
357

351
497
560
566

7
18
17
20

55
76
53
63

39
49
47
52

60
103
51
56

109
131
55
88

112
134
90
154

150
192
215
230

4219
4214
4343
4161

2
11
13
16

17
41
62
68

12
32
60
67

18
49
82
87

31
71
98
102

44
86
114
119

111
182
183
198

1066
1245
1192
1143

DR

TT
TE
TM
TH

0.03
0.08
0.07
0.08

0.21
0.29
0.21
0.25

0.17
0.23
0.20
0.23

0.29
0.44
0.26
0.31

0.45
0.52
0.32
0.48

0.50
0.55
0.42
0.55

0.54
0.67
0.83
0.73

0.99
1.00
1.00
1.00

0.03
0.06
0.05
0.07

0.20
0.29
0.19
0.23

0.18
0.20
0.19
0.22

0.21
0.44
0.21
0.24

0.46
0.53
0.23
0.31

0.46
0.55
0.29
0.54

0.47
0.60
0.78
0.98

0.99
0.96
1.00
1.00

0.01
0.04
0.04
0.05

0.08
0.18
0.26
0.30

0.05
0.14
0.25
0.29

0.08
0.21
0.33
0.35

0.14
0.33
0.47
0.45

0.21
0.47
0.50
0.57

0.44
0.66
0.64
0.90

1.00
1.00
1.00
1.00

VA

VB

Table A.42: MB , char Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

11
11
15
15

53
45
69
79

45
28
63
65

67
50
88
92

87
81
111
173

113
121
160
209

246
292
364
410

396
514
530
558

8
12
16
15

43
39
61
66

31
25
47
43

46
30
76
68

55
47
81
138

70
63
100
213

122
159
176
238

4219
4279
4291
4395

2
7
13
14

17
22
73
67

8
16
72
66

16
24
92
84

37
40
119
117

70
54
135
130

170
79
176
209

1064
1126
1155
1269

DR

TT
TE
TM
TH

0.04
0.04
0.07
0.06

0.19
0.15
0.24
0.27

0.16
0.11
0.22
0.23

0.25
0.18
0.31
0.33

0.29
0.27
0.37
0.56

0.35
0.36
0.52
0.63

0.50
0.62
0.76
0.90

0.96
1.00
1.00
1.00

0.03
0.03
0.04
0.03

0.15
0.13
0.22
0.24

0.13
0.11
0.20
0.18

0.18
0.13
0.30
0.25

0.23
0.20
0.34
0.51

0.24
0.22
0.41
0.92

0.38
0.50
0.57
1.00

0.96
1.00
1.00
1.00

0.01
0.02
0.04
0.04

0.08
0.10
0.31
0.29

0.04
0.07
0.30
0.26

0.07
0.11
0.38
0.35

0.15
0.20
0.48
0.47

0.30
0.27
0.53
0.59

0.90
0.47
0.72
1.00

1.00
1.00
1.00
1.00

VA

VB

Table A.43: MB , char Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

12
12
16
11

41
47
75
95

18
20
61
96

52
55
92
123

102
73
145
200

118
212
184
215

299
399
449
394

365
528
551
503

11
11
14
10

31
49
57
90

19
20
47
99

20
20
48
102

25
148
53
120

72
214
150
156

178
238
223
202

4105
4279
4288
4329

8
6
12
11

42
40
63
86

15
14
62
107

36
17
79
122

113
55
92
130

163
115
129
147

248
1363
333
199

1146
1363
1214
775

DR

TT
TE
TM
TH

0.05
0.03
0.07
0.04

0.14
0.16
0.25
0.32

0.07
0.08
0.22
0.37

0.19
0.18
0.31
0.44

0.33
0.27
0.48
0.63

0.36
0.77
0.66
0.65

0.60
1.00
0.86
0.72

0.99
1.00
0.98
0.97

0.04
0.04
0.05
0.04

0.10
0.18
0.19
0.34

0.08
0.08
0.19
0.41

0.08
0.09
0.19
0.42

0.10
0.69
0.21
0.47

0.27
1.00
0.52
0.63

0.56
1.00
0.72
0.69

1.00
1.00
0.99
0.98

0.03
0.02
0.05
0.04

0.18
0.12
0.25
0.34

0.06
0.05
0.24
0.38

0.15
0.08
0.30
0.46

0.56
0.25
0.45
0.54

0.98
0.59
0.57
0.61

1.00
1.00
1.00
0.68

1.00
1.00
1.00
1.00

VA

VB

Table A.44: MB , word Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

23
30
14
13

88
73
89
93

82
60
74
79

105
85
111
109

129
124
148
146

154
147
178
187

318
269
387
436

479
429
556
551

18
28
17
14

87
67
76
82

71
55
64
72

103
62
85
85

109
85
91
106

119
104
117
135

179
166
185
214

4282
4334
4316
4336

24
19
9
6

110
96
93
88

107
91
87
82

139
117
116
107

165
142
152
142

179
155
172
171

203
217
278
274

1288
1217
1156
1178

DR

TT
TE
TM
TH

0.08
0.12
0.06
0.07

0.32
0.26
0.31
0.33

0.31
0.23
0.29
0.31

0.41
0.29
0.39
0.40

0.49
0.40
0.48
0.49

0.53
0.45
0.54
0.57

0.63
0.58
0.81
0.82

0.80
0.74
0.97
0.93

0.08
0.09
0.07
0.07

0.33
0.25
0.28
0.30

0.32
0.24
0.28
0.30

0.43
0.27
0.35
0.36

0.46
0.35
0.38
0.44

0.47
0.36
0.40
0.47

0.56
0.51
0.55
0.64

0.98
0.99
0.98
0.99

0.08
0.10
0.06
0.04

0.47
0.42
0.39
0.36

0.48
0.40
0.39
0.35

0.54
0.49
0.48
0.44

0.58
0.61
0.57
0.54

0.61
0.70
0.63
0.61

0.66
0.87
0.73
0.77

0.94
0.93
0.93
0.99

VA

VB

44

Table A.45: MB , word Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

34
20
6
5

125
100
89
98

108
95
77
92

145
116
120
134

195
138
175
180

241
158
214
215

346
317
431
409

545
510
547
531

17
8
6
2

110
97
78
86

96
91
66
78

119
109
81
99

154
119
108
123

169
131
158
161

256
185
229
240

4273
4348
4360
4372

21
20
4
0

101
113
84
88

97
109
81
86

119
137
105
115

147
162
134
142

163
175
162
168

203
215
254
272

1146
1135
1246
1256

DR

TT
TE
TM
TH

0.12
0.10
0.03
0.02

0.46
0.37
0.30
0.34

0.43
0.37
0.30
0.36

0.54
0.44
0.43
0.47

0.74
0.53
0.58
0.59

0.77
0.58
0.67
0.66

0.82
0.67
0.82
0.80

0.91
0.88
0.95
0.88

0.11
0.04
0.03
0.01

0.43
0.37
0.28
0.32

0.40
0.38
0.27
0.36

0.50
0.46
0.36
0.41

0.60
0.49
0.45
0.50

0.72
0.52
0.52
0.54

0.80
0.60
0.70
0.73

0.98
1.00
0.99
0.99

0.09
0.09
0.03
0.00

0.44
0.48
0.34
0.36

0.42
0.49
0.35
0.36

0.50
0.55
0.42
0.47

0.59
0.61
0.49
0.58

0.65
0.64
0.56
0.64

0.73
0.73
0.68
0.77

0.97
0.84
0.91
1.00

VA

VB

Table A.46: MB , word Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

57
38
5
55

139
129
89
106

133
117
84
94

163
155
127
110

193
210
173
141

212
257
231
176

259
438
461
350

436
526
571
511

46
32
4
43

115
113
113
103

100
109
137
100

114
122
147
103

152
142
151
104

178
184
185
122

214
267
249
181

4272
4324
4186
3935

13
47
11
42

78
95
94
125

71
86
88
116

101
109
115
149

132
127
146
181

153
139
178
200

238
223
293
322

857
743
1250
1092

DR

TT
TE
TM
TH

0.25
0.16
0.02
0.22

0.52
0.46
0.30
0.39

0.54
0.47
0.32
0.39

0.58
0.55
0.44
0.43

0.62
0.65
0.56
0.47

0.66
0.79
0.69
0.54

0.75
0.90
0.90
0.70

1.00
0.99
0.96
0.89

0.27
0.17
0.02
0.18

0.45
0.43
0.43
0.40

0.42
0.45
0.54
0.42

0.45
0.51
0.62
0.43

0.56
0.53
0.62
0.44

0.61
0.60
0.62
0.44

0.79
0.85
0.74
0.56

1.00
0.98
0.95
0.90

0.06
0.09
0.06
0.22

0.31
0.40
0.37
0.50

0.30
0.38
0.37
0.51

0.40
0.46
0.44
0.56

0.48
0.54
0.51
0.64

0.52
0.66
0.59
0.67

0.72
0.76
0.73
0.75

1.00
0.94
1.00
0.91

VA

VB

Table A.47: MM , char Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

2
0
0
2

7
11
30
38

5
9
31
37

6
13
46
52

10
21
54
67

17
30
62
78

52
68
168
135

70
236
293
325

2
0
0
1

11
16
28
32

4
8
25
28

4
11
26
31

6
18
27
37

8
19
33
48

16
49
77
109

4234
4262
4274
4272

2
0
0
1

5
10
40
43

3
3
43
43

4
13
59
63

9
31
66
75

16
50
82
87

52
62
116
141

485
813
1056
792

DR

TT
TE
TM
TH

0.01
0.00
0.00
0.01

0.02
0.04
0.10
0.13

0.02
0.03
0.11
0.14

0.02
0.05
0.15
0.18

0.03
0.07
0.18
0.23

0.06
0.09
0.22
0.26

0.18
0.20
0.32
0.35

0.25
0.40
0.50
0.58

0.00
0.00
0.00
0.00

0.02
0.04
0.08
0.10

0.02
0.03
0.11
0.12

0.02
0.05
0.11
0.13

0.03
0.07
0.11
0.15

0.03
0.08
0.12
0.17

0.05
0.15
0.23
0.33

0.97
0.97
0.97
0.97

0.00
0.00
0.00
0.00

0.02
0.05
0.16
0.18

0.01
0.01
0.18
0.18

0.02
0.05
0.22
0.23

0.05
0.15
0.29
0.30

0.08
0.28
0.35
0.37

0.18
0.36
0.38
0.43

0.56
0.60
0.77
0.82

VA

VB

Table A.48: MM , char Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

2
0
1
1

6
11
37
37

5
8
35
36

6
15
49
51

10
22
64
69

19
33
84
90

41
88
180
168

61
287
312
393

2
0
0
1

12
15
27
33

4
9
24
26

5
12
26
29

6
13
27
47

9
16
37
48

20
56
91
98

4232
4244
4329
4276

2
0
0
0

6
13
38
45

3
4
43
48

3
16
58
65

14
38
65
79

19
57
78
90

58
73
126
140

609
956
1083
934

DR

TT
TE
TM
TH

0.01
0.00
0.00
0.00

0.02
0.04
0.13
0.13

0.02
0.03
0.13
0.13

0.02
0.05
0.17
0.18

0.03
0.07
0.22
0.23

0.06
0.10
0.27
0.29

0.09
0.23
0.39
0.41

0.22
0.50
0.55
0.68

0.00
0.00
0.00
0.00

0.02
0.03
0.08
0.10

0.02
0.04
0.10
0.11

0.02
0.05
0.11
0.12

0.02
0.05
0.11
0.19

0.03
0.06
0.12
0.20

0.06
0.17
0.28
0.30

0.96
0.97
0.98
0.97

0.01
0.00
0.00
0.00

0.03
0.06
0.15
0.18

0.01
0.02
0.17
0.20

0.02
0.07
0.22
0.25

0.06
0.17
0.28
0.32

0.10
0.27
0.32
0.37

0.26
0.35
0.37
0.41

0.58
0.70
0.81
0.82

VA

VB

Table A.49: MM , char Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

2
1
2
3

6
25
29
40

5
19
32
40

6
30
48
54

9
37
54
71

19
49
64
89

27
197
97
270

52
254
115
327

2
0
0
1

11
13
29
43

4
5
28
45

4
6
28
47

5
7
28
48

7
14
33
49

16
33
82
87

3734
4243
4277
3350

2
0
0
1

6
10
37
41

3
3
44
47

3
7
59
63

10
28
65
68

22
59
77
81

43
78
107
125

466
514
712
677

DR

TT
TE
TM
TH

0.01
0.01
0.01
0.02

0.02
0.09
0.10
0.14

0.02
0.08
0.11
0.14

0.02
0.10
0.16
0.19

0.03
0.12
0.19
0.26

0.06
0.15
0.21
0.30

0.08
0.36
0.28
0.47

0.21
0.43
0.34
0.58

0.01
0.00
0.00
0.00

0.02
0.02
0.09
0.15

0.02
0.02
0.11
0.18

0.02
0.03
0.12
0.20

0.02
0.03
0.12
0.20

0.02
0.05
0.12
0.21

0.06
0.12
0.25
0.29

0.85
0.97
0.97
0.76

0.01
0.00
0.00
0.00

0.02
0.04
0.15
0.16

0.01
0.01
0.17
0.18

0.02
0.03
0.22
0.23

0.04
0.12
0.27
0.29

0.13
0.20
0.31
0.34

0.15
0.36
0.34
0.38

0.41
0.50
0.81
0.79

VA

VB

45

Table A.50: MM , word Tokenization, 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

2
1
2
6

17
41
50
51

9
34
41
43

16
51
68
63

41
78
99
87

64
101
116
106

97
191
223
200

255
305
338
327

2
0
0
2

20
28
38
42

8
19
24
34

10
28
38
48

33
34
53
59

36
43
84
80

99
92
123
116

4374
4374
4374
4374

2
0
0
0

22
43
56
58

11
39
53
55

34
54
76
78

55
76
101
101

70
91
114
116

97
127
153
148

458
966
943
1008

DR

TT
TE
TM
TH

0.01
0.00
0.01
0.02

0.06
0.14
0.17
0.18

0.04
0.13
0.16
0.17

0.06
0.18
0.24
0.22

0.17
0.26
0.31
0.28

0.22
0.31
0.35
0.33

0.29
0.39
0.45
0.42

0.43
0.52
0.56
0.54

0.01
0.00
0.00
0.01

0.05
0.09
0.12
0.14

0.03
0.08
0.11
0.14

0.04
0.12
0.16
0.20

0.14
0.14
0.20
0.22

0.15
0.15
0.28
0.27

0.32
0.28
0.36
0.34

0.99
0.99
0.99
0.99

0.00
0.00
0.00
0.00

0.10
0.18
0.23
0.24

0.04
0.16
0.23
0.24

0.16
0.22
0.31
0.30

0.28
0.32
0.37
0.37

0.32
0.38
0.44
0.42

0.47
0.47
0.52
0.52

0.66
0.71
0.69
0.74

VA

VB

Table A.51: MM , word Tokenization, 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

6
6
0
4

31
40
46
40

13
32
39
35

46
46
61
50

72
74
94
73

90
91
112
89

146
221
223
220

307
288
360
306

2
0
0
0

30
36
31
31

14
27
18
21

31
32
29
29

45
45
45
34

66
70
71
56

119
111
114
104

4374
4374
4381
4374

2
0
0
0

25
41
53
49

15
35
51
49

38
56
70
64

59
73
95
84

72
87
109
96

102
140
142
127

289
854
811
615

DR

TT
TE
TM
TH

0.02
0.02
0.00
0.02

0.11
0.14
0.16
0.14

0.05
0.13
0.14
0.14

0.19
0.17
0.22
0.18

0.26
0.24
0.30
0.24

0.29
0.28
0.34
0.28

0.37
0.41
0.44
0.41

0.52
0.49
0.60
0.51

0.01
0.00
0.00
0.00

0.10
0.12
0.10
0.10

0.06
0.11
0.08
0.09

0.14
0.13
0.12
0.12

0.20
0.17
0.17
0.14

0.24
0.24
0.24
0.19

0.37
0.33
0.35
0.31

0.99
0.99
1.00
0.99

0.00
0.00
0.00
0.00

0.11
0.17
0.22
0.20

0.07
0.15
0.21
0.20

0.18
0.23
0.28
0.26

0.27
0.31
0.36
0.32

0.33
0.41
0.40
0.37

0.48
0.49
0.49
0.46

0.67
0.68
0.70
0.68

VA

VB

Table A.52: MM , word Tokenization, 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

36
6
6
10

80
30
31
73

70
22
27
62

92
31
42
97

118
64
51
123

132
73
66
143

280
235
212
272

332
277
257
337

27
0
0
6

66
18
23
59

53
10
18
50

54
10
18
51

92
18
21
87

116
44
39
111

150
85
90
144

4374
4374
4374
4345

8
0
0
4

49
32
44
78

45
25
47
75

67
46
58
105

81
62
74
124

94
77
88
136

151
143
111
172

466
591
569
938

DR

TT
TE
TM
TH

0.14
0.02
0.02
0.05

0.29
0.10
0.10
0.25

0.29
0.09
0.10
0.24

0.32
0.12
0.14
0.32

0.37
0.20
0.17
0.38

0.39
0.24
0.25
0.41

0.53
0.43
0.39
0.51

0.55
0.49
0.44
0.56

0.08
0.00
0.00
0.03

0.24
0.04
0.06
0.21

0.23
0.04
0.08
0.21

0.25
0.04
0.08
0.21

0.32
0.06
0.09
0.30

0.38
0.15
0.14
0.36

0.43
0.26
0.27
0.41

0.99
0.99
0.99
0.99

0.03
0.00
0.00
0.03

0.22
0.13
0.18
0.33

0.19
0.11
0.19
0.32

0.28
0.19
0.23
0.39

0.40
0.27
0.28
0.48

0.47
0.31
0.33
0.49

0.57
0.50
0.44
0.61

0.69
0.66
0.66
0.70

VA

VB

Table A.53: MT , word Tokenization, Sequence Length of 256, and 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

11
0
0
0

57
27
32
39

20
12
19
32

56
28
37
45

203
61
75
76

217
129
148
120

252
229
189
227

426
440
371
445

11
0
0
0

63
24
39
32

17
11
18
21

39
15
23
22

206
26
137
41

229
50
137
82

246
183
160
155

4321
4350
4339
4338

21
0
0
0

32
33
49
50

24
26
37
44

35
44
54
58

51
62
120
87

65
84
169
140

107
173
219
225

1162
1353
1321
1296

DR

TT
TE
TM
TH

0.04
0.00
0.00
0.00

0.23
0.09
0.11
0.13

0.07
0.05
0.07
0.12

0.20
0.10
0.12
0.18

0.95
0.21
0.23
0.26

0.96
0.36
0.50
0.37

0.96
0.61
0.62
0.57

0.97
0.78
0.82
0.75

0.03
0.00
0.00
0.00

0.24
0.06
0.13
0.10

0.07
0.05
0.08
0.09

0.13
0.06
0.10
0.09

0.96
0.10
0.57
0.17

0.96
0.17
0.58
0.26

0.97
0.60
0.60
0.65

0.99
1.00
0.99
0.99

0.02
0.00
0.00
0.00

0.15
0.14
0.19
0.20

0.11
0.11
0.15
0.18

0.16
0.18
0.24
0.25

0.27
0.30
0.47
0.38

0.39
0.39
0.57
0.51

0.51
0.58
0.65
0.66

0.93
0.99
0.98
0.97

VA

VB

Table A.54: MT , word Tokenization, Sequence Length of 256, and 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

11
0
0
0

71
24
34
28

32
11
23
23

111
30
39
37

198
50
76
52

220
96
155
82

281
221
228
191

593
480
414
382

8
0
0
0

61
25
31
29

16
10
18
19

80
15
21
21

129
27
39
35

207
70
131
74

270
173
188
144

4302
4342
4349
4330

21
0
0
0

38
29
48
45

25
21
43
39

45
41
56
54

70
59
83
78

101
82
138
115

172
191
208
206

1157
1275
1214
1282

DR

TT
TE
TM
TH

0.03
0.00
0.00
0.00

0.26
0.08
0.11
0.09

0.10
0.04
0.08
0.08

0.44
0.10
0.13
0.13

0.65
0.20
0.23
0.18

0.97
0.30
0.54
0.26

0.97
0.69
0.64
0.59

0.99
0.89
0.79
0.74

0.04
0.00
0.00
0.00

0.22
0.07
0.09
0.08

0.07
0.05
0.08
0.08

0.32
0.06
0.09
0.09

0.54
0.09
0.13
0.12

0.95
0.22
0.49
0.24

0.97
0.65
0.62
0.60

0.98
0.99
1.00
0.99

0.02
0.00
0.00
0.00

0.17
0.12
0.19
0.18

0.12
0.09
0.17
0.17

0.20
0.17
0.23
0.23

0.39
0.28
0.34
0.32

0.50
0.35
0.49
0.44

0.70
0.52
0.62
0.57

0.95
0.95
0.96
0.96

VA

VB

46

Table A.55: MT , word Tokenization, Sequence Length of 256, and 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

14
13
0
0

24
68
47
28

15
49
26
15

22
80
51
37

43
112
152
76

60
176
162
115

173
283
249
172

297
468
368
419

12
9
0
0

25
66
48
31

16
39
18
18

16
46
27
19

20
138
139
58

36
183
140
93

87
253
173
139

4237
4341
4318
4346

21
0
0
0

31
58
51
41

24
47
47
38

33
66
62
54

57
133
83
66

69
183
120
87

92
228
235
212

1137
1243
1202
1193

DR

TT
TE
TM
TH

0.03
0.07
0.00
0.00

0.08
0.24
0.15
0.09

0.06
0.21
0.11
0.06

0.08
0.27
0.18
0.12

0.16
0.33
0.50
0.25

0.18
0.52
0.56
0.31

0.31
0.74
0.61
0.59

0.70
0.81
0.70
0.72

0.04
0.05
0.00
0.00

0.07
0.24
0.16
0.09

0.07
0.16
0.08
0.08

0.07
0.19
0.09
0.08

0.09
0.63
0.59
0.20

0.12
0.73
0.60
0.29

0.26
0.77
0.60
0.40

0.97
0.99
0.99
0.99

0.04
0.00
0.00
0.00

0.14
0.24
0.20
0.16

0.11
0.20
0.19
0.16

0.13
0.29
0.24
0.21

0.32
0.48
0.36
0.29

0.42
0.70
0.47
0.35

0.47
0.78
0.61
0.50

0.92
0.91
0.97
0.90

VA

VB

Table A.56: MT , word Tokenization, Sequence Length of 756, and 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

8
0
0
0

161
21
31
30

184
8
19
19

215
14
37
38

257
38
81
59

287
147
148
84

387
196
181
216

589
349
385
420

5
0
0
0

159
20
38
38

175
7
18
19

218
10
19
23

227
14
137
136

241
34
137
140

313
178
144
144

4166
4385
4378
4377

4
0
0
0

29
39
54
49

21
24
40
42

25
47
60
57

54
95
146
97

70
152
176
140

105
220
222
205

1171
1262
1190
1288

DR

TT
TE
TM
TH

0.02
0.00
0.00
0.00

0.61
0.06
0.10
0.10

0.64
0.03
0.07
0.07

0.94
0.05
0.12
0.13

0.96
0.13
0.24
0.20

0.96
0.45
0.50
0.26

0.97
0.57
0.59
0.59

0.98
0.70
0.74
0.80

0.01
0.00
0.00
0.00

0.64
0.05
0.13
0.12

0.74
0.03
0.06
0.08

0.95
0.04
0.08
0.10

0.95
0.06
0.57
0.56

0.95
0.12
0.58
0.57

0.96
0.58
0.62
0.60

0.98
1.00
1.00
1.00

0.01
0.00
0.00
0.00

0.14
0.16
0.21
0.20

0.10
0.10
0.17
0.17

0.13
0.21
0.28
0.24

0.30
0.43
0.54
0.38

0.41
0.56
0.60
0.56

0.52
0.68
0.68
0.67

0.89
0.93
0.87
0.94

VA

VB

Table A.57: MT , word Tokenization, Sequence Length of 756, and 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

7
0
0
0

90
15
23
29

83
7
18
23

129
15
33
39

175
37
45
56

211
54
65
98

342
172
170
198

559
403
360
404

7
0
0
0

88
18
24
26

52
7
18
18

131
11
18
22

180
18
21
24

185
36
40
46

245
144
159
121

4323
4391
4384
4368

5
0
0
0

57
25
45
46

38
18
38
43

71
32
54
58

121
51
77
79

152
74
135
115

227
180
214
207

1158
1263
1249
1289

DR

TT
TE
TM
TH

0.02
0.00
0.00
0.00

0.32
0.05
0.07
0.10

0.32
0.02
0.06
0.09

0.46
0.05
0.11
0.13

0.63
0.12
0.15
0.19

0.68
0.18
0.20
0.30

0.82
0.44
0.53
0.53

0.98
0.76
0.73
0.78

0.02
0.00
0.00
0.00

0.33
0.04
0.07
0.07

0.22
0.03
0.07
0.08

0.55
0.05
0.08
0.09

0.76
0.08
0.09
0.10

0.78
0.12
0.13
0.16

0.87
0.49
0.56
0.35

0.99
1.00
1.00
0.99

0.02
0.00
0.00
0.00

0.25
0.10
0.18
0.18

0.18
0.07
0.17
0.18

0.36
0.14
0.22
0.23

0.52
0.27
0.33
0.33

0.61
0.32
0.46
0.41

0.77
0.51
0.57
0.55

0.91
0.93
0.92
0.97

VA

VB

Table A.58: MT , word Tokenization, Sequence Length of 756, and 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

140
0
0
0

251
25
25
21

239
18
19
21

281
33
37
36

314
44
56
42

341
62
80
44

448
285
156
123

520
375
419
210

75
0
0
0

234
26
29
23

225
19
19
18

226
19
22
21

273
21
49
21

305
48
80
26

344
180
120
75

4356
4174
4310
4323

4
0
0
0

27
28
41
40

21
22
39
38

27
37
52
54

35
51
64
68

55
72
85
80

79
183
209
207

926
1100
962
1118

DR

TT
TE
TM
TH

0.53
0.00
0.00
0.00

0.94
0.08
0.08
0.07

0.94
0.06
0.08
0.07

0.95
0.11
0.12
0.12

0.96
0.15
0.20
0.14

0.96
0.21
0.25
0.16

0.96
0.58
0.34
0.24

0.97
0.73
0.70
0.35

0.74
0.00
0.00
0.00

0.94
0.07
0.08
0.06

0.95
0.08
0.08
0.08

0.95
0.08
0.09
0.09

0.95
0.09
0.18
0.09

0.96
0.17
0.26
0.10

0.96
0.63
0.34
0.24

0.99
0.96
0.98
0.99

0.01
0.00
0.00
0.00

0.13
0.12
0.16
0.16

0.10
0.09
0.16
0.17

0.13
0.16
0.21
0.21

0.23
0.25
0.28
0.28

0.33
0.28
0.36
0.36

0.41
0.50
0.50
0.48

0.68
0.82
0.75
0.82

VA

VB

Table A.59: MT , word Tokenization, Sequence Length of 4188, and 10% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

9
0
0
0

119
20
23
30

35
9
14
21

219
17
31
39

281
38
42
63

309
123
71
100

391
207
182
232

594
388
366
449

8
0
0
0

107
20
27
26

16
8
18
17

211
10
18
21

224
14
22
25

252
35
119
56

324
172
148
125

4306
4352
4349
4347

16
0
0
0

32
32
45
41

22
24
38
34

33
40
54
53

59
61
79
69

70
90
125
87

110
189
210
213

1329
1345
1335
1275

DR

TT
TE
TM
TH

0.02
0.00
0.00
0.00

0.44
0.06
0.08
0.10

0.12
0.03
0.05
0.07

0.94
0.06
0.11
0.13

0.96
0.13
0.14
0.21

0.96
0.31
0.22
0.30

0.98
0.55
0.57
0.50

0.99
0.74
0.74
0.77

0.03
0.00
0.00
0.00

0.41
0.05
0.08
0.07

0.07
0.03
0.08
0.07

0.94
0.04
0.08
0.09

0.95
0.05
0.09
0.10

0.95
0.12
0.39
0.19

0.97
0.57
0.60
0.46

0.99
1.00
1.00
1.00

0.02
0.00
0.00
0.00

0.15
0.13
0.18
0.16

0.11
0.10
0.17
0.15

0.15
0.17
0.22
0.21

0.28
0.29
0.35
0.30

0.40
0.39
0.48
0.38

0.51
0.58
0.64
0.56

0.98
0.99
0.98
0.94

VA

VB

47

Table A.60: MT , word Tokenization, Sequence Length of 4188, and 50% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

15
0
0
0

126
17
26
25

105
7
22
22

188
16
37
37

261
36
51
47

301
60
68
68

395
176
155
165

600
454
377
357

14
0
0
0

118
18
25
27

89
7
18
18

133
10
21
22

237
18
23
28

238
39
43
48

320
144
146
140

4286
4390
4358
4349

4
0
0
0

85
25
45
42

77
17
41
38

126
33
54
54

170
54
77
73

197
84
114
91

281
175
207
207

1281
1270
1332
1306

DR

TT
TE
TM
TH

0.04
0.00
0.00
0.00

0.46
0.05
0.09
0.08

0.42
0.03
0.08
0.08

0.62
0.06
0.13
0.12

1.00
0.12
0.17
0.16

1.00
0.19
0.21
0.21

1.00
0.47
0.59
0.37

1.00
0.77
0.70
0.68

0.04
0.00
0.00
0.00

0.46
0.04
0.07
0.08

0.39
0.03
0.08
0.08

0.55
0.04
0.09
0.09

1.00
0.06
0.10
0.10

1.00
0.13
0.15
0.16

1.00
0.56
0.62
0.39

1.00
1.00
1.00
0.99

0.01
0.00
0.00
0.00

0.36
0.11
0.18
0.17

0.40
0.07
0.17
0.16

0.57
0.15
0.22
0.22

0.66
0.28
0.33
0.32

0.72
0.38
0.47
0.41

0.78
0.58
0.61
0.52

0.96
0.93
0.98
0.96

VA

VB

Table A.61: MT , word Tokenization, Sequence Length of 4188, and 100% of Tx
Trained
on

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT
TE
TM
TH

24
0
1
84

142
14
24
183

142
0
22
174

161
10
37
229

181
44
46
266

198
72
56
290

257
212
123
399

323
382
441
577

21
0
1
72

147
13
26
180

141
0
20
177

148
0
20
189

152
14
24
227

168
42
45
253

210
92
99
311

4298
4317
4340
4297

22
0
0
45

99
21
44
198

93
7
36
193

121
33
56
237

154
50
77
273

178
69
145
294

242
189
221
401

1229
1217
1307
1318

DR

TT
TE
TM
TH

0.09
0.00
0.00
0.42

0.54
0.04
0.08
0.67

0.57
0.00
0.07
0.66

0.63
0.03
0.12
0.80

0.67
0.14
0.16
0.88

0.68
0.22
0.19
0.90

0.71
0.40
0.29
0.93

0.76
0.67
0.78
0.98

0.08
0.00
0.00
0.42

0.59
0.02
0.07
0.72

0.62
0.00
0.08
0.74

0.63
0.00
0.09
0.78

0.63
0.06
0.10
0.88

0.65
0.14
0.15
0.89

0.69
0.27
0.29
0.91

0.99
0.99
0.99
0.99

0.09
0.00
0.00
0.29

0.43
0.09
0.17
0.84

0.44
0.03
0.16
0.86

0.52
0.15
0.23
0.89

0.62
0.28
0.33
0.92

0.69
0.32
0.49
0.93

0.74
0.50
0.59
0.96

0.90
0.89
0.96
0.98

VA

VB

Table A.62: ML , word Tokenization, 100% of TT , evaluated on V ∗
Trained
on

VA

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

VB
q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT

1

77

103

121

136

152

224

291

0

84

92

100

107

129

184

4138

0

83

96

115

131

144

206

1044

DR

TT

0.00

0.27

0.37

0.43

0.48

0.48

0.51

0.55

0.00

0.32

0.40

0.43

0.43

0.44

0.54

0.95

0.00

0.33

0.39

0.45

0.49

0.51

0.57

0.77

max

Table A.63: MB , word Tokenization, 100% of TT , evaluated on V ∗
Trained
on

VA

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

VB
q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

DA

TT

2

40

34

58

78

93

200

390

0

47

49

49

59

77

111

4268

0

37

27

52

75

89

116

891

DR

TT

0.01

0.14

0.13

0.21

0.28

0.31

0.60

0.77

0.00

0.16

0.21

0.21

0.21

0.26

0.33

0.98

0.00

0.16

0.13

0.23

0.31

0.34

0.39

0.65

Table A.64: MT , word Tokenization, Sequence Length of 256, and 100% of TT , evaluated on V ∗
Trained
on

VA

VB

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

q.75

q.90

q.95

q.99

max

min

avg

q.50

VC
q.75 q.90

q.95

q.99

max

DA

TT

0

4

0

3

8

29

56

158

0

10

0

0

4

8

66

4228

0

9

1

8

32

49

76

1125

DR

TT

0.00

0.01

0.00

0.01

0.03

0.10

0.17

0.30

0.00

0.01

0.00

0.00

0.02

0.03

0.20

0.97

0.00

0.04

0.00

0.03

0.17

0.30

0.38

0.92

48

