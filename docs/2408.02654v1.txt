On Using Quasirandom Sequences in Machine Learning for Model
Weight Initialization
Andriy Miranskyy, Adam Sorrenti, and Viral Thakar∗

arXiv:2408.02654v1 [cs.LG] 5 Aug 2024

Department of Computer Science, Toronto Metropolitan University, Toronto, Canada
{avm, adam.sorrenti, vthakar}@torontomu.ca

Abstract
The effectiveness of training neural networks directly impacts computational costs, resource
allocation, and model development timelines in machine learning applications. An optimizer’s
ability to train the model adequately (in terms of trained model performance) depends on the
model’s initial weights. Model weight initialization schemes use pseudorandom number generators
(PRNGs) as a source of randomness.
We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) — namely Sobol’ sequences — as a source of randomness for initializers can
improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on
MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses
ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random
Normal, Truncated Normal, and Random Uniform. Models with weights set using PRNG- and
QRNG-based initializers are compared pairwise for each combination of dataset, architecture,
optimizer, and initialization scheme.
Our findings indicate that QRNG-based neural network initializers either reach a higher
accuracy or achieve the same accuracy more quickly than PRNG-based initializers in 60% of
the 120 experiments conducted. Thus, using QRNG-based initializers instead of PRNG-based
initializers can speed up and improve model training.

1

Introduction

The effectiveness of training deep neural networks has a direct impact on computational costs,
resource allocation, and model development timelines in machine learning applications [20], [49]. The
initialization of the neural network’s weights plays a critical role in its training efficiency: random
initialization methods can introduce variations that slow the training or hinder convergence [20],
[51], [57]. Pseudorandom number generators (PRNGs) are traditionally used to initialize neural
networks [18], [24], [34]. However, there are other ways to generate sequences of random numbers. For
example, low-discrepancy quasirandom number generators (QRNGs), which provide high uniformity
in filling high-dimensional spaces, have proved effective in numerical integration in high-dimensional
spaces via Monte Carlo simulations [25].
QRNGs are also used for optimization [33]. Compared with PRNGs, low-discrepancy QRNGs
ensure a more uniform exploration of the search space. Uniformity allows for a thorough examination
∗

Authors are listed alphabetically.

1

of the parameter space, potentially resulting in more reliable results for functions with multiple local
optima. This leads to the following question.
Can QRNG uniformity properties be applied to neural network weight initialization? Our
hypothesis is that QRNG-based neural network weight initializers can accelerate neural network
training. The following research questions will help us explore various aspects of this hypothesis.
RQ1. What effect does the selection of optimization algorithms have on deep neural networks’
performance when initialized using the QRNG-based weight initialization scheme?
RQ2. How does a QRNG-based weight initialization scheme affect deep neural networks’ performance?
RQ3. How does the choice of deep neural network architecture affect the model’s performance when
it is initialized using the QRNG-based weight initialization scheme?
RQ4. How does the change in the dataset affect the performance of deep neural networks initialized
with the QRNG-based weight initialization scheme?
We empirically investigate how a specific type of QRNG, namely Sobol’ sequences [58], affects the
training of four neural network architectures: Multi-Layer Perceptrons (MLP) [18], Convolutional
Neural Networks (CNN) [18], Long Short-Term Memory (LSTM) [23], and Transformer [64]. Models’
kernel weights are initialized using ten different initializers: Glorot Uniform [17], Glorot Normal [17],
He Uniform [21], He Normal [21], Lecun Uniform [30], Lecun Normal [30], Orthogonal [55], Random
Uniform [34], Random Normal [34], and Truncated Normal [34]. Models are trained on two types
of data: (a) images, represented by Modified National Institute of Standards and Technology
(MNIST) [36] and Canadian Institute For Advanced Research, 10 classes (CIFAR-10) [32] datasets,
and (b) natural language, represented by Internet Movie Database (IMDB) [38] dataset. The
stochastic gradient descent (SGD) [4], [53] and Adam [29] methods optimize the models’ weights.
Summary of contributions We investigate whether QRNG-based initializers can accelerate training in four neural network architectures. Among our contributions are: (a) developing QRNG-based
sampling mechanisms to implement ten popular initializers, (b) providing a Keras-based implementation of these initializers1 , and (c) evaluating QRNG’s impact on training speed across different
datasets and optimization methods. Our research confirms the hypothesis by addressing the research
questions. We showed that QRNG-based initializers improve accuracy or accelerate neural network
training in 60% of the 120 experiments conducted. The top 25% of the improved maximum median
accuracies fall between ≈ 0.0775 and 0.3550. The negative side effects of using QRNG are minimal,
with the top 25% of observed losses leading to a decrease in accuracy between ≈ 0.0031 and 0.0402.
Data complexity may contribute to variability of results. QRNG significantly improved nine of the
ten initializers (except Random Uniform). A generalizability assessment requires further research.
The remainder of the paper is organized as follows. Section 2 discusses the mechanisms for
implementing initializers. Section 3 evaluates the initializers empirically, presents results, discusses
limitations, and poses open questions. Section 4 reviews related work. Finally, Section 5 concludes
the paper.

2

Methods

Below we provide details of the setup needed to compare the performance of PRNGs and QRNGs
for weight initialization of neural networks.
1

Shared via https://github.com/miranska/qrng-init [43].

2

Random number generators Pseudorandom numbers can be generated using various algorithms.
As a baseline, we use the Philox PRNG [54] (as it is the default PRNG in Keras with Tensorflow
backend [40], a popular machine learning library, see Appendix B.1 for details); see Appendix B.1.1
for additional details. Furthermore, we will use the ubiquitous Mersenne Twister PRNG [41] to
compare the performance of TensorFlow’s Philox implementation.
Various QRNGs exist, e.g., those based on Faure [14], Halton [19], Niederreiter [46], and Sobol’ [58]
sequences. They provide a deterministic and evenly distributed set of points in multidimensional
space, thereby overcoming some limitations of traditional pseudorandom sequences [25]. Empirical
evidence suggests that Sobol’ sequences are generally more homogeneous [25]. Therefore, we chose to
use Sobol’ sequences in our study. Further details about Sobol’ sequences are given in Appendix A.
Distributions Detailed information about the implementation of the distributions using PRNGs
and QRNGs is provided in Appendix B. We provide a brief summary below.
Three distributions are used in the initializers under study: random uniform, random normal,
and truncated normal. Keras implementations of uniform distribution with TensorFlow backend use
the Philox PRNG; Box-Muller transforms and sampling with rejection are utilized for drawing from
random normal and truncated normal distributions. Further details are provided in Appendix B.1.
In order to implement QRNG-based distributions, we use inverse transform sampling, taking
values drawn from the Sobol’ sequences as input (see Appendix B.2 for details). For Mersenne
Twister PRNG-based distributions, we follow a similar approach: doing inverse sampling of random
sequences generated by the Mersenne Twister algorithm (see Appendix B.4 for additional details).
Initializers The ten initializers under study can be classified into three groups: those using random
uniform, random normal, or truncated normal distributions. (a) Random uniform distribution is
used by Glorot Uniform, He Uniform, Lecun Uniform, and Random Uniform initializers, (b) Normal
distribution — by Orthogonal and Random Normal initializers, and (c) Truncated normal — by
Glorot Normal, He Normal, Lecun Normal, and Truncated Normal initializers. The initializer details
are provided in the Appendix C.
Initializers can also be categorized on the basis of how they handle random numbers. This
categorization will be helpful during the analysis of the results. We create three groups: shapeagnostic, shape-dependent, and orthogonal. A shape-agnostic initializer, such as Random Normal,
Random Uniform, and Truncated Normal, uses random numbers independent of the object’s shape
whose weights we are initializing. A shape-dependent initializer2 , such as Glorot Uniform, Glorot
Normal, He Uniform, He Normal, Lecun Uniform, and Lecun Normal, transforms random values
according to the parameters that govern the underlying distributions as well as the shapes of the input
tensors. Finally, an orthogonal initializer deserves its own category: while primarily dependent on
the parameters governing the underlying distributions, it also performs a significant transformation
of the data (namely QR decomposition), which has a dramatic impact on the underlying random
sequences.

3

Results

3.1

Experimental setup

Datasets Our models are trained on two image classification datasets, MNIST and CIFAR-10,
and one text classification dataset, IMDB. The details of the datasets and data preparations are
2

All shape-dependent initializers under study use variance scaling.

3

given in Appendix F.
Deep Neural Network Architectures Our models utilize four fundamental deep neural network
architectures: MLP, CNN, LSTM, and Transformer, with detailed configurations provided in
Appendix D. The architectures we use are intentionally simplified to see how different random
number generators affect kernel weights (and avoid the effects of more sophisticated techniques).
Consequently, we have omitted layers (such as extra regularization and dense layers) to minimize
their influence on performance. We keep the number of kernel weights small, isolating the effects of
random number generators (a larger number of weights gives any model better predictive power,
as demonstrated in Appendix H (Figure 12) and discussed in [71]. This simplification will reduce
overall model efficacy, but our primary objective is to assess the effects of random generators, not to
optimize the performance for any particular architecture.
Random number generators The QRNG under study is based on Sobol’ sequences (see Section 2)
implemented in SciPy v.1.13.1 software [56], [65], which utilizes directional integers computed by [26].
More details are available in Appendix A.
Keras v.3.3.3 [11] with TensorFlow v.2.16.1 [40] backend provides the baseline Philox PRNG [54]
(see Appendix B.1), while NumPy library v.1.24 [42] gives us Mersenne Twister PRNG [41].
Distributions and Initializers We intialize models’ kernel weights using ten different initializers
listed in Section 2. Appendices C and B contain implementation details of initializers and their
underlying distributions, respectively. Essentially, we modify the Keras [11] classes responsible for
sampling random numbers and creating initializers. Inverse transform sampling is performed using
functions from the SciPy v.1.13.1.
Optimizers Our models are trained using Adam and SGD algorithms. It allows us to examine how
initialization methods and architectural choices are affected by the choice of optimizer. Appendix E
lists the model hyperparameters.
Combinations and repetitions of experiments For the main experiments, we explore the
following combination of models, optimizers, and datasets. With two image classification datasets,
we explore two architectures (MLP and CNN) and two optimizers (Adam and SGD), resulting
in eight experiments. Four experiments are conducted on the text classification dataset (IMDB)
using two architectures (LSTM and Transformer) and two optimizers (Adam and SGD). The 12
(= 8 + 4) combinations are tested against ten initializers and two random generators (PRNG and
QRNG), bringing the number of combinations to 240. Since optimization is stochastic, experiments
are repeated 100 times to assess robustness.
Training For each individual experiment run, we adhered to the following settings. Every
experiment should have the same epoch count and batch size for consistent comparisons. The models
are trained for 30 epochs, providing a substantial training duration for model convergence and
performance assessment. A fixed batch size of 64 was used, promoting efficient gradient updates and
facilitating fair comparisons.
Testbed In order to eliminate the sources of randomness from GPU software stacks, we chose a
CPU-only testbed. The experiments were performed on high-performance computing clusters with

4

Intel Xeon Gold 6148 Skylake CPUs. All experiments were allocated 4 GB of memory and 2 CPU
cores.

3.2

Measure performance and analyze the results

3.2.1

Compute the central tendency and variability

To assess model performance, we evaluate prediction accuracy on a dataset’s test segment after i
epochs of training. That is, we use an epoch-to-accuracy metric to compare models across different
experimental settings (described in Section 3.1).
Given the stochastic nature of optimization, 100 experimental repetitions can result in varying
accuracy values, detailed in Appendix J. Thus, we cannot use the epoch-to-accuracy metric directly.
Instead, we will measure its central tendency and variability. To mitigate the influence of outliers
and skewness in the data, we use the median rather than the mean to measure central tendency and
the interquartile range3 (IQR) rather than the standard deviation to measure variability.
Our analysis compares the performance of two models, each employing a PRNG- or QRNG-based
source of randomness, across the same dataset, architecture, optimizer, and initializer algorithm. As
discussed in Section 3.1, we have 240 experiments, resulting in 120 pairs of models.
Specifically, we calculate the percentage of relative change in median epoch-to-accuracy (i.e., the
number of epochs required to reach a specific median accuracy threshold):
E(A) =

EQ (A) + ∆Q − EP (A)
× 100,
EP (A)

(1)

where E(A) is an efficiency metric, EQ (A) and EP (A) are the epoch numbers at which the QRNGand PRNG-based versions of an initializer first reach or exceed the median accuracy A, respectively,
and ∆Q = 4 represents additional epochs required so select a good starting seed for QRNG initializer
(see Appendix C.2.2 for details). Using this formula, we can determine how much faster one model
is compared to another in terms of the median epoch-to-accuracy.
Additionally, we assess the variation in accuracy between the two models by the difference of
their IQRs:
D(A) = DQ (A) − DP (A),
(2)
where DQ (A) and DP (A) are the IQR of accuracy measurements for the QRNG-based and PRNGbased versions of the initializer at epochs EQ (A) and EP (A), respectively.
Further details on calculating our performance metrics, E(A) and D(A), are provided with
numeric examples in Appendix G.
3.2.2

Application of performance metrics

In order to compare the performance of models, let us construct four simple classification rules
denoted by S(·) . Note that the maximum accuracy values Amax
may differ with experimental
(·)
configuration. We are not comparing the efficacy of different initializers across all configurations but
rather examining a specific pair of models individually, as discussed in Section 3.2.1.
Compare maximum accuracy values To assess the correctness of prediction, we will use the
values Amax
and Amax
P
Q , which represent the maximum median accuracy attained by the PRNG-based
and QRNG-based models, respectively, for each combination of dataset, architecture, optimizer, and
initializer algorithm.
3

The IQR of a vector x is computed as the difference between the third and first quartile of the data in x.

5

Our first step is to determine which of the two models achieved the highest accuracy. To do this,
we perform a non-parametric, non-paired Mann-Whitney U test [39] implemented in R v.4.4.0 [50].
The null hypothesis is that the distributions of ζ and η differ by a location shift of 0. The one-sided
alternative “greater” hypothesis is that ζ is shifted to the right of η, and the one-sided alternative
“less” hypothesis is that ζ is shifted to the left of η.
In our study, ζ represents a distribution of 100 accuracy values achieved by the PRNG-based
model at the earliest epoch when Amax
was reached, denoted E(Amax
P
P ). Similarly, η represents a
distribution of 100 accuracy values achieved by the QRNG-based model at the earliest epoch when
Amax
was reached, denoted E(Amax
Q
Q ). We set the p-value for the U test at 0.05, resulting in the
following classification rule:

win, if p-value for the U test with “less” alternative hypothesis < 0.05
 max max  
SA AP , AQ
= loss, if p-value for the U test with “greater” alternative hypothesis < 0.05 .


tie, otherwise
(3)
Compare efficiency and relative variability of achieving higher accuracy We also need a
simple but informative mechanism to aggregate the large amount of values returned by E(A) and
D(A). Let us examine the maximum median epoch-to-accuracy values reached by various setups.
To compare the “speed” with which one model achieves higher accuracy than another, we define:

max
Am = min Amax
,
(4)
P , AQ
where Amax
and Amax
represent the maximum accuracies attained by the PRNG-based and QRNGP
Q
based models, respectively, for each combination of dataset, architecture, optimizer, and initializer
algorithm. The value Am reflects the highest accuracy achieved by the less effective model in each
pair4 . E(Am ) indicates the relative speed at which each model reaches Am , while D(Am ) measures
the difference in variability at the epochs where each model reaches Am .
The following are classification rules we use to summarize and compare the performance (in
terms of the best accuracy achieved) where QRNG might outperform PRNG. For E(Am ), we simply
assess the count of epochs with the following conditions:


win, if E(Am ) < 0
SE [E(Am )] = tie, if E(Am ) = 0 .
(5)


loss, if E(Am ) > 0
For D(Am ), we use the non-parametric Fligner-Killeen median test5 [15] of homogeneity of variances
(implemented in R v.4.4.0) to assess whether variability is similar. The null hypothesis is that the
variances are the same in each group. We use the following classification rules since the test does
not have a one-tailed version:


win, if the Fligner-Killeen test’s p-value < 0.05 and D(Am ) < −0.01



loss, if the Fligner-Killeen test’s p-value < 0.05 and D(A ) > 0.01
m
SD [D(Am )] =
.

tie,
if
the
Fligner-Killeen
test’s
p-value
<
0.05
and
−
0.01
≤ D(Am ) ≤ 0.01



tie, if the Fligner-Killeen test’s p-value ≥ 0.05
(6)
4
5

The formulation of Am ensures that we do not observe the special case described in Appendix G.
The test is considered robust against deviations from normality [12].

6

From a practical perspective, a negligible difference is defined as an IQR difference of ±0.01, and
these outcomes are classified as ties. An example of computing SE [E(Am )] and SD [D(Am )] is given
in Appendix G.
Joining the classification rules To conclude whether a PRNG- or QRNG-based model achieved
better results in a given pair, we need to aggregate various permutations of the outputs returned by
the three S(·) rules. Table 1 presents the aggregation of permutations observed in our experiments.
We refer to this aggregation as the “final outcome”. For brevity, we only show a subset of permutations
of our experiments’ outcomes.
Table 1: The rules for aggregating the results of the three S(·) classification rules. The symbol *
denotes any outcome.
Final outcome

SA

SE

SD

Mask

Description

Loss

loss
tie

*
loss

*
*

l:l,*,*
l:t,l,*

QRNG-based model achieved a lower level of accuracy.
QRNG-based model achieved comparable accuracy slower.

Tie

tie

tie

tie

t:t,t,t

QRNG-based model achieved comparable accuracy simultaneously (with similar variation).

tie

win

win or tie

win

loss

*

win

tie

win or tie

win

win

loss

win

win

win or tie

QRNG-based model achieved comparable accuracy faster
(with less or similar variation).
w:w,l,*
QRNG-based model achieved a higher level of accuracy faster
and reached Am values slower.
w:w,t,wt QRNG-based model achieved a higher level of accuracy faster
and reached Am values simultaneously (with less or similar
variation).
w:w,w,l
QRNG-based model achieved a higher level of accuracy faster
and reached Am values faster (with higher variation).
w:w,w,wt QRNG-based model achieved a higher level of accuracy faster
and reached Am values faster (with less or similar variation).

Win

3.3

w:t,w,wt

Answers to the Research Questions

This section addresses the research questions and evaluates the hypotheses presented in Section 1.
As mentioned in Section 3.1, we deliberately simplify the models to isolate the effect of random
number generators, so the absolute accuracy of the models is far from best-in-class. However, we
are interested in the differences in performance between a pair of models rather than their absolute
accuracy.
The raw data that were used to compute the E(A) and D(A) values for the 120 model pairs are
given in Appendix J. Appendix I provides a summary of the values of Amax
(·) , E(·) (Am ), and D(·) (Am ),
max , denoted by ᾱ, and the average
where (·) denotes either P or Q. The average values of Amax
−
A
Q
P
values of E(Am ), denoted by Ē(Am ), are also given in Appendix I. We will use these data to answer
the questions below.
3.3.1

Overview of the results

Before addressing specific research questions, let us review the overall results. Table 2 shows that
the QRNG-based models win 60% of the 120 experiments, tie 1%, and lose 39%.
Drilling down into specific reasons for winning, we can see that 54% of the 72 winning experiments
are all-around wins (w:w,w,wt), where QRNG-based models achieve a higher level of accuracy
7

faster. In 22% of cases, QRNG-based models achieve higher accuracy, but Am values were reached
more slowly. Another 13% result in the outcome w:t,w,wt, where QRNG-based models achieve
comparable accuracy faster. Similarly, 10% result in the outcome w:w,w,l, where QRNG-based
models achieve a higher level of accuracy faster but with higher variation. Finally, 1% result in the
outcome w:w,t,wt, where QRNG-based models achieve a higher level of accuracy and reach the
Am value simultaneously.
We have a single tie, where QRNG- and PRNG-based models achieve a comparable level of
accuracy simultaneously.
Losses are divided into two groups: 53% of the 47 losing experiments are those in which QRNGbased models achieve a similar level of accuracy more slowly (outcome l:t,l,*). The remaining 47%
of the experiments result in outcome l:l,*,*, where the QRNG-based model achieved a lower level
of accuracy than the PRNG-based model.
Furthermore, Figure 1 indicates that when QRNG-based models lose, the decrease in the
max
maximum median accuracy (Amax
Q − AP ) is minimal. Conversely, when QRNG-based models win,
the improvement in maximum median accuracy is substantial. This is further illustrated in the
empirical cumulative distribution function in Figure 2. The top 25% of losses range between ≈ 0.0031
and 0.0402, while the top 25% of wins range between ≈ 0.0775 and 0.3550. Therefore, it may be
beneficial to try QRNG-based models.
Now, let us explore the answers to the specific research questions.
Table 2: Count of the final outcomes grouped by optimizer, dataset, and model.
Adam
Outcome

CIFAR-10

SGD

IMDB

MNIST

CIFAR-10

CNN

MLP

LSTM

Transf.

CNN

MLP

CNN

l:l,*,*
l:t,l,*

4
2

1

1
6

2
1

5
3

5
1

1

Loss Total

6

1

7

3

8

6

1

Grand

IMDB

MNIST

MLP

LSTM

Transf.

CNN

5

2
4

1

2
2

5

6

1

Total

MLP
1

22
25

1

47

t:t,t,t

1

1

Tie Total

1

1

2

2
1

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

4

2
2

Win Total

4

9

2

7

Grand Total

10

10

10

10

3.3.2

5

5
2

8

9

9
16
1
7
39

4

9

9

72

10

10

10

120

2

3

3

3
2

1

4

1
6

2

4

9

8

5

10

10

10

10

10

1
4

RQ1. What effect does the selection of optimization algorithms have on deep
neural networks’ performance when initialized using the QRNG-based weight
initialization scheme?

Table 2 shows significant performance differences in models based on the optimizer used. Models using
QRNG-based initializers perform better with the SGD optimizer compared to Adam. Specifically,
with SGD, models that use QRNG-based initializers outperform PRNG-based ones in 73% of
experiments without ties (ᾱ ≈ 0.06). With Adam, they win in 47% of 60 cases (ᾱ ≈ 0.05) and tie in
2%. This superior performance with SGD is likely due to SGD’s inability to dynamically adjust the
8

40

Experiments count

30

Final outcome
loss
tie

20

win

10

0
0.0

0.1

0.2
max

AQ

0.3

max

− AP

max values for three final outcomes.
Figure 1: Histogram of the Amax
Q − AP

learning rate, while Adam’s dynamic learning rate can compensate for the poorer starting conditions
of the PRNG initializer. For ties and losses, ᾱ ≈ 0.00, reinforcing that QRNG-based initialization
may be beneficial.
3.3.3

RQ2. How does a QRNG-based weight initialization scheme affect deep neural
networks’ performance?

Table 3 indicates that QRNG initializers generally outperform PRNG initializers. Shape-agnostic
initializers win in 47% of 36 experiments without ties; shape-dependent initializers win in 67% of 72
cases and tie in 1%; orthogonal initializers win in 58% of 12 experiments without ties. The average
improvement in the maximum median accuracy for shape-agnostic, shape-dependent, and orthogonal
initializers is ᾱ ≈ 0.02%, 0.07%, and 0.06%, respectively.
For individual initializers, Random Uniform performs poorly, winning only in 25% of the cases,
while Random Normal and Truncated Normal win in 58%. For shape-dependent initializers, He
Normal wins in 50% of the cases, while others win or tie in 67% to 75%. Thus, QRNG benefits a
range of initializers, perhaps with the exception of Random Uniform.
3.3.4

RQ3. How does the choice of deep neural network architecture affect the model’s
performance when it is initialized using the QRNG-based weight initialization
scheme?

For CNN architecture, QRNG-based initializers win in 60% of 40 cases (ᾱ ≈ 0.08) without tie,
for MLP — win in 75% of 40 cases (ᾱ ≈ 0.07) without tie, for LSTM — win in 35% of 20 cases
(ᾱ ≈ 0.00) and tie in 5%, for Transfomer — win in 55% of 20 cases (ᾱ ≈ 0.01) without ties.
Thus, the strongest improvements come from MLP and CNN architectures trained on computer

9

Cumulative distribution function

1.00

0.75

Final outcome
loss
0.50

tie
win

0.25

0.00
1e-05

1e-04

1e-03
max

|AQ

1e-02
max

− AP

1e-01

| + 10−5

max
−5 for three final outcomes.
Figure 2: Empirical cumulative distribution function of |Amax
Q − AP | + 10
The 10−5 term is added to enable rendering of the x-axis values on a log scale.

vision datasets. However, Ē(Am ) ≈ −36% for LSTM and Ē(Am ) ≈ −7% for Transformer, suggesting
that QRNG-based initialization may not always yield higher accuracy but can achieve comparable
accuracy faster, reducing training time and saving computational resources.
Moreover, in 60% of the 10 LSTM cases using the Adam optimizer, the result is l:t,l,* (i.e.,
the same accuracy is achieved slower). The comparison of raw data in Appendix J suggests that
LSTM reach the value Am more quickly. However, the seed selection penalty (∆Q = 4) results in a
performance loss. Improving the seed selection heuristic could potentially mitigate this issue in the
future (see Section 3.4.3 for more details).
Consequently, all deep neural network architectures studied show benefits in either maximum
accuracy or training speed.
3.3.5

RQ4. How does the change in the dataset affect the performance of deep neural
networks initialized with the QRNG-based weight initialization scheme?

Table 2 indicates that the impact of different datasets varies. For the MNIST dataset, QRNG-based
initializers win in 60% of 40 cases (ᾱ ≈ 0.03) without ties. For CIFAR-10, they win in 75% of 40
cases (ᾱ ≈ 0.11) without tie. For IMDB, they win in 45% of 40 cases (ᾱ ≈ 0.00) and tie in 3%.
Partitioning data by optimizer reveals more details. For MNIST with SGD, models with QRNGbased initializers win in 90% of 20 cases (ᾱ ≈ 0.04), while with Adam, they win in 30% of cases
(ᾱ ≈ 0.01).
Our analysis of the accuracy distributions given in the Appendix J suggests that the improvement
of the accuracy of PRNG and QRNG with epochs is similar. The accuracy distributions in Appendix J
suggest similar accuracy improvements with epochs for PRNG- and QRNG-based models, but the
QRNG seed selection heuristic incurs a computational penalty ∆Q . This issue may be resolved in
10

Table 3: Count of the final outcomes grouped by initializer.
Shape dependent
Outcome

Glorot
normal

Glorot
uniform

He
normal

He
uniform

Lecun
normal

Lecun
uniform

l:l,*,*
l:t,l,*

3
1

1
2

2
4

1
3

3

3

Loss Total

4

3

6

4

3

3

Shape agnostic

Orthogonal

Grand

Random
normal

Random
uniform

Truncated
normal

Orthogonal

Total

5

6
3

4
1

5

22
25

9

5

5

47

5

t:t,t,t

1

1

Tie Total

1

1

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

2
2

1
2

2

3

1
2

1
2

1
3

1
5

1
3

4

1
5

1
5

1

1

1

6

1
1

6

1
3
1
1
1

9
16
1
7
39

Win Total

8

9

6

7

9

9

7

3

7

7

72

Grand Total

12

12

12

12

12

12

12

12

12

12

120

the future by improving seed selection heuristics (we will discuss this in Section 3.4.3).
For CIFAR-10, the results are more balanced: QRNG-based initializers win in 75% of 20 cases
with SGD (ᾱ ≈ 0.11) and in 65% of 20 cases with Adam (ᾱ ≈ 0.10). Compared to MNIST, CIFAR-10
is a more challenging dataset, making model learning more difficult. Thus, QRNG-based initialization
may help learn more complex data representations.
For IMDB, the results are fairly balanced. Models with QRNG-based initializers win in 45% of
the 20 cases with Adam (ᾱ ≈ 0.00) and tie in 5% of the experiments, while with SGD they win in
45% of the cases (ᾱ ≈ 0.00).
Based on these data, is QRNG-based initialization more helpful for computer vision datasets than
natural language datasets? The short answer is “not necessarily.” While, on average, we were unable
to improve the accuracy of the LSTM and Transformer models using QRNG-based initialization
(trained on the natural language dataset), we saw that the same accuracy can be achieved faster (as
shown in Section 3.3.4), which is still advantageous.
3.3.6

Hypothesis

The answers to the research questions support the hypothesis that QRNG-based weight initialization
can significantly accelerate neural network training. Improvements are evident across data sets,
architectures, initializers, and optimization algorithms.

3.4

Discussion

3.4.1

Practical considerations

We provide a reference implementation of QRNG-based initializers for Keras v.3.3.3, which can
readily use three different backends: JAX [7], PyTorch, or TensorFlow. It can also be easily ported
to other popular machine learning frameworks. The changes to the initializer’s underlying random
number generator do not require any changes downstream: the code for compiling the model and
training it remains unchanged. By passing a QRNG-based initializer instance to a neural network
layer, practitioners can treat these implementations as “black boxes”. This process conceals the
complexities of the underlying implementation from the practitioner. Those who wish to engage

11

more deeply may draw random values with specific seeds that can be manually applied to the model
weights.
In Appendix H, we observe that a simple neural network performs better with a number of
weights following the pattern 4, 8, 12, . . .. Thus, when designing neural network layers, it is advisable
to select the number of weights that adhere to this pattern (which is often the default behaviour6 ).
3.4.2

Limitations

This section discusses limitations and threats to the validity of our study classified as per [70].
Inaccuracies in our implementation or in the code of the underlying packages can affect the results,
threatening internal validity. To mitigate the first threat, our QRNG-based implementations have
undergone code peer review and were tested against Keras/TensorFlow PRNG-based distributions.
The second threat may be related to issues with the implementation of Philox PRNG in TensorFlow
backend. By training a simple MLP for a single epoch using Philox and the Mersenne Twister
algorithm, we found that the distributions of the results were similar (see Appendix H.1 for details).
Consequently, the observed behaviour is not unique to a particular PRNG or implementation.
To focus on the effect of random generators, we need to isolate other sources that may affect the
model’s performance. We identified five main areas affecting model performance that may threaten
conclusion validity: (a) random selection of training, validation, and testing datasets, (b) stochastic
nature of the optimizer, (c) sophisticated constructs in models’ archtitecture, (d) speed of the
QRNG-based initializers, and (e) limited number of epochs.
To address the first two issues, we ran each experiment 100 times. Although data shuffling (done
after each epoch) may yield different outcomes, we expect the relative performance differences to
remain consistent. We ran the formal experiments only on CPUs to eliminate the effect of randomness
from the GPU software stack (although we ran the code informally on GPUs and observed similar
results).
To address the third issue, we simplified model architectures to avoid the effects of more
sophisticated techniques. Although this reduces the overall model efficacy, our primary goal is to
assess the effects of random generators, not optimize performance.
QRNG-based initializers
 are generally slower than PRNG-based initializers. The computational
2
complexity is O d Nmax , where Nmax is the maximum number of elements for any of the d
dimensions; see Appendix B.3 for details and a way to reduce the complexity to O (dNmax ). However,
the initialization time for setting the weights before training is negligible compared to the duration of
model training. We timed our initialization code to assess the effect of the fourth issue. Appendix B.3
shows that drawing the values from the distributions takes only a fraction of a second, while training
models can take minutes or longer. As a result, QRNG-based initializer can reduce the overall
training time and achieve an accuracy comparable to PRNG with fewer epochs.
Finally, we capped training at 30 epochs in our experiments to ensure consistency across all
setups. Eventually, if we train our models for a larger number of epochs, PRNG-based models may
become more accurate than QRNG-based models, or vice versa (i.e., a different model may reach
a higher value of Am than we have seen in our experiments). The data in Appendix J show that
this is unlikely for most cases based on the analysis of the raw accuracy plots and the changes in
the median accuracy. However, it is possible, though not likely, that extended training could yield
different results. Thus, our findings should be interpreted within the context of a 30-epoch training
limit.
The limited generalizability of our findings challenges its external validity. Although we examined
three datasets (from two modalities), four model architectures, two optimizers, and ten initializers,
6

As 2n is divisible by 4 for n ≥ 2.

12

we cannot guarantee that these results will apply to other datasets, architectures, optimizers, or
initializers. However, the promising results warrant further investigation. The same empirical
methodology can be applied to other modeling scenarios through rigorously designed experiments.
The following are some open questions for future research.
3.4.3

Open questions

In our study, we demonstrated that QRNG-based initializers can outperform PRNG-based ones.
Many questions remain unanswered; below, we sketch some potential future research avenues.
Why is QRNG-based initialization effective? It is possible that insights from Monte Carlo methods
research, where QRNG initialization is commonly used, can be applied here. The dimensional integers
used to seed QRNG (see Appendix A for details) are optimized pairwise (by minimizing the number
of bad correlations between pairs of variables) [26]. The creators of these dimensional integers suggest
that the success of simple pairwise optimization may be because many practical problems can be
reduced to “low effective dimension” [9], which “means either that the integrand depends mostly
on the initial handful of variables, or that the integrand can be well approximated by a sum of
functions with each depending on only a small number of variables at a time” [26], see [66]–[68] for
further analysis. These relations capture enough information necessary for decision making. If we
apply similar logic to neural networks initialization, initializing the weights uniformly at the level of
individual layers and adding additional uniformity for each pair of layers may give an optimizer a
better initial condition than RPNG-based initialization. In order to test whether these conjectures
remain valid empirically, more complex models could be tested on larger, more intricate datasets
and tasks.
Why Random Uniform initializer benefits the least from QRNG? In Section 3.3.3, we observe
that most initializers benefit from QRNG with the exception of the Random Uniform. This initializer
mutates starting random numbers the least, simply by rescaling them to the [−0.05, 0.05] range
(see Appendix C for details). Can this behaviour help us better understand why QRNG-based
initialization yields better results in other cases?
How does the performance of QRNG-based initializers vary with the number of weights? As
mentioned in Section 3.4.1, a simple neural network performs better with a number of weights
following the pattern 4, 8, 12, . . .. This raises questions about the underlying factors of this behaviour,
perhaps related to balance and QRNGs’ low-discrepancy characteristics.
When a sequence length is not a power of two, Sobol’ sequences lose balance [48]. Although our
preliminary tests with a simple model (in Appendix H.2) did not show significant practical effects of
this factor, we speculate that these effects may be more pronounced in larger and more complex
models. Therefore, it is an open question whether incorporating this factor into the design of neural
network layers can further improve QRNG initialization.
Sobol’ sequence performance may be degraded [48] if a first element is removed (as in our
implementation). According to our initial analysis, excluding the zero element did not significantly
affect neural network training and initialization. However, further investigation is needed to determine
whether the QRNG performance can be improved.
The seeds for our QRNG-based initializers are selected sequentially using a heuristic (Appendix C.2), which is not optimal (as shown in Appendix H.2). An alternative approach to seed
selection7 or refining the heuristic could improve our results.
We have only explored one version of the Sobol’ sequences (with a specific set of direction
numbers). Other versions exist (e.g., scrambled sequences [47] or those with optimization after
7
For example, selecting a seed at random for each layer, which would set ∆Q = 0, and potentially leading to a
reduction of l:t,lt,* and w:w,lt,* outcomes, where lt would be replaced with w.

13

sampling [56]) raising the question of which version is most effective for QRNG applications in
initializers. Our initial analysis shows scrambled sequences produce similar results to unscrambled
sequences but introduce numerical instability. However, quantitative assessments need more research.
Other quasirandom sequences exist, such as the Faure, Halton, and Niederreiter sequences. Would
those sequences perform better than Sobol’s sequences if we used them in QRNG-based initializers?
QRNG has a greater impact on shape-dependent and orthogonal initializers than on shapeagnostic ones (as shown in Section 3.3.3). More research is required to determine the reasons for
these discrepancies.
Furthermore, while our initial experiments isolated QRNG’s effects using simple models and
datasets, further research would require scaling up complexity, varying data modalities and volumes,
model sizes, and optimizers to fully grasp QRNG’s implications.

4

Related Work

The following is a brief summary of related literature; all these works are complementary to our own.
Low-discrepancy sequences were tried for various tasks in machine learning, such as selecting a
subset of data for training [44], generating neural networks [28] and their surrogates [37], replacing the
backpropagation algorithm [27], improving optimizers [2], updating belief in Bayesian networks [10],
and tuning hyperparameters and improving regularization [3]. To the best of our knowledge,
researchers have not yet used QRNGs to initialize neural network weights.
Other random number generators have been explored to initialize neural networks. For example,
node dropout regularization was performed using true random number generators [31]. Finally,
machine learning models (including dense neural networks) may perform better when initialized
with random numbers generated by quantum computers [5] (although other researchers were unable
to replicate these results [22]).

5

Summary

We demonstrated that QRNG-based initializers can achieve higher accuracy or speed up neural
network training in 60% of the 120 experiments conducted. The top 25% of the improved maximum
median accuracies range between ≈ 0.0775 and 0.3550. The negative side effects of trying QRNG are
minimal, with the top 25% of observed losses resulting in a drop in accuracy between ≈ 0.0031 and
0.0402. This trend is observed across various data types, architectures, optimizers, and initializers.
The extent to which this effect generalizes remains to be seen. We encourage the community to
explore and validate our findings.

Acknowledgements
This work was partially supported by the Natural Sciences and Engineering Research Council
of Canada (grant # RGPIN-2022-03886). The authors thank the Digital Research Alliance of
Canada and the Department of Computer Science at Toronto Metropolitan University for providing
computational resources. The authors also express their gratitude to the members of our research
group — Montgomery Gole, Mohammad Saiful Islam, and Mohamed Sami Rakha — for their
valuable feedback on the manuscript and insightful discussions.

14

References
[1]

I. Antonov and V. Saleev, “An economic method of computing lpτ -sequences,” USSR Computational Mathematics
and Mathematical Physics, vol. 19, no. 1, pp. 252–256, 1979, issn: 0041-5553. doi: 10.1016/0041-5553(79)900855.

[2]

W. H. Bangyal, K. Nisar, T. R. Soomro, A. A. Ag Ibrahim, G. A. Mallah, N. U. Hassan, and N. U. Rehman,
“An improved particle swarm optimization algorithm for data classification,” Applied Sciences, vol. 13, no. 1,
p. 283, 2022. doi: 10.3390/app13010283.

[3]

J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” J. Mach. Learn. Res., vol. 13,
pp. 281–305, 2012. doi: 10.5555/2503308.2188395.

[4]

J. Bilmes, K. Asanovic, C.-W. Chin, and J. Demmel, “Using phipac to speed error back-propagation learning,”
in 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 5, 1997, pp. 4153–4156.
doi: 10.1109/ICASSP.1997.604861.

[5]

J. J. Bird, A. Ekárt, and D. R. Faria, “On the effects of pseudorandom and quantum-random number generators
in soft computing,” Soft computing, vol. 24, no. 12, pp. 9243–9256, 2020. doi: 10.1007/s00500-019-04450-0.

[6]

G. E. P. Box and M. E. Muller, “A Note on the Generation of Random Normal Deviates,” The Annals of
Mathematical Statistics, vol. 29, no. 2, pp. 610–611, 1958. doi: 10.1214/aoms/1177706645.

[7]

J. Bradbury et al., JAX: Composable transformations of Python+NumPy programs, version 0.3.13, 2018. [Online].
Available: http://github.com/google/jax.

[8]

P. Bratley and B. L. Fox, “Algorithm 659: Implementing sobol’s quasirandom sequence generator,” ACM
Transactions on Mathematical Software (TOMS), vol. 14, no. 1, pp. 88–100, 1988. doi: 10.1145/42288.214372.

[9]

R. Caflisch, W. Morokoff, and A. Owen, “Valuation of morgage backed securities using brownian bridges to
reduce effective dimension,” The Journal of Computational Finance, vol. 1, pp. 27–46, 1997. doi: 10.21314/JCF.
1997.005.

[10]

J. Cheng and M. J. Druzdzel, “Computational investigation of low-discrepancy sequences in simulation algorithms
for bayesian networks,” in UAI ’00: Proceedings of the 16th Conference in Uncertainty in Artificial Intelligence,
Morgan Kaufmann, 2000, pp. 72–81. [Online]. Available: https://dl.acm.org/doi/pdf/10.5555/2073946.
2073956.

[11]

F. Chollet et al., Keras, https://keras.io, 2015.

[12]

W. J. Conover, M. E. Johnson, and M. M. Johnson, “A comparative study of tests for homogeneity of variances,
with applications to the outer continental shelf bidding data,” Technometrics, vol. 23, no. 4, pp. 351–361, 1981.
doi: 10.1080/00401706.1981.10487680.

[13]

L. Devroye, Non-Uniform Random Variate Generation. Springer, 1986. doi: 10.1007/978-1-4613-8643-8.

[14]

H. Faure, “Discrépance de suites associées à un système de numération (en dimension s),” fre, Acta Arithmetica,
vol. 41, no. 4, pp. 337–351, 1982. [Online]. Available: http://eudml.org/doc/205851.

[15]

M. A. Fligner and T. J. Killeen, “Distribution-free two-sample tests for scale,” Journal of the American Statistical
Association, vol. 71, no. 353, pp. 210–213, 1976. doi: 10.1080/01621459.1976.10481517.

[16]

K. Fukushima, “Visual feature extraction by a multilayered network of analog threshold elements,” IEEE Trans.
Syst. Sci. Cybern., vol. 5, no. 4, pp. 322–333, 1969. doi: 10.1109/TSSC.1969.300225.

[17]

X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in
Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS,
ser. JMLR Proceedings, vol. 9, JMLR.org, 2010, pp. 249–256. [Online]. Available: http://proceedings.mlr.
press/v9/glorot10a.html.

[18]

I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016. [Online]. Available: http :
//www.deeplearningbook.org.

[19]

J. H. Halton, “On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional
integrals,” Numerische Mathematik, vol. 2, pp. 84–90, 1960. doi: 10.1007/BF01386213.

[20]

B. Hanin and D. Rolnick, “How to start training: The effect of initialization and architecture,” in Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS, 2018, pp. 569–579. [Online]. Available: https://proceedings.neurips.cc/paper/2018/hash/
d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html.

15

[21]

K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification,” in 2015 IEEE International Conference on Computer Vision, ICCV, IEEE Computer
Society, 2015, pp. 1026–1034. doi: 10.1109/ICCV.2015.123.

[22]

R. Heese, M. Wolter, S. Mücke, L. Franken, and N. Piatkowski, “On the effects of biased quantum random
numbers on the initialization of artificial neural networks,” Mach. Learn., vol. 113, no. 3, pp. 1189–1217, 2024.
doi: 10.1007/S10994-023-06490-Y.

[23]

S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780,
1997. doi: 10.1162/NECO.1997.9.8.1735.

[24]

J. J. Hopfield, “Neural networks and physical systems with emergent collective computational abilities.,”
Proceedings of the national academy of sciences, vol. 79, no. 8, pp. 2554–2558, 1982. doi: 10.1073/pnas.79.8.
2554.

[25]

P. Jäckel, Monte Carlo methods in finance. John Wiley & Sons, 2002.

[26]

S. Joe and F. Y. Kuo, “Constructing sobol sequences with better two-dimensional projections,” SIAM Journal
on Scientific Computing, vol. 30, no. 5, pp. 2635–2654, 2008. doi: 10.1137/070709359.

[27]

I. Jordanov and R. Brown, “Neural network learning using low-discrepancy sequence,” in Advanced Topics in
Artificial Intelligence, 12th Australian Joint Conference on Artificial Intelligence, AI ’99, Sydney, Australia,
December 6-10, 1999, Proceedings, N. Y. Foo, Ed., ser. Lecture Notes in Computer Science, vol. 1747, Springer,
1999, pp. 255–267. doi: 10.1007/3-540-46695-9\_22.

[28]

A. Keller and M. V. Keirsbilck, “Artificial neural networks generated by low discrepancy sequences,” in Monte
Carlo and Quasi-Monte Carlo Methods - MCQMC, ser. Springer Proceedings in Mathematics and Statistics,
vol. 387, Springer, 2020, pp. 291–311. doi: 10.1007/978-3-030-98319-2\_15.

[29]

D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference on
Learning Representations, ICLR 2015, 2015. [Online]. Available: http://arxiv.org/abs/1412.6980.

[30]

G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-normalizing neural networks,” in Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, 2017, pp. 971–980. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2017 / hash /
5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html.

[31]

A. Koivu, J.-P. Kakko, S. Mäntyniemi, and M. Sairanen, “Quality of randomness and node dropout regularization
for fitting neural networks,” Expert Systems with Applications, vol. 207, p. 117 938, 2022, issn: 0957-4174. doi:
10.1016/j.eswa.2022.117938.

[32]

A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” Tech. Rep., 2009.
[Online]. Available: https://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf.

[33]

S. S. Kucherenko and Y. Sytsko, “Application of deterministic low-discrepancy sequences in global optimization,”
Comput. Optim. Appl., vol. 30, no. 3, pp. 297–318, 2005. doi: 10.1007/S10589-005-4615-1.

[34]

“Layer weight initializers — Keras v3 manual.” (2024), [Online]. Available: https://keras.io/api/layers/
initializers/.

[35]

“Layer weight initializers — Orthogonal — Keras v3 manual.” (2024), [Online]. Available: https://keras.io/
api/layers/initializers/#orthogonalinitializer-class.

[36]

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,”
Proc. IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. doi: 10.1109/5.726791.

[37]

M. Longo, S. Mishra, T. K. Rusch, and C. Schwab, “Higher-order quasi-monte carlo training of deep neural
networks,” SIAM J. Sci. Comput., vol. 43, no. 6, A3938–A3966, 2021. doi: 10.1137/20M1369373.

[38]

A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, “Learning word vectors for sentiment
analysis,” in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies, Portland, Oregon, USA: Association for Computational Linguistics, Jun. 2011, pp. 142–
150. [Online]. Available: http://www.aclweb.org/anthology/P11-1015.

[39]

H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two Random Variables is Stochastically Larger
than the Other,” The Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50–60, 1947. doi: 10.1214/aoms/
1177730491.

[40]

Martín Abadi et al., TensorFlow: Large-scale machine learning on heterogeneous systems, Software available
from tensorflow.org, 2015. [Online]. Available: https://www.tensorflow.org/.

16

[41]

M. Matsumoto and T. Nishimura, “Mersenne twister: A 623-dimensionally equidistributed uniform pseudorandom number generator,” ACM Trans. Model. Comput. Simul., vol. 8, no. 1, pp. 3–30, Jan. 1998. doi:
10.1145/272991.272995.

[42]

“Mersenne twister (mt19937) — numpy v1.24 manual.” (2023), [Online]. Available: https://numpy.org/doc/1.
24/reference/random/bit_generators/mt19937.html.

[43]

A. Miranskyy, A. Sorrenti, and V. Thakar. “Source code for this paper.” (2024), [Online]. Available: https:
//github.com/miranska/qrng-init.

[44]

S. Mishra and T. K. Rusch, “Enhancing accuracy of deep learning algorithms by training with low-discrepancy
sequences,” SIAM J. Numer. Anal., vol. 59, no. 3, pp. 1811–1834, 2021. doi: 10.1137/20M1344883.

[45]

A. Nandan. “Text classification with transformer — keras manual.” (2020), [Online]. Available: https://keras.
io/examples/nlp/text_classification_with_transformer/.

[46]

H. Niederreiter, “Low-discrepancy and low-dispersion sequences,” Journal of number theory, vol. 30, no. 1,
pp. 51–70, 1988. doi: 10.1016/0022-314X(88)90025-X.

[47]

A. B. Owen, “Scrambling sobol’ and niederreiter-xing points,” J. Complex., vol. 14, no. 4, pp. 466–489, 1998.
doi: 10.1006/JCOM.1998.0487.

[48]

A. B. Owen, “On dropping the first sobol’ point,” in Monte Carlo and Quasi-Monte Carlo Methods, A. Keller,
Ed., Cham: Springer International Publishing, 2022, pp. 71–86. doi: 10.1007/978-3-030-98319-2_4.

[49]

S. J. Prince, Understanding Deep Learning. MIT Press, 2023. [Online]. Available: http://udlbook.com.

[50]

R Core Team, R: A language and environment for statistical computing, R Foundation for Statistical Computing,
Vienna, Austria, 2024. [Online]. Available: https://www.R-project.org/.

[51]

S. Ramasinghe, L. E. MacDonald, M. R. Farazi, H. Saratchandran, and S. Lucey, “How much does initialization
affect generalization?” In International Conference on Machine Learning, ICML, ser. Proceedings of Machine
Learning Research, vol. 202, PMLR, 2023, pp. 28 637–28 655. [Online]. Available: https://proceedings.mlr.
press/v202/ramasinghe23a.html.

[52]

“Random number generation — tensorflow core.” (2024), [Online]. Available: https://www.tensorflow.org/
guide/random_numbers#algorithms.

[53]

F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain.,”
Psychological review, vol. 65, no. 6, pp. 386–408, 1958. doi: 10.1037/h0042519.

[54]

J. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw, “Parallel random numbers: As easy as 1, 2, 3,” in
Conference on High Performance Computing Networking, Storage and Analysis, S. A. Lathrop, J. Costa, and
W. Kramer, Eds., ACM, 2011, 16:1–16:12. doi: 10.1145/2063384.2063405.

[55]

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks,” in 2nd International Conference on Learning Representations, ICLR, 2014. [Online].
Available: http://arxiv.org/abs/1312.6120.

[56]

“Scipy.stats.qmc.sobol — scipy v1.11 manual.” (2023), [Online]. Available: https://docs.scipy.org/doc/
scipy/reference/generated/scipy.stats.qmc.Sobol.html.

[57]

M. Skorski, A. Temperoni, and M. Theobald, “Revisiting weight initialization of deep neural networks,” in Asian
Conference on Machine Learning, ACML, ser. Proceedings of Machine Learning Research, vol. 157, PMLR,
2021, pp. 1192–1207. [Online]. Available: https://proceedings.mlr.press/v157/skorski21a.html.

[58]

I. M. Sobol’, “On the distribution of points in a cube and the approximate evaluation of integrals,” USSR
Computational Mathematics and Mathematical Physics, vol. 7, no. 4, pp. 86–112, 1967, issn: 0041-5553. doi:
https://doi.org/10.1016/0041-5553(67)90144-9.

[59]

“Tf.math.sobol_sample — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.
org/api_docs/python/tf/math/sobol_sample.

[60]

“Tf.random.algorithm — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/Algorithm.

[61]

“Tf.random.normal — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/normal.

[62]

“Tf.random.truncated_normal — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.
org/api_docs/python/tf/random/truncated_normal.

17

[63]

“Tf.random.uniform — tensorflow v2.16 manual.” (2024), [Online]. Available: https://www.tensorflow.org/
api_docs/python/tf/random/uniform.

[64]

A. Vaswani et al., “Attention is all you need,” in Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, 2017, pp. 5998–6008. [Online]. Available:
https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.

[65]

P. Virtanen et al., “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python,” Nature Methods,
vol. 17, pp. 261–272, 2020. doi: 10.1038/s41592-019-0686-2.

[66]

X. Wang and K.-T. Fang, “The effective dimension and quasi-monte carlo integration,” Journal of Complexity,
vol. 19, no. 2, pp. 101–124, 2003, issn: 0885-064X. doi: 10.1016/S0885-064X(03)00003-7.

[67]

X. Wang and I. H. Sloan, “Low discrepancy sequences in high dimensions: How well are their projections
distributed?” Journal of Computational and Applied Mathematics, vol. 213, no. 2, pp. 366–386, 2008. doi:
10.1016/j.cam.2007.01.005.

[68]

X. Wang and I. H. Sloan, “Why are high-dimensional finance problems often of low effective dimension?” SIAM
Journal on Scientific Computing, vol. 27, no. 1, pp. 159–183, 2005. doi: 10.1137/S1064827503429429.

[69]

M. Watson, C. Qian, J. Bischof, F. Chollet, et al., KerasNLP, 2022. [Online]. Available: https://github.com/
keras-team/keras-nlp.

[70]

R. K. Yin, Case Study Research: Design and Methods (Applied Social Research Methods). SAGE Publications,
2009, isbn: 9781412960991.

[71]

C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep learning requires rethinking
generalization,” in 5th International Conference on Learning Representations, ICLR, OpenReview.net, 2017.
[Online]. Available: https://openreview.net/forum?id=Sy8gdB9xx.

18

A

Low Discrepancy Sequences

Sobol’ sequences, introduced by Sobol’ in 1967 [58], are quasirandom sequences widely used in
numerical analysis and Monte Carlo methods [25]. They provide a deterministic and evenly distributed set of points in multidimensional space, thereby overcoming some limitations of traditional
pseudorandom sequences [25], [58].
Sobol’ sequences can be constructed efficiently [1] using the following recursive formula:
xn,k = xn−1,k ⊕2 vj,k ,

(7)

where xn,k is the n-th draw of Sobol’ integer in dimension k, vj,k is a direction integer (needed to
initialize the recursion), ⊕2 is a single XOR operation for each dimension. The recursion starts with
x0,k = 0.
Various methods exist to compute vj,k . In our work, we use Sobol’ sequences [58] implemented8
in SciPy v.1.13.1 software [56], [65], which is configured to produce Sobol’ sequences for up to
21 200 dimensions using direction integers vj,k computed by [26]. As we use individual dimensions to
initialize the networks’ layers, this number of dimensions is more than adequate. A user who needs
more than 21 200 dimensions may, e.g., reuse the dimensions or try scrambled versions [47] of Sobol’s
sequences.
In general, Sobol’ sequences have d dimensions and are in the range [0, 1)d . We discard the first
element of a sequence (which is always 0), similar to the TensorFlow v.2.16.1 implementation [40], [59].
Due to this, our implementation of Sobol’ sequences reduces the range to (0, 1)d . It is necessary to
discard this element in order to implement QRNG-based normal and truncated normal distributions,
which we will discuss in Appendices B.2.3 and B.2.4, respectively.

B

Sampling from distributions

In this work, we will need to sample values from univariate uniform, normal, and truncated normal
distributions, denoted by U(·) , N(·) , and T(·) , respectively. The subscript (·) indicates the random
number generator to use for sampling and takes the value P for PRNG and Q for QRNG.
Figure 3 shows two-dimensional projections for PRNGs and QRNGs under study. Note that
QRNG aims to cover a domain as homogeneously and uniformly as possible.
The distributions obtained using PRNG and QRNG are compared in Figure 4. The QRNG
version is “smoother” than the PRNG version. This is because QRNGs (that use Sobol’ sequences
discussed in Appendix A) have better uniformity properties than PRNGs. Below are details on how
distributions are implemented.

B.1

Using Keras/TensorFlow PRNG

Keras v.3.3.3 [11] can use three different backends: JAX, PyTorch, or‘ TensorFlow. We use TensorFlow
v.2.16.1 [40]. This version of Keras is designed to use backend implementations of the distributions.
TensorFlow v.2.16.1 uses continuous uniform, normal, or truncated normal distributions sampled with
PRNGs. These implementations are widely used and well tested. Thus, we treat them as baselines
and leverage as-is. Below are some notes on TensorFlow’s implementation of the distributions.
8

The implementation seems to be based on the Algorithm 659 [8].

19

PRNG - Seed 14 vs. 15

0.8

0.8

0.8

0.6

0.4

Seed = 10002

1.0

0.6

0.4

0.2

0.2

0.0
0.2

0.4
0.6
Seed = 1

0.8

1.0

0.6

0.4

0.2

0.0
0.0

0.0
0.0

QRNG - Seed 1 vs. 2

0.2

0.4
0.6
Seed = 14

0.8

1.0

0.0

QRNG - Seed 14 vs. 15
1.0

0.8

0.8

0.8

0.4

0.2

Seed = 10002

1.0

0.6

0.6

0.4

0.2

0.0
0.2

0.4
0.6
Seed = 1

0.8

1.0

0.4
0.6
Seed = 10001

0.8

1.0

0.6

0.4

0.2

0.0
0.0

0.2

QRNG - Seed 10001 vs. 10002

1.0

Seed = 15

Seed = 2

PRNG - Seed 10001 vs. 10002

1.0

Seed = 15

Seed = 2

PRNG - Seed 1 vs. 2
1.0

0.0
0.0

0.2

0.4
0.6
Seed = 14

0.8

1.0

0.0

0.2

0.4
0.6
Seed = 10001

0.8

1.0

Figure 3: A two-dimensional projection of the first 1024 draws of the Keras/TensorFlow pseudorandom
number generator (top pane) and the Sobol’ sequences (bottom pane). Axis labels denote seed values
of the random generator.
B.1.1

Uniform distribution

TensorFlow draws the values from UP in the range [0, 1) [63]. The PRNG algorithm used by
TensorFlow depends on the hardware [60]. In most cases, it will be either Philox or ThreeFry [54],
but it can also select another algorithm from the underlying libraries [60]. We may not know which
specific algorithm is used in advance (although it is typically Philox [52]). However, the selection of
the PRNG algorithm is deterministic (i.e., a testbed should select PRNG consistently).
We empirically validated that on our testbed the sequences produced by automatic selection
matched those produced by the Philox algorithm. Thus, our experiments initialize neural network
weights using Philox-based PRNGs. Tensorflow may use different PRNG algorithms for other sources
of randomness, such as those used by the SGD and Adam optimizers. Nevertheless, they will be
consistent for a given testbed.
B.1.2

Normal distribution

TensorFlow draws the values from NP in the range (−∞, +∞) [61]. The mean µ ∈ R and standard
deviation σ ∈ R>0 (governing the distribution) vary based on application. TensorFlow’s manual and
associated Python code listing do not describe the technical details of sampling from the distribution.
However, code analysis indicates that the Box-Muller transform [6] is used.

20

distribution_type = Uniform

distribution_type = Normal

distribution_type = Truncated Normal

500

400

Count

300

200

100

0

0.0

0.2

0.4

0.6

0.8

1.0

−4

values

−2

0

2

values

4

−2

−1

0

1

2

values

generator
PRNG

QRNG

Figure 4: Sample draws from univariate uniform, normal, and truncated normal distributions.
B.1.3

Truncated normal distribution

TensorFlow implements a specific case of a TP distribution: the values are drawing from NP in
the range [−2σ, +2σ] [62]. Technically, the values are drawn from NP with a specified µ and σ,
discarding and re-drawing samples with more than two standard deviations from the mean [62].

B.2

Using QRNG

We use the inverse transform sampling approach9 , followed by scaling and shifting. Let us look at
the details.
B.2.1

Implementation notes

Our QRNG-based distributions are tested against the baseline Keras v.3.3.3 and TensorFlow v.2.16.1
implementations distributions (described in Appendix B.1) to assure correctness of implementation.
Files src/distributions_qr.py and tests/test_distributions_qr.py contain the code and
associated test cases, respectively [43]. Essentially, we modify Keras [11] classes that are used to
sample random numbers.
Appendix A describes how we implement d-dimensional Sobol’ sequences in the range of (0, 1)d .
To maintain low discrepancy properties of Sobol’ sequences, we should sample from the beginning of
the sequence for a given dimension k. Informally, we can refer to k as the seed for our QRNG.
9
Inverse transformation sampling begins with uniform samples in the range between 0 and 1. These samples can
be viewed as probabilities, which can be fed into an inverse cumulative distribution function (of a distribution of
interest). Details of the method can be found in [13, Section 2.2].

21

B.2.2

Uniform distribution

Let us sample a vector of n random numbers u from a uniform distribution (based on Sobol’ sequences).
It can be achieved by scaling and shifting a Sobol’ sequence of length n for k-th dimension (which
gives us an inverse of the cumulative distribution function for the uniform distribution):
u ∼ UQ (a, b; k) = (b − a)sk + a,

(8)

with UQ (a, k) denoting a uniform distribution with the range (a, b) and sk denoting a Sobol’ sequence
of length n for dimension k.
We can now use UQ (0, 1, k) to get samples for normal and truncated normal distributions via the
inverse transform sampling approach, as discussed below.
B.2.3

Normal distribution

To sample n random numbers n from a normal distribution that uses Sobol’-sequences based QRNG,
we use:
n ∼ NQ (µ, σ; k) = Φ−1 [UQ (0, 1; k)] σ + µ = Φ−1 (sk )σ + µ,
(9)
where NQ (µ, σ; k) denotes a normal distribution with the mean µ and standard deviation σ and Φ−1
is the inverse cumulative distribution function of the standard normal distribution10 . We use an
implementation of Φ−1 provided by SciPy v.1.13.1.
As UQ (0, 1; k) returns values in the range (0, 1), theoretically, the output of Φ−1 should always
be finite. Potentially, when working with floating points numbers, limx→0 Φ−1 (x) = −∞ and
limx→1 Φ−1 (x) = ∞. We have not experienced this issue in practice since we skip the first element
of the Sobol’ sequence. However, this issue will manifest itself if the first element is not skipped
(which can be mitigated using scrambled Sobol’ sequences [47]).
B.2.4

Truncated normal distribution

To sample n random numbers t from a truncated normal distribution that uses Sobol’-sequences
based QRNG, so that we match the [−2σ, +2σ] range discussed in Appendix B.1.3 we do
t ∼ TQ (µ, σ; k) = Φ−1 {Φ(α) + UQ (0, 1; k) [Φ(β) − Φ(α)]}σ + µ
= Φ−1 {Φ(α) + sk [Φ(β) − Φ(α)]}σ + µ
|
{z
}

(10)

γ

≈ Φ−1 {0.02 + 0.95sk }σ + µ,
where Φ is the cumulative distribution function of the standard normal distribution, α = −2, and
β = 2. We use an implementation of Φ and Φ−1 provided by SciPy v.1.13.1. The values of α and β
are chosen to satisfy the range [−2σ, +2σ] requirement.
Equation 10 is slow (since Φ and Φ−1 are expensive to compute) but numerically stable. The
stability analysis is detailed below.
As above, limγ→0 Φ−1 (γ) = −∞. Thus, Equation 10 may be problematic when an element in sk ,
denoted s, yields γ ≈ 0:
γ = Φ(α) + s [Φ(β) − Φ(α)] = 0 ⇒
Φ(α)
s=
≈ −0.02.
Φ(α) − Φ(β)
Therefore, Φ−1 (γ) ̸= −∞ for all s, as s > 0. Similarly, Φ−1 (γ) ̸= ∞ because lims→1 γ ≈ 0.98.
10

A standard normal distribution has a mean of zero and a standard deviation of one.

22

B.3

Performance analysis

Suppose that we want to create d-dimensional sequence of random values with Nk values for each
dimension. Further, suppose that the values for the k-th dimension should be drawn independently
of the previous dimensions (which is typical for our use case, since we usually want to draw the
sequence for a particular value of k).
PRNG generators implement all three distributions (random uniform, random normal, and
truncated normal) independently of their seed values.
QRNGs based on Sobol’ sequences are different. Because Equation 7 is recursive, we must draw
Nk values for dimensions 1 through k and then discard the values for dimensions 1 to k − 1. In this
scenario, the total computation cost would be:
d
X

kNk ≥

k=1

d
X

kNmax =

k=1


d(d + 1)
Nmax = O d2 Nmax ,
2

(11)

where Nmax is the maximum number of elements for any of the d dimensions11 .
Figure 5 presents the amount of time needed to draw the Nmax values from the random uniform
U(·) (0, 1; k), random normal N(·) (0, 1; k), and truncated normal T(·) (0, 1; k) distributions. As expected,
in the case of PRNG the amount of time required to generate the random numbers is independent
of the seed (the lines stay flat). The amount of time required to generate values from the random
uniform distribution is the fastest; the time to generate draws from random normal and truncated
normal distributions is similar but slower than for the random uniform distribution, since they use a
version of the Box-Muller transform [6].
random-normal

random-uniform

truncated-normal

Execution Time (s)

1e-02

1e-03

1e-04

1

10

100

1000

10000

1

10

100

1000

10000

1

10

100

1000

10000

Seed k
Nmax

10

100

1000

10000

PRNG

QRNG

Figure 5: Time needed to draw the Nmax = 10, 100, 1000, 10000 values from the random normal
N(·) (0, 1; k), random uniform U(·) (0, 1; k), and truncated normal T(·) (0, 1; k) distributions. x-axis
shows seed value k, y-axis shows execution time measured in seconds. The lines represent median
accuracy based on 1000 repetitions, while the ribbons represent the range between lower and upper
quartiles.
QRNG requires more time to draw from random uniform, random normal, and truncated normal
distributions than their PRNG counterparts. Within the QRNG group, random uniform is the
fastest, followed by random normal, and truncated normal. This is expected based on the number of
11

Algorithm 659 [8] explicitly adds the cost of XOR bit manipulation O[log(d)]. The computational cost of this
calculation is negligible on modern computers, so we eliminate this term.

23

operations in Equations 8, 9, and 10. For all three distributions, execution time increases with k (as
expected based on Equation 11).
As mentioned above, our QRNG-based formulas are slower than Keras/TensorFlow’s PRNG-based
formulas. However, the time spent initializing the weights before training is immaterial compared to
the time spent training the model. As shown in Figure 5, drawing the values from the distributions
for a specific k takes only a fraction of a second (even for Nmax = 10000). Comparatively, training a
model takes seconds to minutes. Thus, we can still save time training the model (since fewer epochs
are required to achieve a certain level of accuracy).
Caching Sobol’ sequences can improve implementation performance if we need to initialize weights
quickly (e.g., because we use very large k values or because every second matters). The sequences
can be cached in a Nmax × d table if we know in advance Nmax and d. In this case, the computations
will cost only
d
X
Nmax = dNmax = O (dNmax ) .
k=1

Using this method, the order of computational complexity of QRNG would be reduced to that of
PRNG.

B.4

Using Mersenne Twister PRNG

We use the inverse transform sampling approach discussed in Appendix B.2 to compute random
uniform, random normal, and truncated normal distributions based on the Mersenne Twister PRNG.
There is only one difference: to replace sk with mk in Equations 8, 9, and 10, where mk denotes a
vector of draws from the Mersenne Twister PRNG with seed k.

C

Weight Initializers

C.1

Parameters and Implementations

Keras v.3.3.3 implements ten popular kernel initializers that use random number generators [34].
Table 4 list the initializers under study along with the default parameters (which we use in our
study).
Glorot, He and Lecun (Uniform and Normal) adapt the shape of distribution by selecting
parameters based on the number of input units, represented as Nin in the weight tensor, and/or the
number of output units, represented as Nout , in the weight tensor.
The creation of the Orthogonal initializer is more involved. The following is a quote from the
Keras manual [35].
“If the shape of the tensor to initialize is two-dimensional, it is initialized with an
orthogonal matrix obtained from the QR decomposition of a matrix of random numbers
drawn from a normal distribution. If the matrix has fewer rows than columns then the
output will have orthogonal rows. Otherwise, the output will have orthogonal columns.
If the shape of the tensor to initialize is more than two-dimensional, a matrix of
shape (shape[0] * ... * shape[n - 2], shape[n - 1]) is initialized, where n is
the length of the shape vector. The matrix is subsequently reshaped to give a tensor of
the desired shape.”
A multiplicative factor denoted by g is applied to the orthogonal matrix.

24

We use Keras/TensorFlow implementations of the algorithms as-is, but we modify the code
to use QRNG rather than PRNG to sample the values from the respective distributions. Files
src/custom_initializers.py and tests/test_custom_initializers.py contain the code and
associated test cases, respectively [43].
Table 4: Parameters of the initializers under study.

C.2

Initializer

Distribution

Glorot uniform

U(·) (a, b; k)

Parameters
q
q
6
6
,
b
=
a = − Nin +N
Nin +Nout
out

Glorot normal

T(·) (µ, σ; k)

µ = 0, σ =

He uniform

U(·) (a, b; k)

q
q
a = − N6in , b = N6in

He normal

T(·) (µ, σ; k)

µ = 0, σ =

Lecun uniform

U(·) (a, b; k)

q
q
a = − N3in , b = N3in

Lecun normal

T(·) (µ, σ; k)

µ = 0, σ =

Orthogonal

N(·) (µ, σ; k)

µ = 0, σ = 1, g = 1

Random normal

N(·) (µ, σ; k)

µ = 0, σ = 0.05

Random uniform

U(·) (a, b; k)

a = −0.05, b = 0.05

Truncated normal

T(·) (µ, σ; k)

µ = 0, σ = 0.05

q

q

q

2
Nin +Nout

2
Nin

1
Nin

Auto-selection of seed

In our experiments, the seed k is automatically selected using the function get_starting_dim_id_auto
in the file src/train_and_eval.py. The files src/global_config.py and tests/test_global_config.py
contain the code and associated test cases, respectively [43].
C.2.1

PRNG seed selection

For all models, PRNG-based initializers choose k arbitrary for each layer of the model and experiment
(which is a default Keras/TensorFlow behaviour). The seed selection for QRNG-based initializers is
described below.
C.2.2

QRNG seed selection

In QRNG-based initializers, k is chosen sequentially for every layer. As discussed in Appendix B.2.1,
the seed k is mapped to the k-th dimension of the Sobol’ sequences. We have observed that the

25

sequence’s starting value of k may significantly affect the speed with which the optimizer reaches
high accuracy values.
No universal pattern could be applied to all the models under study. However, we have seen that
models that reach high levels of accuracy in early epochs usually retain this competitive advantage
into the future. In addition, the number of seeds that lead to sub-par results is small.
As a result, we introduce a heuristic for selecting a seed for model training, as shown in
Algorithm 1. In essence, we randomly select X seeds from the range [W, Z] and train the model for
Y epochs, repeating12 the training R times. In order to match the range of dimensions used in the
implementation of Sobol’ sequences under study, the minimum value for W, X, and Z is 1 and the
maximum value is 21 200 (see Appendix A for details).
This seed selection process can become expensive, since we must train the models for an additional
∆Q = XY R − Y = Y (XR − 1) epoch. The term −Y refers to the fact that we can continue to
train the best model, saving Y epochs. Due to possible defects (associated with managing global
states) in the underlying libraries, we do not do this in our code to minimize the risk of giving the
QRNG-based model an unfair advantage. It is certainly reasonable to do this in a practical setting.
Our paper uses accuracy to measure performance, so we seek seeds that maximize accuracy. The
following values yield adequate results for our use cases: R = 1, X = 5, Y = 1, W = 1, and Z = 10.
Based on these parameters, QRNG-based initializers have a penalty of ∆Q = Y (XR − 1) = 4 epochs.
Readers may adjust these values according to their use cases through empirical evaluation.
Here are some examples of values of k that we use in our models.
MLP and CNN MLP and CNN models select a new value of k for every layer. In QRNG-based
initializers, k is chosen sequentially for every layer starting at ν (the starting index is provided by
the Algorithm 1). As discussed in Appendix B.2.1, the seed k is mapped to k-th dimension of the
Sobol’ sequences.
For example, our MLP under study (described in Appendix D.1) has three dense layers that
require weight initalization. For the first layer, we will seed the initializer with k = ν, the second
layer — with k = ν + 1, and the third layer — with k = ν + 2. The weights are assigned to specific
units of the layer without reshaping.
Our CNN under study (described in Appendix D.2) has three convolutional and one dense layer.
Three convolutional layers will be initialized with k = ν, ν + 1, ν + 2, respectively; dense layer will
be initialized with k = ν + 3. The tensor of the obtained weights will be reshaped to match the
dimensionality of a layer (in terms of channels and kernels). Keras/TensorFlow standard code
handles this reshaping for us.
LSTM and Transfomers LSTM and Transformer models consist of multiple submodules, each
submodule is initialized with an incrementing value of k. In LSTM models, the four gates (input,
forget, cell, and output) are treated as a single layer, and we experiment with different initializers
while seeding QRNG-based methods with k = ν; the same initializer is used for the dense layer (used
for classification) with k = ν + 1. As described in Appendix D.3, the recurrent states in the LSTM
are initialized using the QRNG-based orthogonal initializer with k = ν.
In Transformer models, the multi-head attention layer consists of four fully connected layers
representing queries, keys, and values (QKV), followed by an output layer. The same initializer type
is used for all layers. The QKV multi-head attention and associated output weights are initialized
with k = ν, ν +1, ν +2, ν +3, respectively. Two additional fully connected layers follow the multi-head
12

Because optimization is stochastic, different instances of the optimizer may converge to different solutions when
starting from the same weights.

26

Algorithm 1: A heuristic to automatically select a starting seed value for a given model.
: W ∈ {1, 2, . . . , 21200} ;
/* The minimum value of the seed to try */
: Z ∈ {1, 2, . . . , 21200} ;
/* The maximum value of the seed to try */
: X ∈ {1, 2, . . . , 21200} ;
/* The number of seeds to try */
: Y ∈ Z≥1 ;
/* The number of epochs to train the model */
: R ∈ Z≥1 ;
/* The number of times to train a model with a particular
seed */
Input : model_cfg ;
/* Model configuration */
Input : train_data ;
/* Dataset to train the model */
Input : test_data ;
/* Dataset to test the model */
Output : Suggested seed / dimension id denoted by ν

Input
Input
Input
Input
Input

1 ν ←∅ ;
2 best_metric_value ← ∅ ;
3 seeds ← Sample without replacement X integers in the range [W, Z] ;
4 seeds ← Sort seeds ;

/* Increase reproducibility when metric values are tied */
5 foreach seed in seeds do
6
for 1 to R do
7
Set starting seed value to seed ;
8
model ← Initialize the model using model_cfg ; /* The seed value is assigned
sequentially beginning at the starting seed value, i.e., seed */
9
trained_model ← Train the model for Y epochs on train_data ;
10
current_metric_value ← Evaluate trained_model on test_data and compute model’s
performance on test_data ;
/* Assume that we want to maximize the metric
*/
11
if best_metric_value < current_metric_value or best_metric_value = ∅ then
12
best_metric_value ← current_metric_value ;
13
ν ← seed ;
/* New best seed */
14 return ν ;

attention layer and are initialized with k = ν + 4, ν + 5, respectively. The dense layer used for
classification is initialized with k = ν + 6.

D

Models

The architecture of the models is shown below. As discussed in Section 3.1, we keep the models
simple to isolate the effect of random number generators on model performance.

D.1

MLP

D.1.1

MLP for the performance experiments in Appendix H

Figure 6 shows an MLP architecture designed for image processing (input images are flattened). It
has one fully connected layer. For a given experiment, the number of neurons in the layer varies
from 1 to 70. ReLU (Rectified Linear Unit) activation functions [16] are used after the layer. We
experiment with different weight initialization techniques for kernels, and biases are initialized to

27

zero. Details of the experiments are given in Appendix H. The source code for the model can be
found in src/model/baseline_ann_one_layer.py [43].
The final output layer consists of a dense layer with a softmax activation function and ten neurons
(one per class). The kernel initializer is Glorot Uniform (the default for Keras v .3.3.3), and the
biases are zero.
Layer Type

Input Layer

Dense

Dense

MNIST

MNIST:

MNIST:

MNIST:

Output Shape

(None, 784)

(None, u)

(None, 10)

Figure 6: MLP architecture (used in the experiments in Appendix H). The number of units (neurons)
u varies from 1 to 70. The leftmost node represents a legend.

D.1.2

MLP for core experiments

Figure 7 shows an MLP architecture designed for image processing (input images are flattened).
It has two fully connected layers consisting of 32 neurons each. ReLU (Rectified Linear Unit)
activation functions [16] are employed after each layer. For kernels, we experiment with different
weight initialization techniques (listed in Section 2), and biases are initialized to zero. The source
code for the model can be found in src/model/baseline_ann.py [43].
The final output layer consists of a dense layer with a softmax activation function and ten neurons
(one per class). In each experiment, the kernel initializer is the same as the fully connected layers
initializer, and biases are initialized to zero.
Layer Type
Dataset
Output Shape

InputLayer
MNIST:

CIFAR:

(None, 784) (None, 3072)

Dense
MNIST:

CIFAR:

(None, 32) (None, 32)

Dense
MNIST:

CIFAR:

(None, 32) (None, 32)

Dense
MNIST:

CIFAR:

(None, 10) (None, 10)

Figure 7: MLP architecture used in the core experiments. The leftmost node represents a legend.

D.2

CNN

Figure 8 pictures a CNN architecture designed for image processing. It consists of three convolutional
layers, each with 32, 64, and 64 filters. The feature maps are downsampled using Max pooling layers,
and ReLU activations are applied after each convolution layer. Similarly to MLP architectures,
kernel initializers (shwon in Section 2) differ based on experiment, while biases are initialized to zero.
The source code for the model can be found in src/model/baseline_cnn.py [43].
The final output layer consists of a dense layer with a softmax activation function and ten
neurons (one per class). In each experiment, the kernel initializer is the same as the convolutional
layer initializer, and biases are initialized to zero.

D.3

LSTM

Figure 9 depicts an LSTM architecture (a type of recurrent neural network) designed for classification.
LSTM cells maintain four gates that constrain recurrent states. A gate consists of 8 neurons whose
weights are initialized using an initializer (listed in Section 2) specific to each experiment. Biases are
28

Layer Type
MNIST

CIFAR

Output Shape Output Shape

Input Layer
MNIST:

Convolution 2D

CIFAR:

MNIST:

(None, 28, 28, 1) (None, 32, 32, 3)

Max Pooling 2D

CIFAR:

MNIST:

(None, 26, 26, 32) (None, 30, 30, 32)

Dense
MNIST:

(None, 10) (None, 10)

MNIST:

MNIST:

(None, 13, 13, 32) (None, 15, 15, 32)

Flatten

CIFAR:

Convolution 2D

CIFAR:

Convolution 2D

CIFAR:

MNIST:

(None, 576) (None, 1024)

CIFAR:

(None, 11, 11, 64) (None, 13, 13, 64)

Max Pooling 2D

CIFAR:

(None, 3, 3, 64) (None, 4, 4, 64)

MNIST:

CIFAR:

(None, 5, 5, 64) (None, 6, 6, 64)

Figure 8: CNN architecture used in the core experiments. Top-left node represents a legend.
initialized to zero. A recurrent weights matrix is populated using an orthogonal initializer which is
initialized with the help of QRNG for QRNG experiments and with PRNG for PRNG experiments.
Sigmoid activation function is use for the recurrent step.
The input data are passed through a 32-dimensional embedding layer. Default Keras v .3.3.3
initialization scheme (PRNG-based) is used to initialize the layer.
The final output layer consists of a dense layer with a softmax activation function and two
neurons (one per class). In each experiment, the kernel initializer is the same as the LSTM gates
initializer, and biases are initialized to zero.
The source code for the model can be found in src/model/baseline_lstm.py [43].
Layer Type

InputLayer

Embedding

LSTM

Flatten

Dense

Dataset

IMDB:

IMDB:

IMDB:

IMDB:

IMDB:

Output Shape

(None, 512)

(None, 512, 32)

(None, 8)

(None, 8)

(None, 2)

Figure 9: LSTM architecture used in the core experiments. The leftmost node represents a legend.

D.4

Transformer

Figure 10 displays an encoder-only Transformer architecture designed for classification. To implement
it, we use KerasNLP v.0.12.1 package [69].
It is a simplified version of the architecture given in the Keras manual [45]. It has two attention
heads that take positional embedding vectors of size 32 for each token (the default Keras v .3.3.3
PRNG-based scheme is used for initialization). Two fully connected layers are used, each consisting
of 32 neurons and a dropout rate of 0.1. We experiment with different weight initialization techniques
(depicted in Section 2) for attention and fully connected layers while uniformly initializing positional
embedding weights. Biases are initialized to zero. The source code for the model can be found in
src/model/baseline_transformer.py [43].
The final output layer consists of a dense layer with a softmax activation function and two
neurons (one per class). In each experiment, the kernel initializer is the same as the attention heads’
initializer, and biases are initialized to zero.
The source code for the model can be found in src/model/baseline_transformer.py [43].

E

Optimizers

Both Adam and SGD optimizers use the same learning rate of 0.0001. The only exception was
SGD-trained LSTM, where we saw no improvement in 30 epochs. For that case, we increased the
learning rate to 0.01.
29

Layer Type

Input Layer

Token And Position Embedding

Transformer Block

Flatten

Dense

Dataset

IMDB:

IMDB:

IMDB:

IMDB:

IMDB:

Output Shape

(None, 512)

(None, 512, 32)

(None, 512, 32)

(None, 16384)

(None, 2)

Figure 10: Transformer architecture used in the core experiments. The leftmost node represents a
legend.
In all experiments, the remaining hyperparameters of the optimizers are left at their default
values, as set by Keras v.3.3.3. In particular, SGD momentum is set to 0; Adam hyperparameters
are β1 = 0.9, β2 = 0.999, and ϵ̂ = 10−7 .

F

Datasets

The source code for the data pipeline can be found in src/datapipeline.py [43]. For all the
datasets, labels are converted using one-hot encoding. To eliminate the risk of randomness from
the data split process, we use complete training and test datasets provided by Keras unchanged
(without splitting the train dataset into train and validation). The following are the details for each
of the three datasets.

F.1

MNIST

The MNIST handwritten digit dataset [36] is widely used in computer vision. It comprises a collection
of 28 × 28 grayscale images of handwritten digits (0–9); the pixel values are in the range of 0 to 255.
In total, there are 60 000 training images and 10 000 testing images. The images are labelled with
their corresponding digits, making them suitable for classification tasks.
The MNIST dataset is divided into two subsets: 60 000 observations for training and 10 000 for
testing. This distribution follows a ratio of 6 : 1, respectively.

F.2

CIFAR-10

Another popular dataset for image classification tasks is the CIFAR-10 [32]. It consists of 60 000
32 × 32 colour images (for each of the three channels, the pixel values range from 0 to 255) spread
across ten different classes, with 6000 images per class. The dataset includes a variety of objects
and animals, making it a more challenging dataset than MNIST. The detailed class list includes
airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.
The CIFAR dataset is split into 50 000 observations for training and 10 000 for testing, maintaining
a ratio of 5 : 1, respectively.

F.3

IMDB

The Large Movie Review Dataset [38] also known as the IMDB dataset is a binary sentiment analysis
dataset using IMDB reviews. Sequence modelling architectures, such as Recurrent Neural Networks
and Transformers, are commonly benchmarked on this dataset. There are 50 000 English movie
reviews tagged with positive or negative sentiments.
The reviews are tokenized by word (as per [38]) using a vocabulary size of 20 000 and a maximum
review length of 512 tokens for all experiments. IMDB reviews with less than 512 tokens are
zero-padded.

30

For the IMDB dataset, the division includes 25 000 observations for training and 25 000 for
testing, resulting in a ratio of 1 : 1, respectively.

G

Examples of computing measures of performance

Special case: level of accuracy not reached When a model in a pair of models fails to reach a
predetermined accuracy level A within 30 epochs, we assign an epoch value of 31 to E(·) (A) of this
model in Equation 1. This assignment offers a conservative approximation of E(A). For example,
if EQ (A) = 20 and EP (A) > 30, using EP (A) = 31 yields E(A) = (20 − 31)/31 ≈ −35%, which
should be interpreted as E(A) ≤ −35%. In this scenario, D(A) remains indeterminate.
Quantitative example Consider an example given in Figure 11. Using box-and-whisker plots,
we show the distribution of test accuracy for the 100 experiments. x-axis represents epoch number,
y-axis represents model accuracy on test dataset.
Note that E(A) and D(A) for A = 0.05, 0.1, 0.15, and 0.2 are identical since we reach A ≈ 0.2
during the first epoch. Since neither model reaches A ≥ 0.55, E and D for A ≥ 0.55 are not
computed.
Let us examine three values of A = 0.2, 0.4, 0.5.
Both models reach A = 0.2 at the first epoch. Based on Equation 1,
E(0.2) =

1+4−1
EQ (0.2) + ∆Q − EP (0.2)
× 100 =
× 100 = 500%
EP (0.2)
1

and, based on Equation 2,
D(0.2) = DQ (0.2) − DP (0.2) ≈ 0.021 − 0.040 = −0.019.
When A = 0.4, the QRNG version of the initializer reaches this median accuracy at epoch 7,
while PRNG version at epoch 14. Thus,
E(0.4) =

EQ (0.4) + ∆Q − EP (0.4)
7 + 4 − 13
× 100 =
× 100 ≈ −15%
EP (0.4)
13

and
D(0.4) = DQ (0.4) − DP (0.4) ≈ 0.019 − 0.029 = −0.010.
Finally, the QRNG version of the initializer reaches A = 0.5 at epoch 23, while PRNG version
does not reach this accuracy in 30 epochs. In this case, we set EP (0.5) = 31 and the computation
becomes:
EQ (0.5) + ∆Q − EP (0.5)
26 + 4 − 31
E(0.5) ≤
× 100 =
× 100 ≈ −3%
EP (0.5)
31
and
D(0.5) = DQ (0.5) − DP (0.5) ≈ 0.023 − indeterminate = indeterminate.
Figure 11 shows us that the maximum values of A achieved by the model with the PRNG
version of the initializer is ≈ 0.47 and with the QRNG version of the initializer is ≈ 0.51. Based on
Equation 4,

max
Am = min Amax
≈ min (0.47, 0.51) = 0.47.
P , AQ

31

This median accuracy is achieved by the PRNG version in epoch 30 and by the QRNG version in
epoch 18. Thus,
E(AM ) = E(0.47) =

EQ (0.47) + ∆Q − EP (0.47)
18 + 4 − 30
× 100 =
× 100 ≈ −26%
EP (0.47)
30

and
D(AM ) = D(0.47) = DQ (0.47) − DP (0.47) ≈ 0.023 − 0.037 = −0.014.
Based on Equation 5,
SE [E(Am )] = SE [E(Am )] = SE [E(0.47)] = SE [−26%] = win,
The p-value of the Fligner-Killeen test is ≈ 3.5 × 10−4 . Based on Equation 6,
SD [D(Am )] = SD [D(Am )] = SD [D(0.47)] = SD [−0.014] = win.
Thus, we can conclude that for the setup under study (CIFAR-10 test dataset, SGD optimizer,
Glorot Uniform initializer, and CNN model), the model with QRNG-based initializer reaches the best
accuracy than the model with PRNG-based initializer faster by ≈ 26%. Moreover, the QRNG-based
model has less variation (in terms of IQRs) by 0.014.
glorot-uniform

0.5

Accuracy

0.4

0.3

0.2

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

Epoch
intializer

PRNG

QRNG

Figure 11: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer, Glorot Uniform
optimizer, and CNN model. Horizontal dotted lines indicate accuracy thresholds of A = 0.2, 0.3, and
0.4. Horizontal dashed line indicated Am ≈ 0.39.

32

H

The impact of QRNG and PRNG on a perceptron performance

We have demonstrated the effectiveness of QRNG by answering RQs 1–4. However, the underlying
reasons for this effectiveness deserve further investigation. Let us examine a two-layer MLP model
(shown in Figure 6) to get an idea of how the random number initializer affects the model performance.
In order to isolate the impact of a random number generator, the output layer initializer will remain
the same (namely, the default choice for Keras and TensorFlow: PRNG-based Glorot Uniform
initializer).
Five random number generators are examined for the hidden layer: two PRNGs (Philox and
Mersenne Twister13 , discussed in Section 3.1) and three QRNGs using Sobol’ sequences. The number
of units in this layer ranges from 1 to 70. The weights are initialized using one of the ten initializers
under study (listed in Section 2).
The seeds for the Philox and Mersenne Twister PRNGs are chosen at random for each experiment
(see Appendix C.2.1). In the three QRNG experiments, the starting seed is set at a constant value
of 1 or 5, or it is automatically selected using the algorithm described in the Appendix C.2.2.
Models are trained on the MNIST dataset for a single epoch using Adam optimizer. We repeat
each experiment 100 times to measure the variance in test accuracy. We use the median as a measure
of central tendency and the IQR to assess dispersion, as these are less sensitive to outliers and
skewness than the mean and standard deviation.

H.1

PRNGs

Figure 12 shows the results. For PRNG-based models, increasing the number of hidden layer units
increases the test accuracy mainly monotonically. Both PRNG initialization methods produce
similar median and IQR accuracy values. The effects of two PRNGs on the optimizer appear similar.
Consequently, the difference between the PRNG and QRNG results in RQs 1–4 cannot
be attributed to Philox algorithm implementation problems.

H.2

QRNGs

Figure 12 suggests that models initialized with QRNGs show a different dynamics of the accuracy
values that change with u than models initialized with PRNGs. Specifically, QRNG-based models
exhibits a sawtooth pattern, that (based on eyeballing of the figure) can be grouped depending on
the odd- and even-numbered count of units u, where the even-numbered count of units is further
partitioned into two sequences as follows:


if (u − 1) mod 2 = 0
odd,
u-sequence = even: 2, 6, 10, . . . , if (u − 2) mod 4 = 0 .
(12)


even: 4, 8, 12, . . . , if u mod 4 = 0
More research is needed to better understand why these three sequences of unit counts perform
differently. Our empirical findings are as follows.
The three sequences yield similar accuracy at high u values (probably because the effect of the
QRNG is overpowered by the flexibility provided by the large number of weights at that stage).
Among the smaller values of u, the odd values yield the lowest levels of accuracy, while u = 2, 6, 10, . . .
gives a higher level of accuracy, and u = 4, 8, 12, . . . — the highest level of accuracy. For example,
13

See Appendix B.4 for details on how Mersenne Twister is used to draw values from different distributions.

33

Table 5: Median accuracy plus-minus IQR values for Random Uniform initializer and u =
31, 32, 33, 34.
Random number generator
PRNG (Mersenne Twister)
PRNG (Philox)
QRNG (Sobol’ — starting seed = 1)
QRNG (Sobol’ — starting seed = 5)
QRNG (Sobol’ — starting seed = auto)

u = 31

u = 32

u = 33

u = 34

0.54 ± 0.11
0.53 ± 0.08
0.40 ± 0.12
0.54 ± 0.10
0.69 ± 0.18

0.55 ± 0.10
0.55 ± 0.09
0.87 ± 0.03
0.67 ± 0.18
0.84 ± 0.05

0.55 ± 0.09
0.56 ± 0.10
0.47 ± 0.13
0.57 ± 0.10
0.69 ± 0.17

0.58 ± 0.11
0.57 ± 0.09
0.70 ± 0.07
0.63 ± 0.07
0.76 ± 0.19

Table 5 illustrates that for u = 33, QRNG with automatic seed selection scheme for Random Uniform
initializer generates a median accuracy of 0.69, for u = 34 — 0.76, and for u = 32 — 0.84.
Figures 13 and 14 show the median and IQR values of accuracy obtained by the models on a
test dataset. We plot the data for the three u-sequences separately for better readability. For all
sequences of u, the median accuracy increases (although non-monotonically), but the pattern of
behavior for u = 4, 8, 12, . . . is the least volatile (but IQR values may sometimes be higher).
H.2.1

PRNG vs. QRNG

The difference in performance between models using different random number generators is sometimes
striking. For instance, Table 5 shows that the median accuracy using the Random Uniform initializer
with 32 units is 0.55 (IQR of 0.09) for PRNG Philox and 0.84 (IQR of 0.05) for QRNG with automatic
seed selection. In this case, QRNG offers 1.5 times higher median accuracy and significantly less
variability (by 0.04).
H.2.2

QRNG seed selection methods

When comparing three QRNG initialization methods, the automatic starting seed selection method
results in the least variability in precision with u (compare the accuracy values in Table 5).
However, for u = 4, 8, 12, . . ., QRNG-based models with a starting seed value of 1 slightly
outperform the automatic method. For instance, Table 5 shows that a starting seed of 1 yields an
accuracy of 0.87 ± 0.03, compared to 0.84 ± 0.05 for the automatic method. Conversely, a starting
seed of 5 yields an accuracy of only 0.68 ± 0.18.
When u is odd or u = 2, 6, 10, . . ., automatic seed selection outperforms the constant seeds 1 and
5. For instance, for u = 33, QRNG-based models using seed 1 and 5 achieve accuracies of 0.47 ± 0.13
and 0.57 ± 0.10, respectively. These values are comparable to or lower than those produced of
PRNG (Philox), which yields an accuracy of 0.56 ± 0.10. In contrast, the QRNG-based model with
automatic seed selection achieves an accuracy of 0.69 ± 0.17, which is 1.2 times better than that of
PRNG (Philox).
This suggests that seed selection is crucial and that our heuristic for seed selection
may need further refinement.
H.2.3

QRNG sequence length

Finally, if the sequence length is not a power of two, Sobol’ sequences lose their balance properties,
which is detrimental to Monte Carlo-based methods [48]. However, Figures 12 and 13 show no
dramatic deviations in accuracy near u = 1, 2, 4, 8, . . . , 64 (compared to the closest value u in the
corresponding sequence). Although some effect may be present (e.g., at u = 32), from a practical
34

perspective, this implies that we can use the sequences for weight initialization even if the
number of samples is not a power of two.

35

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.75

0.50

0.25

0.75

0.50

0.25

Accuracy

0.75

0.50

0.25

0.75

0.50

0.25

0.75

0.50

0.25

0

20

40

60

0

20

40

60

Units Count
PRNG (Mersenne Twister)

PRNG (Philox)

QRNG (Sobol' - starting_seed = 1)

QRNG (Sobol' - starting_seed = 5)

QRNG (Sobol' - starting_seed = auto)

Figure 12: The median test accuracy for the perceptron-based model (given in Figure 6). The x-axis
shows the number of units u in the model. The y-axis represents the model’s accuracy on the MNIST
test dataset. The lines represent median accuracy based on 100 repetitions, while the ribbons indicate
the range between the lower and upper quartiles. Vertical dotted lines show u = 2, 4, 8, 16, 32, 64.
36

even: 2, 6, 10, ...

even: 4, 8, 12, ...

odd

QRNG (Sobol' - starting_seed = 1)

0.75

0.50

0.25

initializer

Median Accuracy

QRNG (Sobol' - starting_seed = 5)

0.75

0.50

0.25

glorot-normal
glorot-uniform
he-normal
he-uniform
lecun-normal
lecun-uniform
orthogonal
random-normal
random-uniform
truncated-normal

QRNG (Sobol' - starting_seed = auto)

0.75

0.50

0.25

0

20

40

60

0

20

40

60

0

20

40

60

Units Count

Figure 13: The median test accuracy for the perceptron-based model (given in Figure 6) using
three different QRNG-based initialization schemes for three sequences of u (shown in Equation 12).
Vertical dotted lines show u = 2, 4, 8, 16, 32, 64.

37

even: 2, 6, 10, ...

even: 4, 8, 12, ...

odd

0.5

QRNG (Sobol' - starting_seed = 1)

0.4

0.3

0.2

0.1

0.0
0.5

initializer
QRNG (Sobol' - starting_seed = 5)

IQR of Accuracy

0.4

0.3

0.2

0.1

glorot-normal
glorot-uniform
he-normal
he-uniform
lecun-normal
lecun-uniform
orthogonal
random-normal
random-uniform
truncated-normal

0.0
0.5
QRNG (Sobol' - starting_seed = auto)

0.4

0.3

0.2

0.1

0.0
0

20

40

60

0

20

40

60

0

20

40

60

Units Count

Figure 14: The median test accuracy for the perceptron-based model (given in Figure 6) using
three different QRNG-based initialization schemes for three sequences of u (shown in Equation 12).
Vertical dotted lines show u = 2, 4, 8, 16, 32, 64.

I

Summary statistics

The values of Amax
and Amax
required to compute SA for all cases are shown graphically in Figure 15.
P
Q
The values of EP (Am ) and EQ (Am ), necessary for calculating E(Am ), are presented in Figure 16.
The values of DP (Am ) and DQ (Am ), used to determine D(Am ), are illustrated in Figure 17. The
max
average values of Amax
Q − AP , denoted by ᾱ, grouped by optimizer, dataset, and model, are shown
in Table 6; grouped by initializer — in Table 7; grouped by model and optimizer — in Table 8; and
grouped by dataset and optimizer — in Table 9 . Finally, the average E(Am ) values, denoted by
Ē(Am ), grouped by model and optimizer are given in Table 10.

38

CIFAR-10, CNN

CIFAR-10, MLP

IMDB, LSTM

1.00

0.75

initializer
glorot-normal

Max Accuracy (QRNG): Amax
Q

0.50

glorot-uniform
he-normal
he-uniform

0.25

lecun-normal
lecun-uniform
IMDB, Transformer

MNIST, CNN

orthogonal

MNIST, MLP

random-normal

1.00

random-uniform
truncated-normal
0.75

optimizer
Adam

0.50

SGD
0.25

0.25

0.50

0.75

1.00

0.25

0.50

0.75

1.00

0.25

0.50

0.75

1.00

Max Accuracy (PRNG): Amax
P

Figure 15: Amax
and Amax
values needed to compute SA for all 120 cases. Black line is governed by
P
Q
max
max
equation AQ = AP .

CIFAR-10, CNN

CIFAR-10, MLP

IMDB, LSTM

30

initializer

20

glorot-normal

Epoch Count (QRNG): EQ(Am)

glorot-uniform
he-normal

10

he-uniform
lecun-normal
lecun-uniform

0
IMDB, Transformer

MNIST, CNN

orthogonal

MNIST, MLP

random-normal

30

random-uniform
truncated-normal
20

optimizer
Adam
SGD

10

0
0

10

20

30

0

10

20

30

0

10

20

30

Epoch Count (PRNG): EP(Am)

Figure 16: EP (Am ) and EQ (Am ) values needed to compute E(Am ) for all 120 cases. Black line is
governed by equation EQ (A) = EP (A). To improve plot readability, EP (Am ) and EQ (Am ) values
are jittered. Horizontal and vertical dotted lines represent 30 epochs.

39

CIFAR-10, CNN

10

-1

10

-2

10

-3

CIFAR-10, MLP

IMDB, LSTM

initializer
glorot-normal

IQR (QRNG): DQ(Am)

glorot-uniform
he-normal
he-uniform
lecun-normal
10

-4

lecun-uniform
IMDB, Transformer

MNIST, CNN

orthogonal

MNIST, MLP

random-normal
10

random-uniform

-1

truncated-normal

10

-2

10

-3

10

-4

optimizer
Adam
SGD

10

-4

10

-3

10

-2

10

-1

10

-4

10

-3

10

-2

10

-1

10

-4

10

-3

10

-2

10

-1

IQR (PRNG): DP(Am)

Figure 17: DP (Am ) and DQ (Am ) values needed to compute D(Am ) for all 120 cases. Black
line is governed by equation D(A) = DQ (A) − DP (A) = 0; blue line is governed by D(A) =
DQ (A) − DP (A) = 0.01; green line is governed by D(A) = DQ (A) − DP (A) = −0.01.

40

41

0.00

0.00
0.00

0.00

l:l,*,*
l:t,l,*

Loss Total

0.02

0.00

0.01
0.00

0.00
0.00

0.00

0.00

0.00

0.00
0.00

0.00

0.01

0.01

-0.01

-0.01
0.00

MLP

MNIST
CNN

0.00

0.01

0.01

0.00

0.00

-0.01
0.00

Total

MNIST

0.02

0.05

0.12
0.03

0.00
0.09

0.00

0.00

0.00

0.00
0.00

Total

Adam

0.15

0.17

0.14
0.12

0.34

-0.04

-0.04

0.04

0.05

0.06
0.07

0.02

0.00

0.00

MLP

CIFAR-10
CNN

0.09

0.11

0.08
0.10

0.15

-0.01

-0.04
0.00

Total

CIFAR-10

0.00

0.00

0.00

0.00

0.00

LSTM

0.00

0.01

0.02

0.01

0.00

-0.01
0.00

Transf.

IMDB

SGD

0.00
0.00

-0.01
0.00

-0.01

l:l,*,*
l:t,l,*

Loss Total

0.14
0.03

0.02
0.04
0.04
0.02

Win Total

Grand Total

0.02

0.03

0.00
0.00

0.00
0.07

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt
0.06

0.12

0.11
0.10

0.17

0.07

0.12

0.09

0.15

0.00

Tie Total

0.00

0.00
0.00

He
uniform

0.00

0.00

-0.01
0.00

He
normal

t:t,t,t

0.00

Glorot
uniform

Shape dependent

0.05

0.06

0.08
0.07

0.00
0.08

0.00

0.00

Lecun
normal

0.05

0.06

0.08
0.07

0.00
0.09

0.00

0.00

Lecun
uniform

0.04

0.07

0.08
0.06

0.00
0.10

0.00

0.00

0.00

-0.01
0.00

dependent
Total

Shape

0.01

0.03

0.03

0.00

0.00

0.00

Random
normal

0.00

0.01

0.03
0.01

0.00

-0.01

-0.01
0.00

Random
uniform

0.01

0.02

0.02

0.00

0.00

0.00
0.00

Truncated
normal

Shape agnostic

0.01

0.01

0.01

0.00

0.00

0.00

0.01

0.02

0.03
0.02

0.00

0.00

-0.01
0.00

agnostic
Total

0.04

0.06

0.00
0.08
0.00
0.14
0.03

0.00

0.00

Total

0.04

0.06

0.00
0.10
0.00
0.07
0.06

0.00

-0.01
0.00

Total

SGD

0.03

0.06

0.00
0.09
0.00
0.08
0.05

0.00

0.00

0.00

-0.01
0.00

Total

Grand

0.04

0.04

0.05

0.00

0.00

0.00
0.00

Total

MNIST

Orthogonal

0.07

0.07

0.07

0.00

0.00

MLP

MNIST
CNN

Shape

0.00

0.00

0.02

0.00
0.01

0.00

-0.01
0.00

Total

IMDB

max
Table 7: Average Amax
Q − AP , denoted by ᾱ, grouped by initializer.

0.00

Glorot
normal

0.06

Outcome

0.10

0.10

0.01

Grand Total

0.11

0.01

0.06

0.00

0.06

0.12
0.05

0.12
0.02

Win Total

0.15

0.00
0.00

0.15

0.00

0.00

0.00
0.00

0.00

0.00

0.00

0.00
0.00

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

0.00

0.00
0.00

Total

IMDB

Tie Total

0.00

0.00
0.00

Transf.

IMDB
LSTM

0.00

0.00

0.00
0.00

Total

CIFAR-10

t:t,t,t

0.00

MLP

CIFAR-10

CNN

Outcome

Adam

max
Table 6: Average Amax
Q − AP , denoted by ᾱ, grouped by optimizer, dataset, and model.

0.03

0.06

0.00
0.09
0.00
0.08
0.05

0.00

0.00

0.00

-0.01
0.00

Total

Grand

max
Table 8: Average Amax
Q − AP , denoted by ᾱ, grouped by model and optimizer.

Outcome

MLP

Transformer

Transformer

Grand

Adam

CNN
SGD

Total

CNN

Adam

LSTM
SGD

LSTM
Total

Adam

MLP
SGD

Total

Adam

SGD

Total

Total

l:l,*,*
l:t,l,*

0.00
0.00

-0.02

-0.01
0.00

0.00
0.00

0.00

0.00
0.00

-0.01
0.00

0.00

-0.01
0.00

0.00
0.00

-0.01
0.00

-0.01
0.00

-0.01
0.00

Loss Total

0.00

-0.02

0.00

0.00

0.00

0.00

-0.01

0.00

-0.01

0.00

0.00

0.00

0.00

t:t,t,t

0.00

0.00

0.00

Tie Total

0.00

0.00

0.00

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

0.00
0.00

0.00

0.06

0.34
0.00
0.14
0.06

0.17
0.00
0.14
0.06

Win Total

0.04

0.09

0.08

0.00

0.00

Grand Total

0.01

0.08

0.04

0.00

0.00

0.00
0.15

0.02

0.10

0.00
0.00

0.02

0.02
0.01

0.00
0.09
0.00
0.08
0.05

0.12
0.01

0.06
0.07

0.09
0.05

0.01

0.00

0.08

0.06

0.07

0.01

0.01

0.01

0.06

0.00

0.05

0.05

0.05

0.00

0.00

0.00

0.03

0.01

0.00
0.01

max
Table 9: Average Amax
Q − AP , denoted by ᾱ, grouped by dataset and optimizer.

Outcome

CIFAR-10

CIFAR-10

IMDB

IMDB

MNIST

MNIST

Grand

Adam

SGD

Total

Adam

SGD

Total

Adam

SGD

Total

Total

l:l,*,*
l:t,l,*

0.00
0.00

-0.04
0.00

-0.01
0.00

0.00
0.00

-0.01
0.00

0.00
0.00

-0.01
0.00

0.00
0.00

-0.01
0.00

-0.01
0.00

Loss Total

0.00

-0.01

-0.01

0.00

0.00

0.00

0.00

0.00

0.00

0.00

t:t,t,t

0.00

0.00

0.00

Tie Total

0.00

0.00

0.00

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

0.15

0.15

0.15

0.00
0.00

0.12
0.05

0.08
0.10

0.10
0.08

0.01

Win Total

0.10

0.11

0.11

0.00

Grand Total

0.06

0.09

0.08

0.00

0.00
0.01

0.02
0.01

0.01

0.05

0.04

0.00
0.09
0.00
0.08
0.05

0.00

0.00

0.01

0.04

0.03

0.06

0.00

0.00

0.00

0.04

0.02

0.03

0.02

42

0.00
0.01

0.00
0.00

0.00
0.00

Table 10: Average E(Am ) values, denoted by Ē(Am ), grouped by model and optimizer.
Outcome

CNN

CNN

LSTM

LSTM

MLP

MLP

Transformer

Transformer

Grand

Adam

SGD

Total

Adam

SGD

Total

Adam

SGD

Total

Adam

SGD

Total

Total

l:l,*,*
l:t,l,*

41
13

72

47
13

47
31

70

47
49

100
31

806

100
612

233
29

104
15

169
18

83
125

Loss Total

31

72

37

33

70

48

90

806

305

165

44

85

106

t:t,t,t

0

0

0

Tie Total

0

0

0

w:t,w,wt
w:w,l,*
w:w,t,wt
w:w,w,l
w:w,w,wt

-8
9

-47

-47

400
0
-63
-43

204
0
-63
-44

Win Total

-28

7

-1

-8

-47

Grand Total

14

14

14

22

11

-36
333

400

358

-18
36

-27
-37

-59
-61

-46
-52

-25

-36

107

21

58

17

101

139

120

43

-3

-3
-25

-32
235
0
-43
-46

-14

7

-7

19

39

30

34

53

11

-18
17

J

Details of Models’ Performance

Using box-and-whisker plots, we show the distribution of test accuracy for the 100 experiments.
Each subplot in a figure shows the accuracy for a given PRNG- and QRNG-based initializer type.
x-axis represents epoch number, y-axis represents model accuracy on test dataset. In order to make
it easier for the reader (since there are 12 figures), we created a table of figures as shown in Table 11.
Additionally, we create a heatmap summary plot, shown in Figure 18, which summarizes the
dynamics depicted in the box-and-whisker plots on one page. Table 12 lists 17 values of E(A) from
Figure 18 that are capped at 100%.
Table 11: A table of figures showing the distributions of test accuracy based on 100 experiments.
Model
MLP
CNN
LSTM
Transformer

Optimizer

MNIST

CIFAR-10

IMDB

SGD
Adam
SGD
Adam
SGD
Adam
SGD
Adam

Fig. 19
Fig. 20
Fig. 21
Fig. 22
–
–
–
–

Fig. 23
Fig. 24
Fig. 25
Fig. 26
–
–
–
–

–
–
–
–
Fig. 27
Fig. 28
Fig. 29
Fig. 30

Table 12: The values of E(A) > 100% (see Figure 18 for details).
Dataset

Architecture

Initializer

Optimizer

E(A)

CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
CIFAR-10
IMDB
IMDB
IMDB
IMDB
MNIST
MNIST

CNN
CNN
MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP
MLP
LSTM
Transformer
Transformer
Transformer
MLP
MLP

He Normal
He Uniform
Glorot normal
Glorot normal
Glorot uniform
He normal
He uniform
He uniform
Lecun normal
Orthogonal
Orthogonal
Lecun normal
He normal
He uniform
Random normal
Glorot normal
Random uniform

SGD
SGD
Adam
SGD
SGD
SGD
Adam
SGD
Adam
Adam
SGD
SGD
Adam
Adam
SGD
Adam
Adam

400
400
400
400
400
2000
400
400
400
400
400
209
238
229
138
154
146

44

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.75

0.50

0.25

0.75

0.50

0.25

Accuracy

0.75

0.50

0.25

0.75

0.50

0.25

0.75

0.50

0.25

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 19: Accuracy distributions for MNIST test dataset, SGD optimizer and MLP model.

45

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

1.00

0.75

0.50

0.25

1.00

0.75

0.50

0.25

1.00

Accuracy

0.75

0.50

0.25

1.00

0.75

0.50

0.25

1.00

0.75

0.50

0.25

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 20: Accuracy distributions for MNIST test dataset, Adam optimizer and MLP model.

46

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

1.0

0.8

0.6

0.4

1.0

0.8

0.6

0.4

1.0

Accuracy

0.8

0.6

0.4

1.0

0.8

0.6

0.4

1.0

0.8

0.6

0.4

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 21: Accuracy distributions for MNIST test dataset, SGD optimizer and CNN model.

47

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.975

0.950

0.925

0.900

0.875

0.975

0.950

0.925

0.900

0.875

Accuracy

0.975

0.950

0.925

0.900

0.875

0.975

0.950

0.925

0.900

0.875

0.975

0.950

0.925

0.900

0.875
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 22: Accuracy distributions for MNIST test dataset, Adam optimizer and CNN model.

48

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.4

0.3

0.2

0.1

0.4

0.3

0.2

0.1

Accuracy

0.4

0.3

0.2

0.1

0.4

0.3

0.2

0.1

0.4

0.3

0.2

0.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 23: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer and MLP model.

49

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

0.5

Accuracy

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 24: Accuracy distributions for CIFAR-10 test dataset, Adam optimizer and MLP model.

50

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

Accuracy

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1

0.5

0.4

0.3

0.2

0.1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 25: Accuracy distributions for CIFAR-10 test dataset, SGD optimizer and CNN model.

51

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.6

0.4

0.2

0.6

0.4

0.2

Accuracy

0.6

0.4

0.2

0.6

0.4

0.2

0.6

0.4

0.2

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 26: Accuracy distributions for CIFAR-10 test dataset, Adam optimizer and CNN model.

52

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.54

0.52

0.50

0.48

0.54

0.52

0.50

0.48

0.54

Accuracy

0.52

0.50

0.48

0.54

0.52

0.50

0.48

0.54

0.52

0.50

0.48

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 27: Accuracy distributions for IMDB test dataset, SGD optimizer and LSTM model.

53

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.9

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5

0.9

Accuracy

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 28: Accuracy distributions for IMDB test dataset, Adam optimizer and LSTM model.

54

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.65

0.60

0.55

0.50

0.65

0.60

0.55

0.50

0.65

Accuracy

0.60

0.55

0.50

0.65

0.60

0.55

0.50

0.65

0.60

0.55

0.50
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 29: Accuracy distributions for IMDB test dataset, SGD optimizer and Transformer model.

55

glorot-normal

glorot-uniform

he-normal

he-uniform

lecun-normal

lecun-uniform

orthogonal

random-normal

random-uniform

truncated-normal

0.9

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5

0.9

Accuracy

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5

0.9

0.8

0.7

0.6

0.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Epoch
intializer

PRNG

QRNG

Figure 30: Accuracy distributions for IMDB test dataset, Adam optimizer and Transformer model.

56

cifar10, baseline-ann,
adam

cifar10, baseline-ann,
sgd

cifar10, baseline-cnn,
adam

cifar10, baseline-cnn,
sgd

imdb_reviews,
baseline-lstm, adam

imdb_reviews,
baseline-lstm, sgd

imdb_reviews,
baseline-transformer,
adam

imdb_reviews,
baseline-transformer,
sgd

truncated-normal
random-uniform
random-normal
orthogonal
lecun-uniform
lecun-normal
he-uniform
he-normal
glorot-uniform
glorot-normal

D(A)

0.0

truncated-normal
random-uniform

-0.1

Initializer

random-normal
orthogonal
lecun-uniform
lecun-normal

E(A)

he-uniform

100

he-normal
glorot-uniform

50

glorot-normal
0
mnist, baseline-ann,
adam

mnist, baseline-cnn,
adam

mnist, baseline-ann, sgd

-50

mnist, baseline-cnn, sgd

-100

truncated-normal
random-uniform
random-normal
orthogonal
lecun-uniform
lecun-normal
he-uniform
he-normal
glorot-uniform
glorot-normal
0.25

0.50

0.75

1.00

0.25

0.50

0.75

1.00

0.25

0.50

0.75

1.00

0.25

0.50

0.75

1.00

Accuracy Threshold (A)

Figure 18: A summary plot showing the values of E(A) in the main colour of a tile and D(A) in
the circle in the middle of a given tile. A yellow circle indicates that D(A) is indeterminate (i.e.,
pseudorandom or quasirandom experiments did not achieve a given level of accuracy in 30 epochs).
The values of the median accuracy thresholds A shown are 0.10, 0.15, 0.20, . . . , 0.95. A 0.05 increment
is chosen for A to ensure the plots are readable.
Among all the tiles, 17 have E(A) > 100%. To improve the readability of the figures’ colours, we
cap these cases at 100% and list them in Table 12.
A negative (blue) value of E(A) indicates that the QRNG-based version of initializer achieves A
faster than the PRNG-based version. A negative (blue) value of D(A) indicates that QRNG-based
version of the initializers exhibit less variation in A than PRNG-based version. The values of Am
are given by the rightmost tile in each row of the heatmaps.

57

